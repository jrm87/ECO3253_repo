[["index.html", "Economics of Public and Social Issues ECO 3253, Fall 2022 About About me Organization of this site Data-driven course Topics to be covered Statistical tools Economic Concepts Projects FAQs", " Economics of Public and Social Issues ECO 3253, Fall 2022 Jonathan Moreno-Medina 2022-09-14 About Hey! Welcome to ECO 3253! In this course we will see how we can use data to understand and solve current and important economic problems! You will get a sense of what is the research frontier in applied economics and social science. These include topics like equality of opportunity and mobility, education, innovation and entrepreneurship, health care, climate change, and crime. How will we get there? By doing 3 things: covering the topics with a focus towards using data that can help answer these questions understanding the intuition for how to use data to answer these questions (basics of statistical analysis) using computational tools to help us with the statistical analysis (basics of R) About me Let me briefly tell you about me: I am Colombian. Did my PhD in Economics at Duke University, a Masters in Economics at Université Catholique de Louvain in Belgium, and an undergraduate in (you guessed it!) Economics at Universidad Nacional de Colombia. I am an applied (micro)economist. What is applied microeconomics? Well, first what I do not work on: macroeconomics (inflation, general unemployment, GDP growth, and so on). The micro part just means I tend to focus on specific markets (housing and media, being the main ones), and the applied just means I use data all the time in my research. Which leads me to my next point. Organization of this site In this site I will put the materials we cover in the lectures so you can refer to it later on your own. I will divide this book into two main parts: economic content, and the tools. The tools are both statistical (correlations, means, distributions, etc) and computational (R). For the first week of class, you shall see two headers on the table of content on the left: one for each. These will get filled up as the semester continues. There is also an Appendix, which you will be able to see on the bottom of the navigation panel, where I will post complementary material where you could brush up several statistical concepts, for example. Lastly, I will also leave a link to the projects you will work on this semester at the end of the list on the left. Data-driven course We will work with data throughout the semester! This will not just be a theoretical course. We will work with real world data, and we will try to make sense of it. We will need theoretical to make sense of the world for sure (either coming from economics or statistics), but we are looking at those tools always eyeing real world instances. The course here presented is partly based on the course by Prof. Raj Chetty at Harvard. As we will see later, I also rely on some material on the book ModernDive for teaching the basics of our statistical software R. Just to give you a sense of how much economics has became a much more empirical discipline in later years, below is the number of articles in leading journals that are data-drive in a way. (#fig:fig_econJournals)Source: Mamermesh (JEL 2013) Fortunately, the same types of skills that are used to solve private market issues using data work to tackle challenges like growing inequality and climate change. In order to achieve that goal, the idea of this class is to introduce a broad range of topics, methods, and real-world applications of these sorts of ideas. Fundamentally, we want to start from the questions that motivate the methods we teach in economics and social science, rather than the traditional approach, which is to do the reverse. Topics to be covered The plan for what we will covered in the semester includes: Geography of Upward Mobility in America Causal Effects of Neighborhoods and Characteristics of High-Mobility Areas Historical and International Evidence on the Drivers of Inequality and Mobility Upward Mobility, Innovation, and Growth Higher Education and Upward Mobility Primary education Teachers and Charter Schools Racial Disparities in Economic Opportunity Improving judicial decisions Immigration Political Economy Income taxation Savings and wealth Housing markets and COVID Intro to air and water pollution, and externalities Discount rates, external validity You can check the Schedule here, or for more details, please check the syllabus on Blackboard. Statistical tools Although we are going to take a topic-oriented focus in this class, we will cover the basics of several methods that will help us make sense of the data. These methods include: Descriptive Data Analysis: correlation, regression Experiments: randomization, non-compliance Quasi-Experiments: regression discontinuity, difference-in-differences Machine Learning: prediction, overfitting, cross-validation R as our statistical software Economic Concepts We will cover and make use if several economic concepts you probably learned in your intro classes, but we will see several instances of how to use it practice. These include: Effects of price incentives Supply and demand Competitive equilibrium Adverse selection Behavioral economics vs. rational models Projects A big part of the course will be the projects you will do during the semester. You will work on 4 projects through the semester. These are more involved than most homeworks you have probably worked up to now. The good news is that they involve doing economics! You will get hands-on experience working with real data on real problems. The main recommendation is to start working on this projects early! I cannot emphasize this enough. There are several moving parts to these projects, and you need to plan in advance your work so you can try, fail, come back to it, and so on. If you try to work on these projects just the night before the deadline, that will not leave much room for experimenting, and trying. Given than several of these tools might be new, you need to give yourself time to try more than once. Important! Do not work on these projects just the day before! They are relatively involved, so give yourself enough time. For more details, please check the syllabus on Blackboard. FAQs Do I need to know how to program? No! I will give you the basic tools to understand and do basic analysis in R even if you have no background in this sort of things. Do I need to have taken econometrics? No! You will have a bit of a head start if you have, but you do not have to have taken the econometrics course to be successful in this class. I will give you some of the basic conceptual frameworks for how we think about statistical analysis and causal inference. Do I need to know statistics? Basic statistics is definitely recommended. We will have plenty of opportunity to brush up some of those concepts throughout the course, though. You can find a refresher in the Appendix Where can I find more details about this class? You can read more in the syllabus uploaded to Blackboard. "],["schedule.html", "Updated Schedule", " Updated Schedule I will keep this page with the updated schedule for the semester. You can find these readings on Blackboard. Week Session Topic Required reading Method Deliverable 1: 08/22 &amp; 08/24 1 Introduction to the course 2 Geography of Upward Mobility in America Chetty, Friedman, Hendren, Jones and Porter (2018)- Non-technical summary Correlation, regression 2: 08/29 &amp; 08/31 3 Intro to R and data 4 Intro to Visualization, Wrangling and RMarkdown First short report (In class) 3: 09/05 &amp; 09/07 - Labor Day  No class 5 Intro to Causal Effects of Neighborhoods Bergman, Chetty, DeLuca, Hendren, Katz and Palmer (2019) [Non-technical summary] Experiments (RCTs) Project 1  Part 1 4: 09/12 &amp; 09/14 6 Characteristics of High-Mobility Areas and Correlation Analysis Bergman, Chetty, DeLuca, Hendren, Katz and Palmer (2019) - Introduction Quasi-experiments 7 Moving to Opportunity and place based policies Cost-benefit analysis Project 1  Part 2 (for Friday 09/16) 5: 09/19 &amp; 09/21 8 Historical and International Evidence onthe Drivers of Inequality and Mobility 9 Higher Education and Upward Mobility Dynarski, Libassi, Michelmore and Owen (2018) Regression discontinuity 6: 09/26 &amp; 09/28 10 Higher Education and Upward Mobility II 11 Review for midterm Project 2 7: 10/03 &amp; 10/05 12 Midterm 13 Solution review 8: 10/10 &amp; 10/12 14 Primary education Chetty, Friedman and Rockoff (2011) [Non-technical summary] Experiments 15 Teachers and Charter Schools Event study designs, competitive equilibrium 9: 10/17 &amp; 10/19 16 Racial Disparities in Economic Opportunity Bertrand, and Mullainathan (2004) Dynamic models and steady states 17 Improving judicial decisions Kleinberg, Lakkaraju, Leskovec, Ludwig and Mullainathan (2017) Machine learning, implicit bias Project 3 10: 10/24 &amp; 10/26 18 Implementing a simple machine learning model in R 19 Immigration Clemens (2011) Welfare analysis 11: 10/31 &amp; 11/02 20 Political Economy 21 Income taxation Diamond and Saez (2011) Supply and demand; synthetic control 12: 11/07&amp; 11/09 22 Savings and wealth Behavioral economics 23 Regression Project 4  Part 1 13: 11/14 &amp; 11/16 24 Housing markets and COVID Glaeser, Edward, Kominers, Luca and Naik (2018) Machine learning 25 Intro to air and water pollution, and externalities Moore, Obradovich, Lehner and Baylis (2019) Difference in difference Project 4  Part 2 14: 11/21 &amp; 11/28 26 Discount rates, external validity Regression discontinuity 27 15: 11/28 &amp; 11/30 28 29 Review session 16: 12/05 -12-09 - Final Exam Week - "],["lec1_geomobility.html", "Chapter 1 Upward Mobility in the US 1.1 Parental and children income rank 1.2 Interpreting the regression line 1.3 Geographic Variation in Upward Mobility by Commuting Zone 1.4 Local Area Variation in Upward Mobility: Los Angeles, CA", " Chapter 1 Upward Mobility in the US We will first dive into a relatively recent (from 2020) called The Opportunity Atlas: Mapping the Childhood Roots of Social Mobility., by Chetty, Friedman, Hendren, Jones and Porter. The previous link should take you to Blackboard where you will be able to download it. The main question in the paper is the following: how do childrens chances of moving up vary across areas in America? To answer it, the authors need to measure upward mobility across the country. How do they do that? They will construct a database called the Opportunity Atlas. How do they measure upward mobility separately by geographic area in the United States? They take data from the 2000 and 2010 Censuses, and link that to information from federal income tax returns. They also use tax return data from 1989 to 2015. Linking those datasets yields information on essentially every American between 1989 and 2015, including how much they are earning, where they live, the dependents they have, and other information, year by year. In that dataset, they want to study economic opportunity across generations. But that requires knowing in the data who is the children of whom. In order to link parents to their children, they use information from dependency claims on tax returns. (In order to receive a tax deduction, parents must enter their childs Social Security Number on their tax returns.) Theyre able to use this information to link 99% of kids in America back to their parents, thereby generating an intergenerational sample where you can study income inequality and mobility across generations. There is a lot you can learn about the US and economic mobility with that sort of data! At the end, they end up with an 8-billion row dataset which covers 20.5 million children born between 1978 and 1983, representing 96% of our target population. They analyze children born during those particular years because we need the children to be old enough that we can measure their earnings reliably. Theyre interested in people who were born in the U.S. or are authorized immigrants who came to the U.S. in childhood. They are focusing on authorized immigrants because these datasets dont go a great job of covering undocumented immigrants. Note here that this is a limitation for the study only in the sense that it does not necessarily paint a picture of how economic mobility looks for undocumented immigrants. Additionally, the number is not 100% because there are some kids who you cant link to their parents and people you cant link the census form to the tax form. How do they measure parents and childrens incomes in tax data? They do so by measuring incomes using information from the anonymized tax return data. For parents, they use average income between 1994 and 2000 reported on Form 1040, the main tax return in the U.S. Similarly, for kids, we measure average income in 2014 and 2015, the last two years of the data theyworked with. That is when the children are in their mid-30s. Using this information, theyre going to focus on percentile ranks in the national distribution. What that means, concretely, is that they rank kids relative to all the other kids born in the same year, and parents relative to all other parents. They are comparing kids to other kids of the same age. Then likewise, they compare parents to other parents. The reason is that because they want to adjust for the fact that as people grow older, their incomes tend to rise. 1.1 Parental and children income rank The chart below was constructed using data for kids who were raised in the Chicago metro area, which consists of Chicago and the surrounding suburbs. Figure 1.1: Source: Chetty, Hendren, Kline, Saez (2014) Lets interpret the figure. On the x-axis, it shows the parent rank in the national income distribution. There are a hundred dots here, one corresponding to each percentile of the distribution (see here for a refresher on what the percentiles are). Then in each of those hundred bins, were plotting the average ranking of the child in the national income distribution. Now as you go to the right, youre looking at kids from richer and richer families, and you see that theres a very strong upward-sloping pattern. This reflects the simple fact that if you were born to a richer family in America, you yourself tend to be richer in adulthood. Now lets find the line that fits that data most accurately using a method called regression. Then Im going to focus on the value of this line, called the predicted value, at the 25th percentile of the parent income distribution. There is a lot of information contained in each dot in the graph, but by focusing on the value of this line we can construct a digestible single statistic (i.e., a number) summarizing what upward mobility looks like in each place. 1.2 Interpreting the regression line In Chicago, on average, kids who start out in families at the 25th percentile end up at the 40th percentile. Kids growing up in low-income families in Chicago, roughly speaking, earn about $30,000, on average, when theyre adults. We cant directly use the value of the dot on the above chart at the 25th percentile. Instead we use a regression line. This is because there is noise and random variation in the data, specifically with smaller samples of people. When working with small samples, it starts to become very important to fit that regression line. That is, we need to use the discipline of a statistical model. Thats the core idea of statistical models, to take the underlying data and represent it in a way that is more stable. 1.2.1 Percentiles The conversion to percentiles is very important here. If we did this analysis in dollars, that relationship is very far from linear. It is very curved, which makes it harder to fit systematically with a statistical model. To construct the Opportunity Atlas, we fit line like this to the kids who grew up in every different census tract in America. 1.2.2 What is a tract? A Census tract is a small definition of a neighborhood that the Census Bureau has created. There are 70,000 Census tracts in America, each of which has about 4,200 people. In order to handle children who might have moved while they were kids, we weigh children by the fraction of their childhood that they spent in each area. 1.3 Geographic Variation in Upward Mobility by Commuting Zone The map below plots average household earnings of children who grew up in low-income families. The map presents this statistic separately for each of the 741 commuting zones (CZs) in the United States. CZs are aggregations of counties based on commuting patterns that are similar to metro areas but cover the entire United States. Figure 1.2: Source: Chetty, Friedman, Hendren, Jones and Porter (2018) Note that the map shows household income in dollars, but the underlying statistic is based on the predicted percentile rank defined earlier. The ranks have been converted to dollars because its more intuitive and concrete. In the map, blue colors depict areas with high levels of upward mobility and red colors depict areas with low levels of upward mobility. The map shows broad geographic variation. One of the most interesting features of this map is that the highest upward mobility areas in America are the Great Plains, the rural parts of the country in the center of the country. Charlotte is one of the cities in America with the highest rates of job growth in the United States. Yet, somehow remarkably, for low-income kids who grow up in Charlotte, they do not have very good chances of moving up. The map shows that in the current generation, there are some parts of America where kids chances of moving up still look fantasticactually better than any other country in the world. Then theres some places, like in much of the industrial Midwest, where your odds of climbing up look worse than any country for which we currently have data. America is a land of tremendous variability in opportunity. 1.3.1 Adjustments for cost of living This map shows nominal incomes, meaning it does not adjust for differences in cost of living. You can redraw this map, adjusting for differences in cost of living. When you do that, you get a map that looks almost identical to the one that Im showing you here. To put it more precisely, the correlation between that data and these data is .9, meaning that it looks essentially the same. Were focusing specifically here on kids growing up in low-income families. If you look at kids growing up in middle-class families, its broadly similar. If you look at kids growing up in high-income families, you see that theres significantly less variation across areas for kids growing up in very-high-income families. 1.4 Local Area Variation in Upward Mobility: Los Angeles, CA This geographic variation in upward mobility is not just about broad regional variation, but its actually about extremely local variation. We can use the Opportunity Atlas to visualize the data. Figure 1.3: Incarceration Rates for Black Men Raised in the Lowest-Income Households in Los Angeles, CA The Opportunity Atlas starts out with the national map of the same statistics by commuting zone that we were looking at before. However, it allows us to zoom in to areas of specific interest. Let us focus on one particular example: Nickerson Gardens in Los Angeles, CA, which is a public housing project in Watts. Lets look at black men growing up in the lowest-income families in the bottom 1% of the income distribution, which is actually representative of the incomes of the families living in this public housing project. The average household income of black men who grew up in the poorest families in Watts is just $3,300 a year. It has to be the case that lots of people are basically not working at all. You can see that in a very direct way in these data because were able to look not just at income, but a variety of other outcomes, including incarceration. Focusing on incarceration rates, you will see a really shocking and disturbing statistic about the United States, and this area in particular, which is that 44% of the black men who grew up in these lowest-income families are incarcerated on a single day, the date of the 2010 census. If you go down to Compton, you see incarceration rates of 6.2%, which is a factor of 10 smaller than the 44% that we were seeing in Watts for black men growing up in low-income families. Compton is a different neighborhood than Watts, its not exactly the same, but I dont think anybody from L.A. would have predicted that Compton would have drastically different outcomes like this from Watts. That shows you that you can go two miles away and just have a dramatically different picture in terms of what kids life trajectories look like. We see that in the stark example here within Los Angeles, but we see that sort of thing more broadly across the United States. "],["lec2_causaleffect.html", "Chapter 2 The Causal Effect of Neighborhoods 2.1 Introduction 2.2 Causal Effects of Neighborhoods vs. Sorting 2.3 Identifying Causal Effects of Neighborhoods 2.4 Stating and assessing the identification assumption for the movers research design 2.5 Why Does Upward Mobility Differ Across Areas? 2.6 The Five Strongest Correlates of Upward Mobility 2.7 Policy Interest in Increasing Upward Mobility 2.8 Policies to Improve Upward Mobility 2.9 Affordable Housing Policies in the United States 2.10 A Framework for Studying the Effects of Housing Vouchers 2.11 Randomized Experiments 2.12 Non-Compliance in Randomized Experiments", " Chapter 2 The Causal Effect of Neighborhoods 2.1 Introduction The national map from the Opportunity Atlas shows rates of upward mobility across America The statistic we are focusing on specifically is the average income in adulthood of kids who grow up in families at the 25th percentile of the parental income distribution. The red colors are areas with lower levels of upward mobility and blue colors are areas with higher levels of upward mobility. For instance, we can look at the household income in adulthood for children from low-income families in South Boston. Figure 2.1: Household Income for Children of Low Income Parents in South Boston In areas like Roxbury, you see outcomes that are on par with some of the lowest upward mobility places in the United States. There is significant local variation across Census tracts in South Boston. This type of variation gives us a potential way to learn about what is driving the difference between the blue and the red areas. 2.2 Causal Effects of Neighborhoods vs. Sorting There are two different explanations for the variation in childrens outcomes across areas. Sorting Causal effects Sorting reflects the simple observation that different people live in different areas. Geographic patterns in upward mobility may reflect the sorting of people across places. If sorting is the main explanation for the patterns we have seen, then the types of policies that we would use to address this issue of equality of opportunity would be people-focused. The causal effects explanation reflects the idea that if we were to take a given child and put that child in the red-colored part of the map versus the blue-colored part of the map, wed see different outcomes for that given child. Under that causal effect explanation, we might actually want to have policies that target the red-colored places on the map. 2.3 Identifying Causal Effects of Neighborhoods An ideal experiment would be to assign children to neighborhoods at random. Social scientists approximate that sort of ideal experiment with a quasi-experimental design, where we come as close to an experiment as we can with the data that we have on-hand. To identify the causal effect of neighborhoods, Chetty and co-authors use a quasi-experimental approach to study three million families who move across Census tracts in observational data. In these data, there are many families who move with their kids across neighborhoods. We exploit variation in the age of the child when a family moves in order to isolate the causal effect of environment. Under an identification assumption that makes this as good as an experiment, this research design allows us to estimate the causal effect of neighborhoods on childrens long run outcomes. For example, imagine a hypothetical set of families that moved from Roxbury, where children from low-income families have an average adult income of \\23,000 to Savin Hill, where children from low-income families have an average adult income over $40,000, with kids of different ages, starting with families who moved when their child is exactly two years old. The first dot in the figure below plots what our estimates imply about the impact of this move for their earnings. Figure 2.2: Income Gain from Moving to a Better Neighborhood We then repeat that analysis for kids who moved when theyre three, four, five and so on. Those are the other dots in the figure. The figure shows a very clear declining pattern. The later you make that move from Roxbury to Savin Hill, the less of a gain you get. There are three key lessons that you can see from this chart. Where you grow up really matters. Childhood environment seems to matter more than where you live as an adult. Every extra year of exposure to a better childhood environment improves kids long-term outcomes. We use the data on all three million families who move across all the different neighborhoods in America to see how the age at which a child moves to a place where we see average incomes of kids who grew up there from birth impacts how much of that gain they pick up. We use a linear regression to regress the childs own income in adulthood on the average incomes of the kids we see in the destination versus the origin Census tract. We run a separate regression for kids who move at each of these different ages. 2.4 Stating and assessing the identification assumption for the movers research design Every quasi-experimental design relies on an identification assumption to make it as good as a randomized experiment. In this case, the identification assumption is that the timing of a childs move to a better or worse area is unrelated to other determinants of the childs outcomes. Under that assumption, we can interpret the variation as a pure causal effect of moving at different ages. There are two possible concerns in making this assumption. First, the families who move when their children are young may just be different from the families who move when their children are older. To address this first concern, we compare siblings outcomes within the same family, in order to control for family effects. We get the same results comparing across siblings within a family, which demonstrates that the results cannot be driven by differences between families who are moving when their kids are young and families who are moving when their kids are old. The second concern with our identification assumption has to do with time-varying factors that are related to where a family moves. To address this second concern, we use differences in neighborhood effects across subgroups to implement what we call placebo tests. In the Opportunity Atlas data, there is variation across neighborhoods and subgroups, such as gender, racial group, and so forth that allows us to run this analysis. For example, imagine you have a family that has a son and a daughter. We measure what happens to those kids outcomes when they move to a place where boys have particularly good outcomes. We find that their sons outcomes improve in proportion to the number of years theyre growing up there, but their daughters outcomes dont change at all. We find that kids outcomes converge to the full distribution of outcomes that you see in the destination to which theyre moving in a very precise way, which rules out this type of bias. 2.5 Why Does Upward Mobility Differ Across Areas? What is driving these differences in mobility across places? Why do some places produce much better outcomes for disadvantaged kids than others? Figure 2.3: Income Gain from Moving to a Better Neighborhood Theres not much of a relationship between upward mobility and rates of job growth. Some cities with high job growth and low upward mobility are importing talent from other parts of America, so the kids who grew up there arent benefiting. High mobility is not fundamentally about indicators of the labor market strength, as measured by variables like job growth as well as wage growth, types of jobs, or types of industry. 2.6 The Five Strongest Correlates of Upward Mobility The five strongest correlates of upward mobility that we have found are: Segregation Places that are more segregated, by race or by income, tend to have significantly lower levels of upward mobility. In very segregated places, like Atlanta, low-income people of all races have poor chances of climbing the income ladder. One explanation is that segregation matters because it creates differences in resources, particularly schools. A different explanation is that the mechanism is spillovers in mentoring and knowledge. Income Inequality Places with a smaller middle class tend to have much less mobility across generations. As we have more inequality within a generation, it might also become harder for kids to climb up across generations. School Quality Places with better schools have higher rates of upward mobility. Family Structure Areas with more single parents have significantly lower rates of upward mobility. This is the strongest predictor in the data. Upward mobility is lower in a community with a lot of single parents, even for children who are growing up in two-parent households. It is not about whether your own parents are married or not, but rather the rate of single-parent households in the broader community. Social Capital Social capital is a bit of a nebulous and complicated concept. It takes a village to raise a child. Will someone else help you out even if youre not doing well? These correlational analyses must be interpreted cautiously. It just gives you clues about what might be going on. 2.7 Policy Interest in Increasing Upward Mobility As a result of putting out these studies in the past four or five years, we have seen a real shift in the national conversation to focus on income mobility and the role of childhood environment, much of it coming through the media. A lot of local areas started to focus on these issues in a very systematic way. One example is Charlotte, N.C. Charlotte is a unique city in that it ranks lowest50th out of 50in terms of rates of upward mobility for kids born into low-income families despite being a very successful economy by all traditional measures. Charlotte formed a task force and a commission to focus with all its government agencies, local philanthropies, and so forth to make increasing upward mobility for kids growing up in Charlotte a central priority for the city. Our research group at Harvard has now started to team up with the Charlotte local government and local actors to try to address this problem. The release of the Opportunity Atlas data was really critical for doing this, because those data change the scale of the problem. It allows us to examine data at the level of Census tract, which collapses the scale of the problem. 2.8 Policies to Improve Upward Mobility There are two different conceptual approaches that try to improve outcomes. Figure 2.4: Two Approaches to Increasing Moibility The first is a Moving to Opportunity approach. The simplest thing that you can do to help low-income kids do better is to provide them better access to those neighborhoods. A different approach is Place-Based Investments. The idea is to take the places that are in the red colors of the map and turn them blue. Theres no way well ever be able to achieve full scale purely through the Moving to Opportunity approach. Ultimately, the long run path is to improve low-opportunity places. 2.9 Affordable Housing Policies in the United States In the U.S., we spend a tremendous amount of money in the US on affordable housing policies to help low-income families move to better neighborhoods. These policies include subsidized housing vouchers to rent better apartments. There are also many efforts to create mixed-income affordable housing development, like the low-income housing tax credit. Are these types of housing policies effective in increasing social mobility? Does this Moving to Opportunity sort of approach work? How can we make it better? A very useful benchmark to start from is to think about the simplest alternative to address any problem, which is to just give people cash. Which policy will have a greater impact on kids rates of upward mobility, giving you $1,000 in cash or a $1,000 voucher to move to a higher opportunity neighborhood? The conventional economics answer from a traditional economic point of view is that cash grants of an equivalent dollar amount are strictly better than giving someone a voucher specifically focused on housing. Vouchers work out to be more effective than giving people cash, which violates the standard economic model. 2.10 A Framework for Studying the Effects of Housing Vouchers What is the impact of giving a family a housing voucher, the $10,000 of assistance to rent an apartment or a house, on kids rates of upward mobility that is kids earnings in adulthood? Imagine that you have 10,000 different children and we will number them \\(i=1,,10000\\). Let us call child \\(i\\)s earnings \\(Y_i\\). We are going to define \\(Y_i (V=1)\\) as child is earnings if the family gets a housing voucher and \\(Y_i (V=0)\\) as the childs earnings if the family does not get a voucher. \\(Y_i (V=1)\\) and \\(Y_i (V=0)\\) are just two different hypothetical numbers. If your family had gotten a voucher, your earnings are \\(Y_i (V=1)\\), which might be $20,000 a year in the average case. If your family didnt get a voucher, your earnings are \\(Y_i (V=0)\\), which might another number like $15,000 a year. The objective of the empirical analysis is to estimate the difference \\(Y_i (V=1)-Y_i (V=0)\\). That difference is the causal effect of getting the voucher on kids earnings. Lets call that \\(G_i\\). My goal is to estimate that \\(G_i=Y_i (V=1)-Y_i (V=0)\\). The fundamental problem in empirical science is that you dont observe \\(Y_i (V=1)\\) and \\(Y_i (V=0)\\) for the same person. The gold standard solution that people have come up with in order to solve that problem is to run a randomized experiment. 2.11 Randomized Experiments Suppose you take those 10,000 kids and you flip a coin to determine if each of them gets a voucher or not. Then you compute the average level of earnings for the 5,000 who randomly got a voucher and the 5,000 who randomly did not get a voucher. The difference between those two averages gives you an estimate of \\(G_i\\), the average treatment effect of getting a voucher. The randomized experiment works because it ensures that the two groups are identical, except for the fact that one of them got the vouchers and one of them didnt. Everything else is going to be similar about them, which allows you to isolate just the causal effect of getting the voucher itself. Suppose you just compared the average earnings of people who applied versus people who didnt. The core problem is that there is no guarantee that any difference in earnings that you see between those two groups is driven by getting the voucher itself. 2.12 Non-Compliance in Randomized Experiments A common practical problem that we face in randomized experiments when we try to do this in the field is called noncompliance, when subjects dont comply with the treatment protocol. For instance, in the voucher case theres no guarantee that all families given a voucher are going to use it to rent a new apartment. You can adjust the estimated impact for the rate of compliance by dividing the Estimated Impact by the Compliance Rate: \\[\\text{True Effect}=\\frac{\\text{Estimated Impact}}{\\text{Compliance Rate}}\\] In the previous example, we would do \\[\\text{True Effect}=\\frac{\\text{Estimated Impact}}{\\text{Compliance Rate}}=\\frac{1000}{0.5}=2000\\] An important issue is that this estimate is the implied effect for the compliersthe families who would comply with the experimental protocol. A separate question is what would the effect have been for the people who chose not to comply? The local average treatment effect (LATE) applies to a certain set of compliers, but It may not apply to noncompliers. From a policy perspective, we typically care about the treatment effect for the compliers typically. "],["lec3_mto-placebased.html", "Chapter 3 Policies to Increase Upward Mobility 3.1 The Moving to Opportunity Experiment 3.2 Reevaluating the Moving to Opportunities Experiment with a Big Data Approach 3.3 The Three Limits of Randomized Controlled Trials 3.4 How Quasi-Experimental Methods can Address Randomized Controlled Trial Limitations 3.5 Implications for Housing Voucher Policies 3.6 The Stability of Historical Measures of Opportunity 3.7 The Creating Moves to Opportunity Pilot in Seattle 3.8 Three Concerns about the CMTO Approach 3.9 The Idea Behind Place Based Investment 3.10 Place Based Intervention Policies", " Chapter 3 Policies to Increase Upward Mobility 3.1 The Moving to Opportunity Experiment The Moving to Opportunity experiment was implemented in the Clinton administration, by the Housing and Urban Development agency, from 1994 to 1998 in five major cities in the United States: Baltimore, Boston, Chicago, L.A., and New York. The Housing and Urban Development agency randomly assigned 4,600 families to three different groups. The experimental group was offered a housing voucher that required them to move to a low-poverty census tract (less than 10% poverty rate). The Section 8 voucher group was offered the same value of housing voucher but with no restrictions. The control group was not offered a voucher. There was non-compliance and 48% of families assigned to the experimental voucher group actually used the voucher to move to a new place in a low-poverty census tract, and 66% of the Section 8 group used the voucher to lease a new place. Several papers have been written on the Moving to Opportunity experiment. Early research found little effects of moving on earnings and employment rates for adults and older children. Motivated by the experiment on moving between neighborhoods, this experiment was reassessed to look for exposure effects among children. 3.2 Reevaluating the Moving to Opportunities Experiment with a Big Data Approach The hypothesis was that earlier studies didnt find an effect because they didnt look at kids who were young enough. Childhood exposure effects look at impacts proportional to how long a child was exposed to a better environment. The new question was does the MTO experiment, the Moving to Opportunity approach, improve outcomes for children who moved particularly when they were very young? To revaluate this experiment, the data from HUD was linked to tax records to follow young children over a long period of time. The way these bars are constructed is that the grey bar always represents the mean for the control group. The blue bar then adds the effect of being in the Section 8 group to the mean for the control group to display the mean for the Section 8 group. The red bar does the same but for the experimental group. These calculations are made adjusting for non-compliance. Theres a very clear difference in the earnings in adulthood for the kids who were assigned to the experimental group relative to Section 8 group relative to the control group. The values below the means are the p-values, which is a way of stating the likelihood that these differences occur due to random variation in the data as opposed to a real casual effect. To interpret these results its possible to think of them in two different ways. First, what is the percentage change between the control and the other groups? The difference in earnings between the control group and the experimental group is about a 30% increase, which is large. However, in a real world sense earning about $15,000 a year is still not a large income. One way to tell this story is that there is a large effect but another way to look at it is that this approach isnt going to help people reach the middle class; it only seems to bring people out of extreme poverty. Below are the results of this analysis for several other outcomes. All graphs are constructed in the way explained above. Neighborhood quality measures the poverty rate in the neighborhood these children live in as adults. On a wide variety of dimensions, it really seems like the Moving to Opportunity intervention actually does work in quite a significant way in improving outcomes for kids who were young. However if this approach is applied to older children (over the age of 13 at the time of the experiment) or adults, then all of these impacts disappear. That is consistent with the previous literature that found little to no effect of this experiment on adults and older children. 3.3 The Three Limits of Randomized Controlled Trials There are three limits to randomized controlled trials. First is attrition. When long-term experiments are conducted, its very hard to follow people over long periods of time. However, big data can help address this problem by systematically tracking people via things such as tax records. Second is sample sizes. Its difficult and expensive to run large experiments however lots of data is needed to have precise statistically analysis. Big data actually cannot solve this problem because while the cost of data has fallen, the cost of running an experiment has not. Third is generalizability. Experiments are always conducted in specific settings and that means that those results dont always apply to other settings. Theres actually no way to know before hand if the results of an experiment will generalize to some other location with different conditions. 3.4 How Quasi-Experimental Methods can Address Randomized Controlled Trial Limitations Quasi-experimental methods using big data can address some of the limitations of randomized controlled trials. The way to think about quasi-experiments is that what they are doing is approximating experimental conditions by comparing groups that are similar. Its like the same idea, at a conceptual level, as the experiment. Rather than achieving comparability by randomizing, a quasi-experiment makes a reasonable identification assumption. Quasi-experiments can be much larger which allows them to reach greater levels of statistically precision than randomized controlled trials. With larger samples, one can also run the analysis separately subgroups, such as by race, to that the findings are very generalizable. The limitation of the quasi-experimental method is that it relies on a stronger assumption. One must believe that the two groups in the quasi-experiment are comparable to each other whereas in a randomized controlled trial all you have to believe is that randomization occurred. 3.5 Implications for Housing Voucher Policies The first implication of this evidence is that vouchers should be targeted at families with young children. What the US tends to do currently is put families on wait lists, when they have a kid, to get a housing voucher. Often, those wait lists can be incredibly long; sometimes they can be as long as 10 years. Whats going to happen there is that families get on the wait list when their kids are relatively young. They finally get to move when their kids are older, exactly when it has less benefit. Thus the current US system seems to have this large inefficiency of not targeting the right group. Second, its very important to explicitly design these policies to help families move to affordable, high-opportunity areas. The families who had to find a low-poverty census tract to live in to use their voucher are the ones that saw the biggest improvements for their kids. This is especially important because it turns out that something like 80% of the 2.1 million vouchers that are issued by the US government each year are currently used in very high-poverty, low-opportunity neighborhoods. This is possibly happening because moving to opportunity is too expensive. To examine that point look at the following chart. This it the price of buying opportunity for children in Seattle. Each dot here represents a different census tract or a different neighborhood in Seattle and the vertical axis is the measures of upward mobility. The horizontal axis is median two-bedroom rent in that neighborhood in 2015. There is a strong upward-sloping relationship, which shows that more expensive areas tend to be better for kids. However at each median rent there is a lot of variety in outcomes. Take a place like the Central District (shown above), which is where a lot of families with housing vouchers in Seattle currently live. It has poor outcomes. However, Normandy Park is actually less expensive than the Central District but produces dramatically better outcomes for children. This is the idea of an opportunity bargain which is currently being used in a pilot study. 3.6 The Stability of Historical Measures of Opportunity On concern is that this data is outdated. To prove that the historical estimates useful predictors of opportunity for children who are growing up in these neighborhoods now this study also examined the accuracy of the predictive power of the data. The methodology was to predict outcomes for kids in 1990 using data that is only one year old, then to predict outcomes for kids in 1990 using data that is two years old. The process was repeated for a total of 10 years. Using data that was 10 years old had a 90% accuracy rate, which is not perfect but still highly informative. 3.7 The Creating Moves to Opportunity Pilot in Seattle A current pilot project is being run in Seattle using all of the previous findings. The pilot provides information to tenants on which neighborhoods might provide better opportunities for their children. It also recruits landlords to accepting tenants with housing vouchers by simplifying the process of dealing with the housing authority. Finally, it offers housing search assistance. 3.8 Three Concerns about the CMTO Approach There are three important potential concerns with the Moving to Opportunity approach. First there is a large cost to the housing voucher program. However, one also has to consider the improved outcomes of children in the future and what that will contribute to future taxes. These children can also have lower incarceration rates and they might depend less on the welfare system so there are lower social costs as well from implementing this policy. Analysis from the Moving to Opportunities experiment showed that the incremental cost of providing vouchers relative to having a family in traditional public housing is actually negative. That is, taxpayers in the long run will actually save money from this program because theyll end up benefiting more down the road than it cost them to pay for the program in the beginning. The second concern is negative spillovers. Some might be concerned that the integration of lower income families into neighborhoods hurts the higher income families living there already. This, I think, is the most important concern from a political point of view. That can be evaluated by looking at how the outcomes of the rich vary across areas in relation to the outcomes of the poor. Empirically, more integrated cities do not have worse outcomes for the rich, on average. The third concern is a simple limit to scalability. It is impossible to move everyone from one neighborhood to another and expect to have significant effects. Ultimately, this means that a policy-based approach will need to use strategies to improve low-mobility neighborhoods as well as use the moving to opportunity approach. 3.9 The Idea Behind Place Based Investment The goal with place-based investment is to figure out what makes high mobility areas into what they are and change the low mobility areas to resemble the high mobility areas. The first step to do this is to look at key predictors of differences in mobility across places. There are four strong predictors. The strongest predictor these differences are differences in poverty rates. Places that have less concentrated poverty tend to have better outcomes for low-income kids. Another way to say this is that the more mixed-income areas have better outcomes for low-income kids. The second predictor is more stable family structures. That means these places have a larger share of two-parent families. The third predictor is more social capital. Social capital can be defined as connections between low and high-income people. This is the kind place where somebody else might help you out even if youre not doing well. One example was Salt Lake City with the Mormon Church. The fourth predictor, as you might expect intuitively, is better school quality. The limitation of this type of correlational analysis is that it gives you a sense of what factors might be associated with these differences in outcomes across places, but it doesnt indicated which of these things have a causal effect. Current research is attempting to figure what actually is having a casual effect in these neighborhoods. Researchers have found that these correlations hold at an incredibly granular level. Once the poverty rate in one specific neighborhood is accounted for, the poverty rate of the next closest neighborhood has practically no predictive power on childrens mobility. This is important because it indicates that environment matters in a very local way. Rerunning this correlation using city blocks rather than census tracts yields that only the poverty rates in the blocks within a .6 mile radius of the block of interest matter in predicting childrens outcomes. This evidence should motivate policy makers to look at addressing mobility not on a national scale but on a local scale. 3.10 Place Based Intervention Policies There are lots of efforts by local governments, by non-profits, lots of people who care about these issues in the United States and elsewhere to try to revitalize neighborhoods. The problem there is very little systematic evidence to date on which of these programs actually work and which of them dont work. The reason this data doesnt exist at the moment is that just because neighborhoods get new houses or better schools doesnt mean that the people living there originally are better off, it could be that better off people moved into the area once the neighborhood was improved. Currently social scientists are working on ways to track these people, who lived in neighborhoods before interventions, across time. Other work at the moment is focusing on building a pipeline for economic opportunity, starting from interventions at very early ages which could be things related to family stability or prenatal care, things like that, to early childhood education. This process would continue throughout childhood with different programs as children age. Throughout this whole process, affordable housing is key since as neighborhoods are improved then more people want to move into the area, which drives up prices, and drives out the original inhabitants. Other issues with this approach are price and scalability. These large interventions can be incredibly expensive. Also, when many interventions are implemented at the same time then it can be difficult to pinpoint which interventions are the most important to improve childrens outcomes. Harlem Childrens Zone is an example of building a pipeline through its Promise Academy, which offers much more support than a typical school in this neighborhood. Other programs such as Becoming a Man or Credible Messengers seek to build social capital by providing mentoring to minority youth. Other programs that are examples of place-based investments include Peer Forward and the Harmony Project. Place based investments are also being investigated using historical data. Census Bureau records actually exist going back to the 1950s, which has allowed social scientists to identify individuals who were affected by government programs such as the Harlem Childrens Zone or the Hope IV Demolitions, when HUD demolished lots of public housing projects to build mixed income housing in other areas. Social scientists are still working on getting through all of this data to be able to study these historical events using a quasi-experimental design. In conclusion, lots of work is currently being done to pinpoint what place-based investments might be the most efficient but at the moment no one knows what the best solution is. "],["getting-started.html", "Chapter 4 Very Brief Intro to Data in R 4.1 Why R again? 4.2 Key concepts before we start: 4.3 Lets open R! 4.4 How do I code in R? 4.5 In Class Exercise 4.6 What are R packages? 4.7 Hands-on exercise! 4.8 Conclusion", " Chapter 4 Very Brief Intro to Data in R 4.1 Why R again? 4.1.1 Why are we learning R? I wanted to learn about economics of public and social issues This class is about social issues in economics. But what are those social issues? Economic mobility and inequality Effects of education on wages and inequality Criminal justice system outcomes Pollution and climate change and so on How can we know if going to school increases wages? Or if economic mobility is low or high? We need to analyze data! We can do that analysis by hand but that would be very time consuming. Or we can use a super calculator with amazing capabilities to explore data, maps, etc: enter R and R Studio. This course is not about teaching you all about R! We will only cover the very basics so you can jump into doing some empirical analysis by yourself. You will be able to expand much more on the tools briefly described here in other, more advanced, courses in the Economics sequence. Important! For the vast majority of exercises in our course, I will give you all the code you will have to run. So, its not like you need to write anything from scratch! I do want you to get a basic understanding of what we will be doing when we run those lines. 4.1.2 Ok, but why R? R is free and open source! R has a vibrant online community! R is very flexible and powerful  adaptable to nearly any task ( e.g., correlations, econometrics, spatial data analysis, machine learning, web scraping, data cleaning, website building, teaching.) Employers like R over alternatives 4.1.3 Added benefits of learning R Employers hire people that knows R. Again, we will only cover the essentials, but maybe you want to keep this in mind as you go along with your studies. 4.2 Key concepts before we start: Before we can start exploring data in R, there are some key concepts to understand first: What are R and RStudio? How do I code in R? What are R packages? Well introduce these concepts in upcoming Sections 4.2.1-4.6. Then well introduce our first data set: data on the economic mobility for all neighborhoods across the US in the atlas dataset. 4.2.1 What are R and RStudio? For much of this book, we will assume that you are using R via RStudio. First time users often confuse the two. At its simplest: R is like a cars engine. RStudio is like a cars dashboard. R: Engine RStudio: Dashboard More precisely, R is a programming language that runs computations while RStudio is an integrated development environment (IDE) that provides an interface by adding many convenient features and tools. So just as having access to a speedometer, rearview mirrors, and a navigation system makes driving much easier, using RStudios interface makes using R much easier as well. 4.2.2 R and RStudio: In your computer or in the cloud? To use R and RStudio, you can: install it in your computer (see the book) or  run it on someone elses computer (the cloud!). We will do that, so you dont have to worry about installations. In this course I wont require you to install R and RStudio in your own computer. Instead, we will use the cloud! The Department of Economics is graciously paying for your accounts this semester. If you would still like to install R and RStudio in your own computer, please follow the instructions in Section 1.1.1 in this link. Of course, you are encouraged to experiment in your own machine as well. Notice, however, that you should carry out the assignments and projects in RStudio Cloud. Important! Homework and projects should be carried in RStudio Cloud (not your personal computer). 4.3 Lets open R! 4.3.1 RStudio in the cloud Lets jump in! You should have received a link for you to access RStudio in the cloud. The Department of Economics is paying for us to use this service for this class. Remember that all the code is running on someones computer. In this case, it is running on a computer owned by the folks at RStudio. Receive link with invitation You should have received an email with a link inviting you to join RStudio Cloud. Once you click on the link, you should land in a page like this: Then you should fill out the information for your account. Please use your utsa email account (the account you should have received the invitation to). Once you click on Sign Up, it will show this: Then you need to go back to your email and verify it. Then log back in again! Inside RStudio Cloud select ECO3253 Once you have logged back in, you should land in the main page, which should look like this: Click on the left where it says ECO3253. That is where you will see all the material and projects for this class. Select the appropriate project you want to work on Once you are in the ECO3253 tab, you should see a list of individual projects we will work on throughout the semester. It should look something like this: For our first class on R you will select the link that says introRClass. That is it! Now we are ready to get to work! 4.3.2 Using R via RStudio Recall our car analogy from above. Much as we dont drive a car by interacting directly with the engine but rather by interacting with elements on the cars dashboard, we wont be using R directly but rather we will use RStudios interface. After you open RStudio Cloud and follow the previous instructions, you should see a panel like the following: Note the three panes, which are three panels dividing the screen: The Console pane, the Files pane, and the Environment pane. Over the course of this chapter, youll come to learn what purpose each of these panes serve. 4.4 How do I code in R? Now that youre set up with RStudio Cloud, you are probably asking yourself OK. Now how do I use R? The first thing to note as that unlike other statistical software programs like Excel, STATA, or SAS that provide point and click interfaces, R is an interpreted language, meaning you have to enter in R commands written in R code. In other words, you have to code/program in R. Note that well use the terms coding and programming interchangeably in this book. While it is not required to be a seasoned coder/computer programmer to use R, there is still a set of basic programming concepts that R users need to understand. Consequently, while this course is not a book on programming, you will still learn just enough of these basic programming concepts needed to explore and analyze data effectively. 4.4.1 Basic programming concepts and terminology We now introduce some basic programming concepts and terminology. Instead of asking you to learn all these concepts and terminology right now, well guide you so that youll learn by doing. Note that in this book we will always use a different font to distinguish regular text from computer_code. The best way to master these topics is, in our opinions, learning by doing and lots of repetition. Basics: Console: Where you enter in commands. Running code: The act of telling R to perform an action by giving it commands in the console. Objects: Where values are saved in R. In order to do useful and interesting things in R, we will want to assign a name to an object. For example we could do the following assignments: x &lt;- 44 - 20 and three &lt;- 3. This would allow us to run x + three which would return 27. Data types: Integers, doubles/numerics, logicals, and characters. Vectors: A series of values. These are created using the c() function, where c() stands for combine or concatenate. For example: c(6, 11, 13, 31, 90, 92). Factors: Categorical data are represented in R as factors. Data frames: Data frames are like rectangular spreadsheets: they are representations of datasets in R where the rows correspond to observations and the columns correspond to variables that describe the observations. Well cover data frames later in Section 4.7.1. Conditionals: Testing for equality in R using == (and not = which is typically used for assignment). Ex: 2 + 1 == 3 compares 2 + 1 to 3 and is correct R code, while 2 + 1 = 3 will return an error. Boolean algebra: TRUE/FALSE statements and mathematical operators such as &lt; (less than), &lt;= (less than or equal), and != (not equal to). Logical operators: &amp; representing and as well as | representing or. Ex: (2 + 1 == 3) &amp; (2 + 1 == 4) returns FALSE since both clauses are not TRUE (only the first clause is TRUE). On the other hand, (2 + 1 == 3) | (2 + 1 == 4) returns TRUE since at least one of the two clauses is TRUE. Functions, also called commands: Functions perform tasks in R. They take in inputs called arguments and return outputs. You can either manually specify a functions arguments or use the functions default values. This list is by no means an exhaustive list of all the programming concepts and terminology needed to become a savvy R user; such a list would be so large it wouldnt be very useful, especially for novices. Rather, we feel this is a minimally viable list of programming concepts and terminology you need to know before getting started. Im confident you can learn the rest as you go. Remember that your mastery of all of these concepts and terminology will build as you practice more and more.1 4.5 In Class Exercise Try running the following commands in the console. What do you see for each one? 2*3 2*pi log(10) exp(2) sqrt(25) 3==3 3==4 3&lt;=4 3!=4 x &lt;- c(1,3,2,5)# this is called a &#39;vector&#39; x #what do you see? x &lt;- c(1,6,2) x #now what do you see? y &lt;- c(1,4,3) # USE ARROW! length(x) length(y) x+y #write this write this again 4.5.1 Errors, warnings, and messages Noticed the last thing that appeared in the console when you wrote write this again? It had scary red letters. It is an example of something that intimidates new R and RStudio users: how it reports errors, warnings, and messages. R reports errors, warnings, and messages in a glaring red font, which makes it seem like it is scolding you. However, seeing red text in the console is not always bad. R will show red text in the console pane in three different situations: Errors: When the red text is a legitimate error, it will be prefaced with Error in and try to explain what went wrong. Generally when theres an error, the code will not run. For example, well see in Subsection 4.6.3 if you see Error in ggplot(...) : could not find function \"ggplot\", it means that the ggplot() function is not accessible because the package that contains the function (ggplot2) was not loaded with library(ggplot2). Thus you cannot use the ggplot() function without the ggplot2 package being loaded first. Warnings: When the red text is a warning, it will be prefaced with Warning: and R will try to explain why theres a warning. Generally your code will still work, but with some caveats. For example, you will see in Chapter 5 if you create a scatterplot based on a dataset where one of the values is missing, you will see this warning: Warning: Removed 1 rows containing missing values (geom_point). R will still produce the scatterplot with all the remaining values, but it is warning you that one of the points isnt there. Messages: When the red text doesnt start with either Error or Warning, its just a friendly message. Youll see these messages when you load R packages in the upcoming Subsection 4.6.2 or when you read data saved in spreadsheet files with the read_csv() function as youll see in Chapter ??. These are helpful diagnostic messages and they dont stop your code from working. Additionally, youll see these messages when you install packages too using install.packages(). Important! When you see red text in the console, dont panic! Just check out what it could be. Remember, when you see red text in the console, dont panic. It doesnt necessarily mean anything is wrong. Rather: If the text starts with Error, figure out whats causing it. Think of errors as a red traffic light: something is wrong! If the text starts with Warning, figure out if its something to worry about. For instance, if you get a warning about missing values in a scatterplot and you know there are missing values, youre fine. If thats surprising, look at your data and see whats missing. Think of warnings as a yellow traffic light: everything is working fine, but watch out/pay attention. Otherwise the text is just a message. Read it, wave back at R, and thank it for talking to you. Think of messages as a green traffic light: everything is working fine. 4.5.2 Tips on learning to code Learning to code/program is very much like learning a foreign language, it can be very daunting and frustrating at first. Such frustrations are very common and it is very normal to feel discouraged as you learn. However just as with learning a foreign language, if you put in the effort and are not afraid to make mistakes, anybody can learn. Here are a few useful tips to keep in mind as you learn to program: Remember that computers are not actually that smart: You may think your computer or smartphone are smart, but really people spent a lot of time and energy designing them to appear smart. Rather you have to tell a computer everything it needs to do. Furthermore the instructions you give your computer cant have any mistakes in them, nor can they be ambiguous in any way. Take the copy, paste, and tweak approach: Especially when learning your first programming language, it is often much easier to taking existing code that you know works and modify it to suit your ends, rather than trying to write new code from scratch. We call this the copy, paste, and tweak approach. So early on, we suggest not trying to write code from memory, but rather take existing examples we have provided you, then copy, paste, and tweak them to suit your goals. Dont be afraid to play around! The best way to learn to code is by doing: Rather than learning to code for its own sake, we feel that learning to code goes much smoother when you have a goal in mind or when you are working on a particular project, like analyzing data that you are interested in. Practice is key: Just as the only method to improving your foreign language skills is through practice, practice, and practice; so also the only method to improving your coding is through practice, practice, and practice. Dont worry however; well give you plenty of opportunities to do so! 4.6 What are R packages? Another point of confusion with many new R users is the idea of an R package. R packages extend the functionality of R by providing additional functions, data, and documentation. They are written by a world-wide community of R users and can be downloaded for free from the internet. For example, among the many packages we will use in this book are the ggplot2 package for data visualization which we will cover later (or you can check here) or the dplyr package for data wrangling (again, we will cover later, but check this if you want to know more). A good analogy for R packages is they are like apps you can download onto a mobile phone: R: A new phone R Packages: Apps you can download So R is like a new mobile phone: while it has a certain amount of features when you use it for the first time, it doesnt have everything. R packages are like the apps you can download onto your phone from Apples App Store or Androids Google Play. Lets continue this analogy by considering the Instagram app for editing and sharing pictures. Say you have purchased a new phone and you would like to share a recent photo you have taken on Instagram. You need to: Install the app: Since your phone is new and does not include the Instagram app, you need to download the app from either the App Store or Google Play. You do this once and youre set. You might do this again in the future any time there is an update to the app. Open the app: After youve installed Instagram, you need to open the app. Once Instagram is open on your phone, you can then proceed to share your photo with your friends and family. The process is very similar for using an R package. You need to: Install the package: This is like installing an app on your phone. Most packages are not installed by default when you install R and RStudio. Thus if you want to use a package for the first time, you need to install it first. Once youve installed a package, you likely wont install it again unless you want to update it to a newer version. Load the package: Loading a package is like opening an app on your phone. Packages are not loaded by default when you start RStudio on your computer; you need to load each package you want to use every time you start RStudio. Lets now show you how to perform these two steps for the ggplot2 package for data visualization. 4.6.1 Package installation For the most part, in RStudio Cloud, I will pre-install the packages you are going to need to use. But just in case you also want to work on your own machine, or install your own packages, here I explain that a bit more. There are two ways to install an R package. For example, to install the ggplot2 package: Easy way: In the Files pane of RStudio: Click on the Packages tab Click on Install Type the name of the package under Packages (separate multiple with space or comma): In this case, type ggplot2 Click Install Slightly harder way: An alternative but slightly less convenient way to install a package is by typing install.packages(\"ggplot2\") in the Console pane of RStudio and hitting enter. Note you must include the quotation marks. Much like an app on your phone, you only have to install a package once. However, if you want to update an already installed package to a newer verions, you need to re-install it by repeating the above steps. Learning check (LC2.1) Repeat the above installing steps, but for the dplyr, and knitr packages. This will install the earlier mentioned dplyr package, and the knitr package for writing reports in R. 4.6.2 Package loading Recall that after youve installed a package, you need to load it, in other words open it. We do this by using the library() command. For example, to load the ggplot2 package, run the following code in the Console pane. What do we mean by run the following code? Either type or copy &amp; paste the following code into the Console pane and then hit the enter key. library(ggplot2) If after running the above code, a blinking cursor returns next to the &gt; prompt sign, it means you were successful and the ggplot2 package is now loaded and ready to use. If however, you get a red error message that reads Error in library(ggplot2) : there is no package called ggplot2  it means that you didnt successfully install it. In that case, go back to the previous subsection Package installation and install it. Learning check (LC2.2) Load the dplyr, and knitr packages as well by repeating the above steps. 4.6.3 Package use One extremely common mistake new R users make when wanting to use particular packages is that they forget to load them first by using the library() command we just saw. Remember: you have to load each package you want to use every time you start RStudio. If you dont first load a package, but attempt to use one of its features, youll see an error message similar to: Error: could not find function R is telling you that you are trying to use a function in a package that has not yet been loaded. Almost all new users forget do this when starting out, and it is a little annoying to get used to. However, youll remember with pratice. 4.7 Hands-on exercise! 4.7.1 Explore your first dataset: economic mobility in the US Lets put everything weve learned so far into practice and start exploring some real data! These spreadsheet-type datasets are called data frames in R; we will focus on working with data saved as data frames throughout this course. Step 1: Load all the packages needed for this exercise (assuming youve already installed them). library(dplyr) library(tibble) atlas &lt;- readRDS(gzcon(url(&quot;https://raw.githubusercontent.com/jrm87/ECO3253_repo/master/data/atlas.rds&quot;))) atlas&lt;-tibble(atlas) 4.7.2 Economic mobility data The Opportunity Atlas is a freely available interactive mapping tool that traces the roots of outcomes such as poverty and incarceration back to the neighborhoods in which children grew up. The atlas dataset we loaded has the underlying data to describe equality of opportunity across the 73,278 different neighborhoods in the United States. Lets unpack these data a bit more! 4.7.3 atlas data frame We will begin by exploring the atlas data frame we just loaded to get an idea of its structure. Run the following code in your console (either by typing it or cutting &amp; pasting it): it loads the atlas dataset into your Console. Note depending on the size of your monitor, the output may vary slightly. atlas ## # A tibble: 73,278 x 62 ## tract county state cz czname hhinc_mean2000 mean_commutetime2000 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 20100 1 1 11101 Montgomery 68639. 26.2 ## 2 20200 1 1 11101 Montgomery 57243. 24.8 ## 3 20300 1 1 11101 Montgomery 75648. 25.3 ## 4 20400 1 1 11101 Montgomery 74852. 23.0 ## 5 20500 1 1 11101 Montgomery 96175. 26.2 ## 6 20600 1 1 11101 Montgomery 68096. 21.6 ## 7 20700 1 1 11101 Montgomery 65182. 23.2 ## 8 20801 1 1 11101 Montgomery 76874. 30.3 ## 9 20802 1 1 11101 Montgomery 77310. 30.7 ## 10 20900 1 1 11101 Montgomery 66234. 36.4 ## # ... with 73,268 more rows, and 55 more variables: frac_coll_plus2010 &lt;dbl&gt;, ## # frac_coll_plus2000 &lt;dbl&gt;, foreign_share2010 &lt;dbl&gt;, med_hhinc2016 &lt;dbl&gt;, ## # med_hhinc1990 &lt;dbl&gt;, popdensity2000 &lt;dbl&gt;, poor_share2010 &lt;dbl&gt;, ## # poor_share2000 &lt;dbl&gt;, poor_share1990 &lt;dbl&gt;, share_black2010 &lt;dbl&gt;, ## # share_hisp2010 &lt;dbl&gt;, share_asian2010 &lt;dbl&gt;, share_black2000 &lt;dbl&gt;, ## # share_white2000 &lt;dbl&gt;, share_hisp2000 &lt;dbl&gt;, share_asian2000 &lt;dbl&gt;, ## # gsmn_math_g3_2013 &lt;dbl&gt;, rent_twobed2015 &lt;dbl&gt;, ... Lets unpack this output: A tibble: 73,278 x 62: A tibble is a kind of data frame used in R. This particular data frame has 73,278 rows (one for each neighborhood) 62 columns corresponding to 62 variables describing each observation (e.g. neighborhood in this case) tract county state cz czname hhinc_mean2000 mean_commutetime2000 ... are different columns, in other words variables, of this data frame. We then have the first 10 rows of observations corresponding to 10 neighborhoods. ... with 73,268 more rows, and 52 more variables: indicating to us that 73,268 more rows of data and 52 more variables could not fit in this screen. Unfortunately, this output does not allow us to explore the data very well. Lets look at different tools to explore data frames. 4.7.4 Exploring data frames Among the many ways of getting a feel for the data contained in a data frame such as atlas, we present three functions that take as their argument, in other words their input, the data frame in question. We also include a fourth method for exploring one particular column of a data frame: Using the View() function built for use in RStudio. We will use this the most. Using the glimpse() function, which is included in the dplyr package. Using the $ operator to view a single variable in a data frame. 1. View(): Run View(atlas) in your Console in RStudio, either by typing it or cutting &amp; pasting it into the Console pane, and explore this data frame in the resulting pop-up viewer. You should get into the habit of always Viewing any data frames that come your way. Note the capital V in View. R is case-sensitive so youll receive an error is you run view(atlas) instead of View(atlas). Learning check (LC2.3) What does any ONE row in this atlas dataset refer to? A. Data on an neighborhood B. Data on a state C. Data on an person D. Data on multiple neighborhood By running View(atlas), we see the different variables listed in the columns and we see that there are different types of variables. Some of the variables like poor_share2010, hhinc_mean2000, and share_hisp2010 are what we will call quantitative variables. These variables are numerical in nature. Other variables, like tract are categorical: they are just names (even if they have numbers). For example tract represents a Tract FIPS Code, that is, a 6-digit code assigned by the census folks to each neighborhood in 2010. Note that if you look in the leftmost column of the View(atlas) output, you will see a column of numbers. These are the row numbers of the dataset. If you glance across a row with the same number, say row 5, you can get an idea of what each row corresponds to. 2. glimpse(): The second way to explore a data frame is using the glimpse() function included in the dplyr package. Thus, you can only use the glimpse() function after youve loaded the dplyr package. This function provides us with an alternative method for exploring a data frame: glimpse(atlas) ## Rows: 73,278 ## Columns: 62 ## $ tract &lt;dbl&gt; 20100, 20200, 20300, 20400, 20500, 20600,~ ## $ county &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3,~ ## $ state &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,~ ## $ cz &lt;dbl&gt; 11101, 11101, 11101, 11101, 11101, 11101,~ ## $ czname &lt;chr&gt; &quot;Montgomery&quot;, &quot;Montgomery&quot;, &quot;Montgomery&quot;,~ ## $ hhinc_mean2000 &lt;dbl&gt; 68639, 57243, 75648, 74852, 96175, 68096,~ ## $ mean_commutetime2000 &lt;dbl&gt; 26.2, 24.8, 25.3, 23.0, 26.2, 21.6, 23.2,~ ## $ frac_coll_plus2010 &lt;dbl&gt; 0.2544, 0.2672, 0.1642, 0.2527, 0.3751, 0~ ## $ frac_coll_plus2000 &lt;dbl&gt; 0.1565, 0.1469, 0.2244, 0.2305, 0.3212, 0~ ## $ foreign_share2010 &lt;dbl&gt; 0.00995, 0.01634, 0.02710, 0.01508, 0.046~ ## $ med_hhinc2016 &lt;dbl&gt; 66000, 41107, 51250, 52704, 52463, 63750,~ ## $ med_hhinc1990 &lt;dbl&gt; 27375, 19000, 29419, 37891, 41516, 29000,~ ## $ popdensity2000 &lt;dbl&gt; 195.72, 566.38, 624.20, 713.80, 529.93, 4~ ## $ poor_share2010 &lt;dbl&gt; 0.1050, 0.1476, 0.0804, 0.0632, 0.0596, 0~ ## $ poor_share2000 &lt;dbl&gt; 0.1268, 0.2271, 0.0766, 0.0455, 0.0368, 0~ ## $ poor_share1990 &lt;dbl&gt; 0.0989, 0.1983, 0.1140, 0.0679, 0.0547, 0~ ## $ share_black2010 &lt;dbl&gt; 0.1192, 0.5650, 0.1980, 0.0467, 0.1397, 0~ ## $ share_hisp2010 &lt;dbl&gt; 0.02301, 0.03456, 0.02579, 0.01938, 0.032~ ## $ share_asian2010 &lt;dbl&gt; 0.004707, 0.002304, 0.004744, 0.003648, 0~ ## $ share_black2000 &lt;dbl&gt; 0.0755, 0.6221, 0.1491, 0.0259, 0.0601, 0~ ## $ share_white2000 &lt;dbl&gt; 0.897, 0.355, 0.820, 0.938, 0.897, 0.799,~ ## $ share_hisp2000 &lt;dbl&gt; 0.00625, 0.00846, 0.01647, 0.02217, 0.015~ ## $ share_asian2000 &lt;dbl&gt; 0.003644, 0.003171, 0.003893, 0.007288, 0~ ## $ gsmn_math_g3_2013 &lt;dbl&gt; 2.76, 2.76, 2.76, 2.76, 2.76, 2.76, 2.76,~ ## $ rent_twobed2015 &lt;dbl&gt; NA, 907, 583, 713, 923, 765, 645, 532, 67~ ## $ singleparent_share2010 &lt;dbl&gt; 0.1139, 0.4885, 0.2281, 0.2275, 0.2597, 0~ ## $ singleparent_share1990 &lt;dbl&gt; 0.1812, 0.3525, 0.1259, 0.1268, 0.0744, 0~ ## $ singleparent_share2000 &lt;dbl&gt; 0.251, 0.393, 0.245, 0.191, 0.168, 0.289,~ ## $ traveltime15_2010 &lt;dbl&gt; 0.2730, 0.1520, 0.2055, 0.3507, 0.2505, 0~ ## $ emp2000 &lt;dbl&gt; 0.567, 0.493, 0.579, 0.597, 0.661, 0.643,~ ## $ mail_return_rate2010 &lt;dbl&gt; 83.5, 81.3, 79.5, 83.5, 77.3, 82.8, 83.2,~ ## $ ln_wage_growth_hs_grad &lt;dbl&gt; 0.03823, 0.08931, -0.17774, -0.07231, -0.~ ## $ jobs_total_5mi_2015 &lt;dbl&gt; 10109, 9948, 10387, 12933, 12933, 9193, 1~ ## $ jobs_highpay_5mi_2015 &lt;dbl&gt; 3396, 3328, 3230, 3635, 3635, 3052, 3389,~ ## $ nonwhite_share2010 &lt;dbl&gt; 0.1627, 0.6111, 0.2476, 0.0812, 0.2162, 0~ ## $ popdensity2010 &lt;dbl&gt; 504.8, 1682.2, 1633.4, 1780.0, 2446.3, 11~ ## $ ann_avg_job_growth_2004_2013 &lt;dbl&gt; -0.00677, -0.00425, 0.01422, -0.01984, 0.~ ## $ job_density_2013 &lt;dbl&gt; 92.133, 971.318, 340.920, 207.386, 800.27~ ## $ kfr_natam_p25 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N~ ## $ kfr_natam_p75 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N~ ## $ kfr_natam_p100 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N~ ## $ kfr_asian_p25 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N~ ## $ kfr_asian_p75 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N~ ## $ kfr_asian_p100 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N~ ## $ kfr_black_p25 &lt;dbl&gt; 26819, 18138, 20515, 12883, 26594, 19108,~ ## $ kfr_black_p75 &lt;dbl&gt; 45926, 33842, 34133, 40334, 42575, 26062,~ ## $ kfr_black_p100 &lt;dbl&gt; 84690, 60512, 56516, 105250, 72565, 35737~ ## $ kfr_hisp_p25 &lt;dbl&gt; NA, NA, NA, 26363, 17234, NA, NA, NA, 329~ ## $ kfr_hisp_p75 &lt;dbl&gt; NA, NA, NA, 67532, 44642, NA, NA, NA, 655~ ## $ kfr_hisp_p100 &lt;dbl&gt; NA, NA, NA, NA, 93976, NA, NA, NA, 147274~ ## $ kfr_pooled_p25 &lt;dbl&gt; 27621, 22303, 28215, 33331, 34633, 23583,~ ## $ kfr_pooled_p75 &lt;dbl&gt; 51531, 46650, 50754, 52337, 57007, 47735,~ ## $ kfr_pooled_p100 &lt;dbl&gt; 78922, 74225, 76055, 72586, 81792, 75188,~ ## $ kfr_white_p25 &lt;dbl&gt; 30328, 42189, 33670, 34181, 39540, 27835,~ ## $ kfr_white_p75 &lt;dbl&gt; 50820, 54239, 51579, 52848, 58699, 51198,~ ## $ kfr_white_p100 &lt;dbl&gt; 75126, 66646, 71991, 74330, 80415, 80144,~ ## $ count_pooled &lt;dbl&gt; 519, 530, 960, 1123, 1867, 994, 772, 632,~ ## $ count_white &lt;dbl&gt; 457, 173, 774, 1033, 1626, 756, 630, 523,~ ## $ count_black &lt;dbl&gt; 42, 336, 151, 40, 137, 198, 111, 89, 290,~ ## $ count_asian &lt;dbl&gt; 3, 1, 1, 6, 13, 2, 1, 1, 5, 0, 0, 0, 0, 3~ ## $ count_hisp &lt;dbl&gt; 4, 5, 21, 37, 39, 19, 14, 9, 29, 17, 7, 3~ ## $ count_natam &lt;dbl&gt; 6, 1, 2, 0, 8, 2, 9, 1, 5, 4, 8, 3, 20, 8~ We see that glimpse() will give you the first few entries of each variable in a row after the variable. In addition, the data type of the variable is given immediately after each variables name inside &lt; &gt;. Here, int and dbl refer to integer and double, which are computer coding terminology for quantitative/numerical variables. In contrast, chr refers to character, which is computer terminology for text data. Text data, such as the czname (the name of the metro area), are categorical variables. 3. $ operator Lastly, the $ operator allows us to explore a single variable within a data frame. For example, run the following in your console atlas$tract We used the $ operator to extract only the tract variable and return it as a vector of length 73,278. We will only be occasionally exploring data frames using this operator, instead favoring the View() and glimpse() functions. 4.8 Conclusion Weve given you what we feel are the most essential concepts to know before you can start exploring data in R. There is much more to explore in R but this is a great place to get started! 4.8.1 Additional resources If you want to dive more and feel you could benefit from a more detailed introduction, check this short book: Getting used to R, RStudio, and R Markdown short book. It has screencast recordings that you can follow along and pause as you learn. Furthermore, there is an introduction to R Markdown, a tool used for reproducible research in R. We will see more about that in the next class. If you truly insist on getting more information, you can check this link explaining some of the basics: https://rstudio-education.github.io/hopr/basics.html ;but again, this is not required or expected. "],["viz.html", "Chapter 5 Data Visualization 5.1 The Grammar of Graphics 5.2 Three Important Graphs - 5.3 Scatterplots 5.4 Histograms 5.5 Barplots 5.6 Conclusion", " Chapter 5 Data Visualization This chapter is based in big part on the chapter on visualization by the folks at Northwestern. Please be check that link if you want to dive into how to make even cooler graphs than the ones we will cover here. We will learn basic tools to visualize our data. By visualizing our data, we gain valuable insights that we couldnt initially see from just looking at the raw data in spreadsheet form. We will use the ggplot2 package as it provides an easy way to customize your plots. ggplot2 is rooted in the data visualization theory known as The Grammar of Graphics (wilkinson2005?). At the most basic level, graphics/plots/charts (we use these terms interchangeably in this guide) provide a nice way for us to get a sense for how quantitative variables compare in terms of their center (where the values tend to be located) and their spread (how they vary around the center). Graphics should be designed to emphasize the findings and insight you want your audience to understand. This does however require a balancing act. On the one hand, you want to highlight as many meaningful relationships and interesting findings as possible; on the other you dont want to include so many as to overwhelm your audience. As we will see, plots/graphics also help us to identify patterns in our data. We will see that a common extension of these ideas is to compare the distribution of one quantitative variable (i.e., what the spread of a variable looks like or how the variable is distributed in terms of its values) as we go across the levels of a different categorical variable. Needed data and packages Lets load all the packages and data needed for this chapter (this assumes youve already installed them). Read Section 4.6 for information on how to install and load R packages. As before, we will load the atlas dataset as well. We will also plot some of the information in the package gapminder for some of our examples. atlas &lt;- readRDS(gzcon(url(&quot;https://raw.githubusercontent.com/jrm87/ECO3253_repo/master/data/atlas.rds&quot;))) library(ggplot2) library(dplyr) library(gapminder) 5.1 The Grammar of Graphics We begin with a discussion of a theoretical framework for data visualization known as The Grammar of Graphics, which serves as the foundation for the ggplot2 package. Think of how we construct sentences in English to form sentences by combining different elements, like nouns, verbs, particles, subjects, objects, etc. However, we cant just combine these elements in any arbitrary order; we must do so following a set of rules known as a linguistic grammar. Similarly to a linguistic grammar, The Grammar of Graphics define a set of rules for constructing statistical graphics by combining different types of layers. This grammar was created by Leland Wilkinson (wilkinson2005?) and has been implemented in a variety of data visualization software including R. 5.1.1 Components of the Grammar In short, the grammar tells us that: A statistical graphic is a mapping of data variables to aesthetic attributes of geometric objects. Specifically, we can break a graphic into the following three essential components: data: the data set composed of variables that we map. geom: the geometric object in question. This refers to the type of object we can observe in a plot. For example: points, lines, and bars. aes: aesthetic attributes of the geometric object. For example, x-position, y-position, color, shape, and size. Each assigned aesthetic attribute can be mapped to a variable in our data set. You might be wondering why we wrote the terms data, geom, and aes in a computer code type font. Well see very shortly that well specify the elements of the grammar in R using these terms. However, lets first break down the grammar with an example unrelated to our mobility data, but worry not! We will return to the atlas data. First 5.1.2 Gapminder data In February 2006, a statistician named Hans Rosling gave a TED talk titled The best stats youve ever seen where he presented global economic, health, and development data from the website gapminder.org. For example, for the 142 countries included from 2007, lets consider only the first 6 countries when listed alphabetically in Table 5.1. Table 5.1: Gapminder 2007 Data: First 6 of 142 countries Country Continent Life Expectancy Population GDP per Capita Afghanistan Asia 43.8 31889923 975 Albania Europe 76.4 3600523 5937 Algeria Africa 72.3 33333216 6223 Angola Africa 42.7 12420476 4797 Argentina Americas 75.3 40301927 12779 Australia Oceania 81.2 20434176 34435 Each row in this table corresponds to a country in 2007. For each row, we have 5 columns: Country: Name of country. Continent: Which of the five continents the country is part of. (Note that Americas includes countries in both North and South America and that Antarctica is excluded.) Life Expectancy: Life expectancy in years. Population: Number of people living in the country. GDP per Capita: Gross domestic product (in US dollars). Now consider Figure 5.1, which plots this data for all 142 countries in the data. Figure 5.1: Life Expectancy over GDP per Capita in 2007 Lets view this plot through the grammar of graphics: The data variable GDP per Capita gets mapped to the x-position aesthetic of the points. The data variable Life Expectancy gets mapped to the y-position aesthetic of the points. The data variable Population gets mapped to the size aesthetic of the points. The data variable Continent gets mapped to the color aesthetic of the points. Well see shortly that data corresponds to the particular data frame where our data is saved and a data variable corresponds to a particular column in the data frame. Furthermore, the type of geometric object considered in this plot are points. That being said, while in this example we are considering points, graphics are not limited to just points. Other plots involve lines while others involve bars. Lets summarize the three essential components of the Grammar in Table 5.2. Table 5.2: Summary of Grammar of Graphics for this plot data variable aes geom GDP per Capita x point Life Expectancy y point Population size point Continent color point 5.1.3 Other components There are other components of the Grammar of Graphics we can control as well. As you start to delve deeper into the Grammar of Graphics, youll start to encounter these topics more frequently. In this book however, well keep things simple and only work with the two additional components listed below: faceting breaks up a plot into small multiples corresponding to the levels of another variable (Section ??) position adjustments for barplots (Section 5.5) Other more complex components like scales and coordinate systems are left for a more advanced text such as R for Data Science (rds2016?). Generally speaking, the Grammar of Graphics allows for a high degree of customization of plots and also a consistent framework for easily updating and modifying them. 5.1.4 ggplot2 package In this book, we will be using the ggplot2 package for data visualization, which is an implementation of the Grammar of Graphics for R (R-ggplot2?). As we noted earlier, a lot of the previous section was written in a computer code type font. This is because the various components of the Grammar of Graphics are specified in the ggplot() function included in the ggplot2 package, which expects at a minimum as arguments (i.e. inputs): The data frame where the variables exist: the data argument. The mapping of the variables to aesthetic attributes: the mapping argument which specifies the aesthetic attributes involved. After weve specified these components, we then add layers to the plot using the + sign. The most essential layer to add to a plot is the layer that specifies which type of geometric object we want the plot to involve: points, lines, bars, and others. Other layers we can add to a plot include layers specifying the plot title, axes labels, visual themes for the plots, and facets (which well see in Section ??). Lets now put the theory of the Grammar of Graphics into practice. 5.2 Three Important Graphs - In order to keep things simple, we will only focus on 3 types of graphics in this section, each with a commonly given name. scatterplots histograms barplots We will discuss some variations of these plots, but with this basic repertoire of graphics in your toolbox you can visualize a wide array of different variable types. Note that certain plots are only appropriate for categorical variables and while others are only appropriate for quantitative variables. Youll want to quiz yourself often as we go along on which plot makes sense a given a particular problem or data set. 5.3 Scatterplots The simplest of the figrue we will cover are scatterplots, also called bivariate plots. They allow you to visualize the relationship between two numerical variables. While you may already be familiar with scatterplots, lets view them through the lens of the Grammar of Graphics. Specifically, we will visualize the relationship across neighborhoods between the following two numerical variables in the atlas data frame: kfr_pooled_p25: upward mobility for children with parents on the percentile 25 on the horizontal y axis med_hhinc2016: median household income in 2016 on the vertical x axis 5.3.1 Scatterplots via geom_point Lets now go over the code that will create the desired scatterplot, keeping in mind our discussion on the Grammar of Graphics in Section 5.1. Well be using the ggplot() function included in the ggplot2 package. ggplot(data = atlas, mapping = aes(x = med_hhinc2016, y = kfr_pooled_p25)) + geom_point() Lets break this down piece-by-piece: Within the ggplot() function, we specify two of the components of the Grammar of Graphics as arguments (i.e. inputs): The data frame to be atlas by setting data = atlas. The aesthetic mapping by setting aes(x = med_hhinc2016, y = kfr_pooled_p25). Specifically: the variable med_hhinc2016 maps to the x position aesthetic the variable kfr_pooled_p25 maps to the y position aesthetic We add a layer to the ggplot() function call using the + sign. The layer in question specifies the third component of the grammar: the geometric object. In this case the geometric object are points, set by specifying geom_point(). After running the above code, youll notice two outputs: a warning message and the graphic shown in Figure 5.2. Lets first unpack the warning message: y axis ## Warning: Removed 1372 rows containing missing values (geom_point). Figure 5.2: Median Household Income in 2016 vs Mobility for Children with Parents in Percentile 25 After running the above code, R returns a warning message alerting us to the fact that 1372 rows were ignored due to them being missing. For 1372 rows either the value for med_hhinc2016 or kfr_pooled_p25 or both were missing (recorded in R as NA), and thus these rows were ignored in our plot. Turning our attention to the resulting scatterplot in Figure 5.2, we see that a positive relationship exists between med_hhinc2016 and kfr_pooled_p25: as the median income level of the neighborhood increases, the mobility for children of parents in the percentile 25 tend to also increase. Before we continue, lets consider a few more notes on the layers in the above code that generated the scatterplot: Note that the + sign comes at the end of lines, and not at the beginning. Youll get an error in R if you put it at the beginning. When adding layers to a plot, you are encouraged to start a new line after the + so that the code for each layer is on a new line. As we add more and more layers to plots, youll see this will greatly improve the legibility of your code. To stress the importance of adding layers in particular the layer specifying the geometric object, consider Figure 5.3 where no layers are added. A not very useful plot! ggplot(data = atlas, mapping = aes(x = med_hhinc2016, y = kfr_pooled_p25)) Figure 5.3: Plot with No Layers Learning check (LC3.1) What are some other features of the plot that stand out to you? (LC3.2) Create a new scatterplot using different variables in the atlas data frame by modifying the example above. 5.3.2 Over-plotting Sometimes you end up with a large mass of points, which can cause some confusion as it is hard to tell the true number of points that are plotted. This is the result of a phenomenon called overplotting. As one may guess, this corresponds to values being plotted on top of each other over and over again. It is often difficult to know just how many values are plotted in this way when looking at a basic scatterplot as we have here. The main methods to address the issue of overplotting is: By adjusting the transparency of the points. Changing the transparency The main way of addressing overplotting is by changing the transparency of the points by using the alpha argument in geom_point(). By default, this value is set to 1. We can change this to any value between 0 and 1, where 0 sets the points to be 100% transparent and 1 sets the points to be 100% opaque. Note how the following code is identical to the code in Section 5.3 that created the scatterplot with overplotting, but with alpha = 0.2 added to the geom_point(): ggplot(data = atlas, mapping = aes(x = med_hhinc2016, y = kfr_pooled_p25))+ geom_point(alpha = 0.2) Figure 5.4: Delay scatterplot with alpha=0.2 The key feature to note in Figure 5.4 is that the transparency of the points is cumulative: areas with a high-degree of overplotting are darker, whereas areas with a lower degree are less dark. Note furthermore that there is no aes() surrounding alpha = 0.2. This is because we are not mapping a variable to an aesthetic attribute, but rather merely changing the default setting of alpha. In fact, youll receive an error if you try to change the second line above to read geom_point(aes(alpha = 0.2)). Learning check (LC3.3) Why is setting the alpha argument value useful with scatterplots? What further information does it give you that a regular scatterplot cannot? 5.3.3 Summary Scatterplots display the relationship between two numerical variables. They are among the most commonly used plots because they can provide an immediate way to see the trend in one variable versus another. However, if you try to create a scatterplot where either one of the two variables is not numerical, you might get strange results. Be careful! With medium to large data sets, you may need to play around with the different modifications one can make to a scatterplot. This tweaking is often a fun part of data visualization, since youll have the chance to see different relationships come about as you make subtle changes to your plots. Last thing: remember Figure 5.1? Here is the code of how we did that. It has a few pieces that you would need to figure out on your own, but you should get the essence by now. In particular, in the first line we pass on a filtered data for the year 2007. Ill explain that more in the next Section 6 on Data Wrangling. Still, try it out! ggplot(data = gapminder%&gt;%filter(year == 2007), mapping = aes(x=gdpPercap, y=lifeExp, size=pop, col=continent)) + geom_point() + labs(x = &quot;GDP per capita&quot;, y = &quot;Life expectancy&quot;) 5.4 Histograms Lets consider the kfr_pooled_p25 variable in the atlas data frame once again, but now we want to understand how the values of kfr_pooled_p25 distribute. In other words, for economic mobility for children from parents in the percentile 25: What are the smallest and largest values? What is the center value? How do the values spread out? What are frequent and infrequent values? One way to visualize this distribution of this single variable kfr_pooled_p25 is to plot what is know as a histogram. A histogram is a plot that visualizes the distribution of a numerical value as follows: We first cut up the x-axis into a series of bins, where each bin represents a range of values. For each bin, we count the number of observations that fall in the range corresponding to that bin. Then for each bin, we draw a bar whose height marks the corresponding count. Lets drill-down on an example of a histogram, shown in Figure 5.5. Figure 5.5: Example histogram. Observe that there are six bins of equal width between $ 30,000 and $ 60,000, thus we have three bins of width $ 5,000 each: one bin for the 30-35k range, and so on, until the bin for the 55-60k range. Since: The bin for the 30-35k range has a height of around 9000, this histogram is telling us that around 9000 neighborhoods in the US have an average mobility measure for children of parents in the percentile 25th of between $30,000 and $35,000. The remaining bins all have a similar interpretation. 5.4.1 Histograms via geom_histogram Lets now present the ggplot() code to plot your first histogram! Unlike with scatterplots and linegraphs, there is now only one variable being mapped in aes(): the single numerical variable temp. The y-aesthetic of a histogram gets computed for you automatically. Furthermore, the geometric object layer is now a geom_histogram() ggplot(data = atlas, mapping = aes(x = kfr_pooled_p25)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 1267 rows containing non-finite values (stat_bin). Figure 5.6: Histogram of mobility for children of p25 in the US. Lets unpack the messages R sent us first. The first message is telling us that the histogram was constructed using bins = 30, in other words 30 equally spaced bins. This is known in computer programming as a default value; unless you override this default number of bins with a number you specify, R will choose 30 by default. Well see in the next section how to change this default number of bins. The second message is telling us once again that there are some missing values: that because one row has a missing NA value for kfr_pooled_p25, it was omitted from the histogram. R is just giving us a friendly heads up that this was the case. Nows lets unpack the resulting histogram in Figure 5.6. Observe that values less than $12,000 as well as values above $80,000 are rather rare. However, because of the large number of bins, its hard to get a sense for which range of temperatures is covered by each bin; everything is one giant amorphous blob. So lets add white vertical borders demarcating the bins by adding a color = \"white\" argument to geom_histogram(): ggplot(data = atlas, mapping = aes(x = kfr_pooled_p25)) + geom_histogram(color = &quot;white&quot;) Figure 5.7: Histogram of mobility for children of p25 in the US with white borders. We can now better associate ranges of mobility to each of the bins. We can also vary the color of the bars by setting the fill argument. Run colors() to see all 657 possible choice of colors! ggplot(data = atlas, mapping = aes(x = kfr_pooled_p25)) + geom_histogram(color = &quot;white&quot;, fill = &quot;steelblue&quot;) Figure 5.8: Histogram of mobility for children of p25 in the US with white borders. 5.4.2 Adjusting the bins Lets now adjust the number of bins in our histogram in one of two methods: By adjusting the number of bins via the bins argument to geom_histogram(). By adjusting the width of the bins via the binwidth argument to geom_histogram(). Using the first method, we have the power to specify how many bins we would like to cut the x-axis up in. As mentioned in the previous section, the default number of bins is 30. We can override this default, to say 40 bins, as follows: ggplot(data = atlas, mapping = aes(x = kfr_pooled_p25)) + geom_histogram(bins = 40, color = &quot;white&quot;) Figure 5.9: Histogram with 40 bins. Using the second method, instead of specifying the number of bins, we specify the width of the bins by using the binwidth argument in the geom_histogram() layer. For example, lets set the width of each bin to be 10°F. ggplot(data = atlas, mapping = aes(x = kfr_pooled_p25)) + geom_histogram(binwidth = 4000, color = &quot;white&quot;) Figure 5.10: Histogram with binwidth 10. Learning check (LC3.4) What does changing the number of bins from 30 to 40 tell us about the distribution of mobility? (LC3.5) Would you classify the distribution of mobility as symmetric or skewed? (LC3.6) What would you guess is the center value in this distribution? Why did you make that choice? (LC3.7) Is this data spread out greatly from the center or is it close? Why? 5.4.3 Summary Histograms, unlike scatterplots, present information on only a single numerical variable. Specifically, they are visualizations of the distribution of the numerical variable in question. 5.5 Barplots Histograms are tools to visualize the distribution of numerical variables. Another common task is visualize the distribution of a categorical variable. This is a simpler task, as we are simply counting different categories, also known as levels, of a categorical variable. Often the best way to visualize these different counts, also known as frequencies, is with a barplot (also known as a barchart). One complication, however, is how your data is represented: is the categorical variable of interest pre-counted or not? For example, run the following code that manually creates two data frames representing a collection of fruit: 3 apples and 2 oranges. fruits &lt;- data_frame( fruit = c(&quot;apple&quot;, &quot;apple&quot;, &quot;orange&quot;, &quot;apple&quot;, &quot;orange&quot;) ) fruits_counted &lt;- data_frame( fruit = c(&quot;apple&quot;, &quot;orange&quot;), number = c(3, 2) ) We see both the fruits and fruits_counted data frames represent the same collection of fruit. Whereas fruits just lists the fruit individually ## # A tibble: 5 x 1 ## fruit ## &lt;chr&gt; ## 1 apple ## 2 apple ## 3 orange ## 4 apple ## 5 orange  fruits_counted has a variable count which represents pre-counted values of each fruit. ## # A tibble: 2 x 2 ## fruit number ## &lt;chr&gt; &lt;dbl&gt; ## 1 apple 3 ## 2 orange 2 Depending on how your categorical data is represented, youll need to use add a different geom layer to your ggplot() to create a barplot, as we now explore. 5.5.1 Barplots via geom_bar or geom_col Lets generate barplots using these two different representations of the same basket of fruit: 3 apples and 2 oranges. Using the fruits data frame where all 5 fruits are listed individually in 5 rows, we map the fruit variable to the x-position aesthetic and add a geom_bar() layer. ggplot(data = fruits, mapping = aes(x = fruit)) + geom_bar() Figure 5.11: Barplot when counts are not pre-counted However, using the fruits_counted data frame where the fruit have been pre-counted, we map the fruit variable to the x-position aesthetic as with geom_bar(), but we also map the count variable to the y-position aesthetic, and add a geom_col() layer. ggplot(data = fruits_counted, mapping = aes(x = fruit, y = number)) + geom_col() Figure 5.12: Barplot when counts are pre-counted Compare the barplots in Figures 5.11 and 5.12. They are identical because they reflect count of the same 5 fruit. However depending on how our data is saved, either pre-counted or not, we must add a different geom layer. When the categorical variable whose distribution you want to visualize is: Is not pre-counted in your data frame: use geom_bar(). Is pre-counted in your data frame, use geom_col() with the y-position aesthetic mapped to the variable that has the counts. Lets now go back to the atlas data frame and visualize the distribution of the categorical variable state. In other words, lets visualize the number of neighborhoodsin the data belonging to each state. Recall from Section 4.7.4 when you first explored the atlas data frame you saw that each row corresponds to a neighborhood. In other words the atlas data frame is more like the fruits data frame than the fruits_counted data frame above, and thus we should use geom_bar() instead of geom_col() to create a barplot. Much like a geom_histogram(), there is only one variable in the aes() aesthetic mapping: the variable state gets mapped to the x-position. ggplot(data = atlas, mapping = aes(x = state)) + geom_bar() Figure 5.13: Number of neighborhoods by State in the atlas data using geom_bar Observe in Figure 5.13 that state 06, which is California, has the most number of neighborhoods in the data. If you dont know which State FIPS code correspond to which State, you can see it here. For example: TX is State 48, while NC is State 37. Learning check (LC3.8) Why are histograms inappropriate for visualizing categorical variables? (LC3.9) What is the difference between histograms and barplots? (LC3.10) How many neighborhoods are there in Texas in the atlas data? 5.5.2 Summary Barplots are the preferred way of displaying the distribution of a categorical variable, or in other words the frequency with which the different categories called levels occur. They are easy to understand and make it easy to make comparisons across levels. When trying to visualize two categorical variables, you have many options: stacked barplots, side-by-side barplots, and faceted barplots. Depending on what aspect of the joint distribution you are trying to emphasize, you will need to make a choice between these three types of barplots. 5.6 Conclusion 5.6.1 Summary table Lets recap all five of the three main figures in Table ?? summarizing their differences. Using these 5NG, youll be able to visualize the distributions and relationships of variables contained in a wide array of datasets. 5.6.2 Argument specification Run the following two segments of code. First this: ggplot(data = atlas, mapping = aes(x = state)) + geom_bar() then this: ggplot(atlas, aes(x = state)) + geom_bar() Youll notice that that both code segments create the same barplot, even though in the second segment we omitted the data = and mapping = code argument names. This is because the ggplot() by default assumes that the data argument comes first and the mapping argument comes second. So as long as you specify the data frame in question first and the aes() mapping second, you can omit the explicit statement of the argument names data = and mapping =. Going forward for the rest of this book, all ggplot() will be like the second segment above: with the data = and mapping = explicit naming of the argument omitted and the default ordering of arguments respected. 5.6.3 Additional resources If you want to further unlock the power of the ggplot2 package for data visualization, you can check out RStudios Data Visualization with ggplot2 cheatsheet. This cheatsheet summarizes much more than what weve discussed in this chapter, in particular the many more than the 3 geom geometric objects we covered in this Chapter, while providing quick and easy to read visual descriptions. You can access this cheatsheet by going to the RStudio Menu Bar -&gt; Help -&gt; Cheatsheets -&gt; Data Visualization with ggplot2: Figure 5.14: Data Visualization with ggplot2 cheatsheat "],["wrangling.html", "Chapter 6 Data Wrangling 6.1 The pipe operator: %&gt;% 6.2 filter rows 6.3 mutate existing variables 6.4 Other verbs 6.5 Conclusion", " Chapter 6 Data Wrangling So far in our journey, weve seen how to look at data saved in data frames using the glimpse() and View() functions in Chapter 4 on and how to create data visualizations using the ggplot2 package in Chapter 5. In particular we studied what we term the the following three graphs: scatterplots via geom_point() histograms via geom_histogram() barplots via geom_bar() or geom_col() We created these visualizations using the Grammar of Graphics, which maps variables in a data frame to the aesthetic attributes of one the above 3 geometric objects. We can also control other aesthetic attributes of the geometric objects such as the size and color as seen in the Gapminder data example in Figure 5.1. Recall however in previous chapters we discussed that for two of our visualizations we needed transformed/modified versions of existing data frames. Recall for example the scatterplot of economic mobility only for neighborhoods in Wisconsin. In order to create this visualization, we needed to first pare down the atlas data frame to a new data frame atlas_wisconsin. consisting of only state == 55 neighborhoods using the filter() function. atlas_wisconsin &lt;- atlas %&gt;% filter(state == 55) ggplot(data = atlas_wisconsin, mapping = aes(x = med_hhinc2016, y = kfr_pooled_p25)) + geom_point() In this chapter, well introduce two very useful functions from the dplyr package that will allow you to take a data frame and filter() its existing rows to only pick out a subset of them. For example, the atlas_wisconsin data frame above. mutate() its existing columns/variables to create new ones. For example, convert dollars to log dollars. There are other functions we will not cover here, but that could be really useful for you to know if you ever get to work with data outside this course: summarize(), group_by(), arrange() and join(). I will leave those to learn on your own if you want to. You can check this very good explanation. Back to our functions! Notice how we used computer code font to describe the actions we want to take on our data frames. This is because the dplyr package for data wrangling that well introduce in this chapter has intuitively verb-named functions that are easy to remember. Well start by introducing the pipe operator %&gt;%, which allows you to combine multiple data wrangling verb-named functions into a single sequential chain of actions. Needed packages Lets load all the packages needed for this chapter (this assumes youve already installed them). If needed, read Section 4.6 for information on how to install and load R packages. We will also load the atlas data once more. library(dplyr) library(ggplot2) atlas &lt;- readRDS(gzcon(url(&quot;https://raw.githubusercontent.com/jrm87/ECO3253_repo/master/data/atlas.rds&quot;))) 6.1 The pipe operator: %&gt;% Before we start data wrangling, lets first introduce a very nifty tool that gets loaded along with the dplyr package: the pipe operator %&gt;%. Say you would like to perform a hypothetical sequence of operations on a hypothetical data frame x using hypothetical functions f(), g(), and h(): Take x then Use x as an input to a function f() then Use the output of f(x) as an input to a function g() then Use the output of g(f(x)) as an input to a function h() One way to achieve this sequence of operations is by using nesting parentheses as follows: h(g(f(x))) The above code isnt so hard to read since we are applying only three functions: f(), then g(), then h(). However, you can imagine that this can get progressively harder and harder to read as the number of functions applied in your sequence increases. This is where the pipe operator %&gt;% comes in handy. %&gt;% takes one output of one function and then pipes it to be the input of the next function. Furthermore, a helpful trick is to read %&gt;% as then. For example, you can obtain the same output as the above sequence of operations as follows: x %&gt;% f() %&gt;% g() %&gt;% h() You would read this above sequence as: Take x then Use this output as the input to the next function f() then Use this output as the input to the next function g() then Use this output as the input to the next function h() So while both approaches above would achieve the same goal, the latter is much more human-readable because you can read the sequence of operations line-by-line. But what are the hypothetical x, f(), g(), and h()? Throughout this chapter on data wrangling: The starting value x will be a data frame. For example: atlas. The sequence of functions, here f(), g(), and h(), will be a sequence of any number of the 6 data wrangling verb-named functions we listed in the introduction to this chapter. For example: filter(state == 55). The result will be the transformed/modified data frame that you want. For example: a data frame consisting of only the subset of rows in atlas corresponding to neighborhoods in the State of Wisconsin. Much like when adding layers to a ggplot() using the + sign at the end of lines, you form a single chain of data wrangling operations by combining verb-named functions into a single sequence with pipe operators %&gt;% at the end of lines. So continuing our example involving neighborhoods in Wisconsin, we form a chain using the pipe operator %&gt;% and save the resulting data frame in atlas_wisconsin: atlas_wisconsin &lt;- atlas %&gt;% filter(state == 55) Keep in mind, there are many more advanced data wrangling functions than just the 2 listed in the introduction to this chapter; youll see some examples of these in Section 6.4. However, just with these 2 verb-named functions youll be able to perform a broad array of data wrangling tasks for the rest of this course. 6.2 filter rows Figure 6.1: Diagram of The filter() function here works much like the Filter option in Microsoft Excel; it allows you to specify criteria about the values of a variable in your dataset and then filters out only those rows that match that criteria. We begin by focusing only on neighborhoods from the state of North Carolina. The state code (or airport code) for Portland, Oregon is 37. Run the following and look at the resulting spreadsheet to ensure that only neighborhoods from North Carolina are chosen here: atlas_nc&lt;- atlas %&gt;% filter(state == 37) View(atlas_nc) Note the following: The ordering of the commands: Take the atlas data frame then filter the data frame so that only thosewith state equals 37 are included. We test for equality using the double equal sign == and not a single equal sign =. In other words filter(state = 37) will yield an error. This is a convention across many programming languages. If you are new to coding, youll probably forget to use the double equal sign == a few times before you get the hang of it. You can use other mathematical operations beyond just == to form criteria: &gt; corresponds to greater than &lt; corresponds to less than &gt;= corresponds to greater than or equal to &lt;= corresponds to less than or equal to != corresponds to not equal to. The ! is used in many programming languages to indicate not. Furthermore, you can combine multiple criteria together using operators that make comparisons: | corresponds to or &amp; corresponds to and To see many of these in action, lets filter atlas for all rows that: Had a mean household income over 30,000 in the year 2000 and Are in North Carolina or Massachusetts; and Had an employment rate in the year 2000 of at least 80% Run the following: atlas_filter1 &lt;- atlas %&gt;% filter(hhinc_mean2000 &gt;30000 &amp; (state ==37 | state == 25) &amp; emp2000 &gt;= 0.8) View(atlas_filter1) Note that even though colloquially speaking one might say all neighborhoods from North Carolina and Massachusetts in terms of computer operations, we really mean all neighborhoods from North Carolina or Massachusetts. For a given row in the data, state can be 37, 25, or something else, but not 37 and 25 at the same time. Furthermore, note the careful use of parentheses around the state ==37 | state == 25. We can often skip the use of &amp; and just separate our conditions with a comma. In other words the code above will return the identical output atlas_filter1 as this code below: atlas_filter1 &lt;- atlas %&gt;% filter(hhinc_mean2000 &gt;30000, (state ==37 | state == 25), emp2000 &gt;= 0.8) View(atlas_filter1) Lets present another example that uses the ! not operator to pick rows that dont match a criteria. As mentioned earlier, the ! can be read as not. Here we are filtering rows corresponding to neighborhoods that are not in North Carolina or Massachusetts. atlas_not_nc_ma &lt;- atlas %&gt;% filter(!(state ==37 | state == 25)) View(atlas_not_nc_ma) Again, note the careful use of parentheses around the (state ==37 | state == 25). If we didnt use parentheses as follows: atlas %&gt;% filter(!state ==37 | state == 25) We would be returning all neighborhoods not in 37 or those in 25, which is an entirely different resulting data frame. Now say we have a large list of airports we want to filter for, say NC (37), MA (25), FL(12) and PA(42). We could continue to use the | or operator as so: atlas_many_states &lt;- atlas %&gt;% filter(state ==37 | state == 25 | state == 12 | state == 42) View(atlas_many_states) but as we progressively include more airports, this will get unwieldy. A slightly shorter approach uses the %in% operator: atlas_many_states &lt;- atlas %&gt;% filter(state %in% c(37, 25, 12, 42)) View(atlas_many_states) What this code is doing is filtering atlas for all neighborhoods where state is in the list of airports c(37, 25, 12, 42). Recall from Chapter 4 that the c() function combines or concatenates values in a vector of values. Both outputs of atlas_many_states are the same, but as you can see the latter takes much less time to code. As a final note we point out that filter() should often be among the first verbs you apply to your data. This cleans your dataset to only those rows you care about, or put differently, it narrows down the scope of your data frame to just the observations your care about. 6.3 mutate existing variables Figure 6.2: Mutate diagram from Data Wrangling with dplyr and tidyr cheatsheet A common transformation of data is to create/compute new variables based on existing ones. For example, say you are more comfortable thinking of income in terms of the logarithm of income instead of in dollars. We will apply this to the variable hhinc_mean2000 (the mean household income in 2000). You want to implement the following formula: \\[ \\text{income} = log(\\text{income}) \\] We can apply this formula to the hhinc_mean2000 variable using the mutate() function, which takes existing variables and mutates them to create new ones. atlas &lt;- atlas %&gt;% mutate(log_hhinc_mean2000= log(hhinc_mean2000)) View(atlas) Note that we have overwritten the original atlas data frame with a new version that now includes the additional variable log_hhinc_mean2000. In other words, the mutate() command outputs a new data frame which then gets saved over the original atlas data frame. Furthermore, note how in mutate() we used log_hhinc_mean2000= log(hhinc_mean2000) to create a new variable log_hhinc_mean2000. Why did we overwrite the data frame atlas instead of assigning the result to a new data frame like atlas_new, but on the other hand why did we not overwrite hhinc_mean2000, but instead created a new variable called temp_in_C? As a rough rule of thumb, as long as you are not losing original information that you might need later, its acceptable practice to overwrite existing data frames. On the other hand, had we used mutate(hhinc_mean2000 = log(hhinc_mean2000) instead of mutate(log_hhinc_mean2000= log(hhinc_mean2000)), we would have overwritten the original variable hhinc_mean2000 and lost its values. 6.4 Other verbs Here are some other useful data wrangling verbs that might come in handy: select() only a subset of variables/columns rename() variables/columns to have new names Return only the top_n() values of a variable 6.4.1 select variables Figure 6.3: Select diagram from Data Wrangling with dplyr and tidyr cheatsheet Weve seen that the atlas data frame contains 62 different variables. You can identify the names of these 19 variables by running the glimpse() function from the dplyr package: glimpse(atlas) However, say you only need two of these variables, say state and emp2000. You can select() these two variables: atlas %&gt;% select(state, emp2000) This function makes exploring data frames with a very large number of variables easier for humans to process by restricting consideration to only those we care about, like our example with state and emp2000 above. This might make viewing the dataset using the View() spreadsheet viewer more digestible. However, as far as the computer is concerned, it doesnt care how many additional variables are in the data frame in question, so long as state and state are included. Lastly, the helper functions starts_with(), ends_with(), and contains() can be used to select variables/column that match those conditions. For example: atlas_begin_kfr &lt;- atlas %&gt;% select(starts_with(&quot;kfr&quot;)) atlas_begin_kfr 6.4.2 rename variables Another useful function is rename(), which as you may have guessed renames one column to another name. Suppose we want emp2000 and popdensity2010 to be employmentrate2000 and populationdensity2010 instead in the atlas data frame: atlas &lt;- atlas %&gt;% rename(populationdensity2010 = popdensity2010, employmentrate2000 = emp2000) glimpse(atlas) Note that in this case we used a single = sign within the rename(). This is because we are not testing for equality like we would using ==, but instead we want to assign a new variable populationdensity2010 to have the same values as popdensity2010 and then delete the variable popdensity2010. Its easy to forget if the new name comes before or after the equals sign. I usually remember this as New Before, Old After or NBOA. 6.4.3 top_n values of a variable We can also return the top n values of a variable using the top_n() function. For example, we can return a data frame of the top neighborhoods by mobility for children from parents in the percentile 25. Observe that we set the number of values to return to n = 10 and wt = kfr_pooled_p25 to indicate that we want the rows of corresponding to the top 10 values of kfr_pooled_p25. See the help file for top_n() by running ?top_n for more information. atlas %&gt;% top_n(n = 10, wt = kfr_pooled_p25) Lets further arrange() these results in descending order of kfr_pooled_p25: atlas %&gt;% top_n(n = 10, wt = kfr_pooled_p25) %&gt;% arrange(desc(kfr_pooled_p25)) You can read more about the function arrange() here, but the logic is fairly simple. We are organizing the neighborhoods in descending order in the variable kfr_pooled_p25. 6.5 Conclusion 6.5.1 Additional resources If you want to further unlock the power of the dplyr package for data wrangling, we suggest you that you check out RStudios Data Transformation with dplyr cheatsheet. This cheatsheet summarizes much more than what weve discussed in this chapter, in particular more-intermediate level and advanced data wrangling functions, while providing quick and easy to read visual descriptions. You can access this cheatsheet by going to the RStudio Menu Bar -&gt; Help -&gt; Cheatsheets -&gt; Data Transformation with dplyr: Figure 6.4: Data Transformation with dplyr cheatsheat On top of data wrangling verbs and examples we presented in this section, if youd like to see more examples of using the dplyr package for data wrangling check out Chapter 5 of Garrett Grolemund and Hadley Wickhams and Garretts book (rds2016?). "],["regression.html", "Chapter 7 Simple and Multivariate Regression 7.1 One numerical explanatory variable 7.2 Related topics 7.3 Two numerical explanatory variables 7.4 How to read p-values 7.5 Conclusion", " Chapter 7 Simple and Multivariate Regression As a reminder, this is not a course on econometrics, so I will not delve deep into the issues of regression. Nonetheless, there are basic intuitions we can construct when analyzing the data with these models. I will summarize the main issues in this chapter. Nonetheless, feel free to dive deeper here. Now that we are equipped with data visualization skills from Chapter 5, and data wrangling skills from Chapter 6, we now proceed with data modeling. The fundamental premise of data modeling is to make explicit the relationship between: an outcome variable \\(y\\), also called a dependent variable and an explanatory/predictor variable \\(x\\), also called an independent variable or covariate. Another way to state this is using mathematical terminology: we will model the outcome variable \\(y\\) as a function of the explanatory/predictor variable \\(x\\). Why do we have two different labels, explanatory and predictor, for the variable \\(x\\)? Thats because roughly speaking data modeling can be used for two purposes: Modeling for prediction: You want to predict an outcome variable \\(y\\) based on the information contained in a set of predictor variables. You dont care so much about understanding how all the variables relate and interact, but so long as you can make good predictions about \\(y\\), youre fine. For example, if we know many individuals risk factors for lung cancer, such as smoking habits and age, can we predict whether or not they will develop lung cancer? Here we wouldnt care so much about distinguishing the degree to which the different risk factors contribute to lung cancer, but instead only on whether or not they could be put together to make reliable predictions. Modeling for explanation: You want to explicitly describe the relationship between an outcome variable \\(y\\) and a set of explanatory variables, determine the significance of any found relationships, and have measures summarizing these. Continuing our example from above, we would now be interested in describing the individual effects of the different risk factors and quantifying the magnitude of these effects. One reason could be to design an intervention to reduce lung cancer cases in a population, such as targeting smokers of a specific age group with an advertisement for smoking cessation programs. In this course, well focus more on this latter purpose. Data modeling is used in a wide variety of fields, including statistical inference, causal inference, artificial intelligence, and machine learning. There are many techniques for data modeling, such as tree-based models, neural networks and deep learning, and supervised learning. We will briefly touch on those at the end of the course. For now, well focus on one particular technique: linear regression, one of the most commonly-used and easy-to-understand approaches to modeling. Recall our discussion in Subsection 4.7.4 on numerical and categorical variables. Linear regression involves: an outcome variable \\(y\\) that is numerical and explanatory variables \\(x_i\\) (e.g. \\(x_1, x_2, ...\\)) that are either numerical or categorical. With linear regression there is always only one numerical outcome variable \\(y\\) but we have choices on both the number and the type of explanatory variables to use. Were going to cover the following regression scenarios: Only one explanatory variable: simple linear regression In Section 7.1, this explanatory variable will be a single numerical explanatory variable \\(x\\). In the next Section on Experiments we will use an explanatory variable that is a categorical explanatory variable \\(x\\) (being in a treatment or in a control). This is an example of a broader class of regressions using explanatory variables, but we will see those later. More than one explanatory variable: multiple regression Well focus on two numerical explanatory variables, \\(x_1\\) and \\(x_2\\), in Section 7.3. Well use one numerical and one categorical explanatory variable. As before, we will see this in later sections. Well study the first of each of these two types of regression scenarios using the atlas data! Needed packages Lets now load all the packages needed for this chapter (this assumes youve already installed them). In this chapter we introduce some new packages: The tidyverse umbrella package. You can load the tidyverse package by running library(tidyverse), which loads several other packages we have used thus far, including: ggplot2 for data visualization dplyr for data wrangling As well as the more advanced purrr, tidyr, readr, tibble, stringr, and forcats packages The skimr (R-skimr?) package, which provides a simple-to-use function to quickly compute a wide array of commonly-used summary statistics. If needed, read Section 4.6 for information on how to install and load R packages. We will also load the atlas dataset. library(tidyverse) library(skimr) atlas &lt;- readRDS(gzcon(url(&quot;https://raw.githubusercontent.com/jrm87/ECO3253_repo/master/data/atlas.rds&quot;))) 7.1 One numerical explanatory variable Lets say you want to explore what correlates with economic mobility for children of poor parents (those at percentile 25 of the parental income distribution). To try to keep things simple, you want to understand if neighborhoods that have more jobs, tend to provide more or less economic mobility for this population. We will try to keep things simple for now and explain the differences in mobility with the average job growth rate between 2004 and 2013 (a rough measurement of how much jobs grow in that area) correlates with economic mobility for children from poor parents Well achieve ways to address these questions by modeling the relationship between these two variables with a particular kind of linear regression called simple linear regression. Simple linear regression is the most basic form of linear regression. With it we have A numerical outcome variable \\(y\\). In this case, economic mobility for children of parents in the percentile 25. A single numerical explanatory variable \\(x\\). In this case, the average annual job growth rate between 2004 and 2013 for that neighborhood. 7.1.1 Exploratory data analysis A crucial step before doing any kind of modeling or analysis is performing an exploratory data analysis, or EDA, of all our data. Exploratory data analysis can give you a sense of the distribution of the data and whether there are outliers and/or missing values. Most importantly, it can inform how to build your model. There are many approaches to exploratory data analysis; here are three: Most fundamentally: just looking at the raw values, in a spreadsheet for example. While this may seem trivial, many people ignore this crucial step! Computing summary statistics like means, medians, and standard deviations. Creating data visualizations. Lets load the atlas data, select only a subset of the variables (including the names, which are the codes of tract, country and state), and look at the raw values. Recall you can look at the raw values by running View() in the console in RStudio to pop-up the spreadsheet viewer with the data frame of interest as the argument to View(). Here, however, we present only a snapshot of five randomly chosen rows: atlas_mob_jobs &lt;- atlas %&gt;% select(tract, county, state, kfr_pooled_p25, ann_avg_job_growth_2004_2013) atlas_mob_jobs %&gt;% sample_n(5) Table 7.1: Random sample of 5 neighborhoods tract county state kfr_pooled_p25 ann_avg_job_growth_2004_2013 90200 61 37 28016 0.020 602004 37 6 30825 0.000 16000 13 34 48741 0.030 40101 5 44 35800 -0.024 211400 79 42 34588 0.013 You can see the full description of each variable in the Section on Data Description for Project 1 and the bulk of Lecture 1 on the Geography of Economic Mobility, but lets summarize what each variable represents: tract: FIPS code for the tract country: FIPS code for the county state: FIPS code for the state kfr_pooled_p25: Numerical variable of the average income that children that grew up in that neighborhood from parents in the percentile 25th receive when adults. ann_avg_job_growth_2004_2013: Numerical variable of average job growth between 2004 and 2013. Notice that the numbers on job growth need to be multiplied by 100 to get percentages. That is, 0 is 0\\%, while 1 would be 100\\%. An alternative way to look at the raw data values is by choosing a random sample of the rows in atlas_mob_jobs by piping it into the sample_n() function from the dplyr package. Here we set the size argument to be 5, indicating that we want a random sample of 5 rows. We display the results in Table 7.2. Note that due to the random nature of the sampling, you will likely end up with a different subset of 5 rows. atlas_mob_jobs %&gt;% sample_n(size = 5) Table 7.2: A random sample of 5 neighborhoods tract county state kfr_pooled_p25 ann_avg_job_growth_2004_2013 9200 157 47 34943 -0.090 2605 99 6 32270 0.014 15100 71 36 29352 -0.102 950700 17 16 33694 0.024 52902 183 37 33021 -0.017 Now that weve looked at the raw values in our atlas_mob_jobs data frame and got a preliminary sense of the data, lets move on to the next common step in an exploratory data analysis: computing summary statistics. Lets start by computing the mean and median of our numerical outcome variable kfr_pooled_p25 and our numerical explanatory variable on ob growth denoted as ann_avg_job_growth_2004_2013. Well do this by using the summarize() function from dplyr along with the mean() and median() summary functions, in a function that although we did not cover, is fairly intuitive: summarize. It will take some data, and summarize it according to what you ask. ## # A tibble: 1 x 4 ## mean_ann_avg_job_gro~ mean_kfr_pooled_~ median_ann_avg_job_~ median_kfr_poole~ ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0153 34443. 0.00850 33733. atlas_mob_jobs %&gt;% summarize(mean_ann_avg_job_growth_2004_2013 = mean(ann_avg_job_growth_2004_2013, na.rm=TRUE), mean_kfr_pooled_p25 = mean(kfr_pooled_p25, na.rm=TRUE), median_ann_avg_job_growth_2004_2013 = median(ann_avg_job_growth_2004_2013, na.rm=TRUE), median_kfr_pooled_p25 = median(kfr_pooled_p25, na.rm=TRUE)) However, what if we want other summary statistics as well, such as the standard deviation (a measure of spread), the minimum and maximum values, and various percentiles? Typing out all these summary statistic functions in summarize() would be long and tedious. Instead, lets use the convenient skim() function from the skimr package. This function takes in a data frame, skims it, and returns commonly used summary statistics. Lets take our atlas_mob_jobs data frame, select() only the outcome and explanatory variables mobility kfr_pooled_p25 and job growth ann_avg_job_growth_2004_2013, and pipe them into the skim() function: atlas_mob_jobs %&gt;% select(kfr_pooled_p25, ann_avg_job_growth_2004_2013) %&gt;% skim() Skim summary statistics n obs: 73278 n variables: 2  Variable type:numeric  variable missing complete n mean sd p0 p25 p50 p75 p100 kfr_pooled_p25 1267 0.983 34443. 8169. 0 28973 33733. 39167. 105732. ann_avg_job_growth_2004_2013 2614 0.964 0.0153 0.0762 -0.607 -0.0189 0.00850 0.0410 1.34 (Note that for formatting purposes, the inline histogram that is usually printed with skim() has been removed. This can be done by running skim_with(numeric = list(hist = NULL)) prior to using the skim() function as well.) For our two numerical variables mobility kfr_pooled_p25 and job growth ann_avg_job_growth_2004_2013 it returns: missing: the number of missing values complete: the number of non-missing or complete values n: the total number of values mean: the average sd: the standard deviation p0: the 0th percentile: the value at which 0% of observations are smaller than it (the minimum value) p25: the 25th percentile: the value at which 25% of observations are smaller than it (the 1st quartile) p50: the 50th percentile: the value at which 50% of observations are smaller than it (the 2nd quartile and more commonly called the median) p75: the 75th percentile: the value at which 75% of observations are smaller than it (the 3rd quartile) p100: the 100th percentile: the value at which 100% of observations are smaller than it (the maximum value) Looking at this output, we get an idea of how the values of both variables distribute. For example, the mean mobility was $ 34,443 whereas the mean job growth was 0.0153 or 1.53%. Furthermore, the middle 50% of mobility were between $28,973 and $39,167 (the first and third quartiles) whereas the middle 50% of job growth were between -0.0189 (-1.89%) and 0.0410 (+4.10%). The skim() function only returns what are known as univariate summary statistics: functions that take a single variable and return some numerical summary of that variable. However, there also exist bivariate summary statistics: functions that take in two variables and return some summary of those two variables. In particular, when the two variables are numerical, we can compute the correlation coefficient. Generally speaking, coefficients are quantitative expressions of a specific phenomenon. A correlation coefficient is a quantitative expression of the strength of the linear relationship between two numerical variables. Its value ranges between -1 and 1 where: -1 indicates a perfect negative relationship: As the value of one variable goes up, the value of the other variable tends to go down following along a straight line. 0 indicates no relationship: The values of both variables go up/down independently of each other. +1 indicates a perfect positive relationship: As the value of one variable goes up, the value of the other variable tends to go up as well in a linear fashion. Figure 7.1 gives examples of different correlation coefficient values for hypothetical numerical variables \\(x\\) and \\(y\\). We see that while for a correlation coefficient of -0.75 there is still a negative relationship between \\(x\\) and \\(y\\), it is not as strong as the negative relationship between \\(x\\) and \\(y\\) when the correlation coefficient is -1. Figure 7.1: Different correlation coefficients The correlation coefficient is computed using the cor() function, where the inputs to the function are the two numerical variables for which we want to quantify the strength of the linear relationship. Note, however, that because the dataset has missing values for some neighborhoods, if you run cor() without the option use = \"pairwise.complete.obs\", you will get NA - which is a missing value. See that we add this option below: atlas_mob_jobs %&gt;% summarise(correlation = cor(kfr_pooled_p25, ann_avg_job_growth_2004_2013, use = &quot;pairwise.complete.obs&quot;)) ## # A tibble: 1 x 1 ## correlation ## &lt;dbl&gt; ## 1 0.0716 You can also use the cor() function directly instead of using it inside summarise, but you will need to use the $ syntax to access the specific variables within a data frame (See Subsection 4.7.4): cor(x = atlas_mob_jobs$ann_avg_job_growth_2004_2013, y = atlas_mob_jobs$kfr_pooled_p25, use = &quot;pairwise.complete.obs&quot;) ## [1] 0.0716 In our case, the correlation coefficient of NA indicates that the relationship between mobility and job growth is weakly positive There is a certain amount of subjectivity in interpreting correlation coefficients, especially those that arent close to -1, 0, and 1. For help developing such intuition and more discussion on the correlation coefficient see Subsection 7.2.1 below. Lets now proceed by visualizing this data. Since both the kfr_pooled_p25 and ann_avg_job_growth_2004_2013 variables are numerical, a scatterplot is an appropriate graph to visualize this data. Lets do this using geom_point() and set informative axes labels and title and display the result in Figure 7.2. ggplot(atlas_mob_jobs, aes(x = ann_avg_job_growth_2004_2013, y = kfr_pooled_p25)) + geom_point(alpha = 0.2) + labs(x = &quot;Average Job Growth Rate 2004-2013&quot;, y = &quot;Mobility for Children of Parents in P25&quot;, title = &quot;Relationship of mobility and job growth&quot;) Figure 7.2: Economic mobility for children of parents at percentile 25 across the US Observe the following: Most average job growth rates lie between -0.25 (-25%) and 0.25 (+25%). Most mobility outcomes lie between $18,000 and $50,000. Recall our earlier computation of the correlation coefficient, which describes the strength of the linear relationship between two numerical variables. Looking at Figure 7.2, it is not immediately apparent that these two variables are positively related. This is to be expected given the positive, but rather weak (close to 0), correlation coefficient of NA. Going back to scatterplot in Figure 7.2, lets improve on it by adding a regression line in Figure 7.3. This is easily done by adding a new layer to the ggplot code that created Figure 7.2: + geom_smooth(method = \"lm\"). A regression line is a best fitting line in that of all possible lines you could draw on this plot, it is best in terms of some mathematical criteria. There is a formal definition for what best means, but we do not need to go into detail here. Suffice to say, this line minimizes errors in the prediction in the sense that the square of the distance between our prediction (the line) and the real data. ggplot(atlas_mob_jobs, aes(x = ann_avg_job_growth_2004_2013, y = kfr_pooled_p25)) + geom_point(alpha = 0.2) + labs(x = &quot;Average Job Growth Rate 2004-2013&quot;, y = &quot;Mobility for Children of Parents in P25&quot;, title = &quot;Relationship of mobility and job growth&quot;) + geom_smooth(method = &quot;lm&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; Figure 7.3: Regression line When viewed on this plot, the regression line is a visual summary of the relationship between two numerical variables, in our case the outcome variable kfr_pooled_p25 and the explanatory variable ann_avg_job_growth_2004_2013. The positive slope of the blue line is consistent with our observed correlation coefficient of NA suggesting that there is a positive relationship between kfr_pooled_p25 and ann_avg_job_growth_2004_2013. Well see later however that while the correlation coefficient is not equal to the slope of this line, they always have the same sign: positive or negative. You can barely see it. perhaps, but the blue line has some very thin grey bands surrounding it. What are those? These are standard error bands, which can be thought of as error/uncertainty bands. Lets skip this idea for now and suppress these grey bars by adding the argument se = FALSE to geom_smooth(method = \"lm\"). Well briefly introduce standard errors below, but we can skip that for now. ggplot(atlas_mob_jobs, aes(x = ann_avg_job_growth_2004_2013, y = kfr_pooled_p25)) + geom_point(alpha = 0.2) + labs(x = &quot;Average Job Growth Rate 2004-2013&quot;, y = &quot;Mobility for Children of Parents in P25&quot;, title = &quot;Relationship of mobility and job growth&quot;) + geom_smooth(method = &quot;lm&quot;, se = FALSE) ## `geom_smooth()` using formula &#39;y ~ x&#39; Figure 7.4: Regression line without error bands Learning check (LC5.1) Conduct a new exploratory data analysis with the same outcome variable \\(y\\) being kfr_pooled_p25 but with poor_share2010 as the new explanatory variable \\(x\\). Remember, this involves three things: Looking at the raw values. Computing summary statistics of the variables of interest. Creating informative visualizations. What can you say about the relationship between the share of people living under the poverty line in 2010 and mobility measures for children of parents in p25 based on this exploration? 7.1.2 Simple linear regression You may recall from secondary school / high school algebra, in general, the equation of a line is \\(y = a + bx\\), which is defined by two coefficients. Recall we defined this earlier as quantitative expressions of a specific property of a phenomenon. These two coefficients are: the intercept coefficient \\(a\\), or the value of \\(y\\) when \\(x = 0\\), and the slope coefficient \\(b\\), or the increase in \\(y\\) for every increase of one in \\(x\\). However, when defining a line specifically for regression, like the blue regression line in Figure 7.4, we use slightly different notation: the equation of the regression line is \\(\\widehat{y} = b_0 + b_1 \\cdot x\\) where the intercept coefficient is \\(b_0\\), or the value of \\(\\widehat{y}\\) when \\(x=0\\), and the slope coefficient \\(b_1\\), or the increase in \\(\\widehat{y}\\) for every increase of one in \\(x\\). Why do we put a hat on top of the \\(y\\)? Its a form of notation commonly used in regression, which well introduce in the next Subsection ?? when we discuss fitted values. For now, lets ignore the hat and treat the equation of the line as you would from secondary school / high school algebra recognizing the slope and the intercept. We know looking at Figure 7.4 that the slope coefficient corresponding to ann_avg_job_growth_2004_2013 should be positive. Why? Because as ann_avg_job_growth_2004_2013 increases, professors tend to roughly have higher teaching evaluation kfr_pooled_p25. However, what are the specific values of the intercept and slope coefficients? Lets not worry about computing these by hand, but instead let the computer do the work for us. Specifically, lets use R! Lets get the value of the intercept and slope coefficients by outputting something called the linear regression table. We will fit the linear regression model to the data using the lm() function and save this to mob_model. lm stands for linear model. When we say fit, we are saying find the best fitting line to this data. The lm() function that fits the linear regression model is typically used as lm(y ~ x, data = data_frame_name) where: y is the outcome variable, followed by a tilde (~). This is likely the key to the left of 1 on your keyboard. In our case, y is set to kfr_pooled_p25. x is the explanatory variable. In our case, x is set to ann_avg_job_growth_2004_2013. We call the combination y ~ x a model formula. data_frame_name is the name of the data frame that contains the variables y and x. In our case, data_frame_name is the atlas_mob_jobs data frame. mob_model &lt;- lm(kfr_pooled_p25 ~ ann_avg_job_growth_2004_2013, data = atlas_mob_jobs) mob_model ## ## Call: ## lm(formula = kfr_pooled_p25 ~ ann_avg_job_growth_2004_2013, data = atlas_mob_jobs) ## ## Coefficients: ## (Intercept) ann_avg_job_growth_2004_2013 ## 34276 7783 This output is telling us that the Intercept coefficient \\(b_0\\) of the regression line is 3.8803, and the slope coefficient for by_avg is 0.0666. Therefore the blue regression line in Figure 7.4 is \\[\\widehat{\\text{mobilityp25}} = b_0 + b_{\\text{avg job growth}} \\cdot\\text{avg job growth} = 34276.27 + 7782.82\\cdot\\text{ avg job growth}\\] where The intercept coefficient \\(b_0 = 34276.27\\) means for instructors that had a hypothetical average job growth of 0, we would expect them to have on average mobility of $ 34,276. Of more interest is the slope coefficient associated with ann_avg_job_growth_2004_2013: \\(b_{\\text{bty avg}} = +7782.82\\). This is a numerical quantity that summarizes the relationship between the outcome and explanatory variables. Note that the sign is positive, suggesting a positive relationship between job growth and mobility, meaning as average job growth goes up, so does the mobility measure. The slopes precise interpretation is: For every increase of 1 unit in ann_avg_job_growth_2004_2013, there is an associated increase of, on average, 7782.82 units of kfr_pooled_p25. A very important point to mention here is that you need to recall that the variable ann_avg_job_growth_2004_2013 is coded such that a 1 unit increment is equivalent to increasing the average annual job growth by 100%. Given that changing the average growth rate by 100% is a very large change, perhaps we should choose a smaller change to get a sense of how much the correlation explains mobility. That is, we can interpret the previous set of results by dividing the coefficient on average job growth rate by 100, and saying: For every increase of 1% in the average job growth rate ann_avg_job_growth_2004_2013, there is an associated increase of, on average, 77.82 units of mobility. Such interpretations need be carefully worded: We only stated that there is an associated increase, and not necessarily a causal increase. For example, perhaps its not that job growth directly affects mobility, but instead areas with higher mobility attract firms to locate there, thereby increasing job creation. Avoiding such reasoning can be summarized by the adage correlation is not necessarily causation. In other words, just because two variables are correlated, it doesnt mean one directly causes the other. We discuss these ideas more in Subsection 7.2.2 and in following chapters. We say that this associated increase is on average 77.82 dollars of mobility kfr_pooled_p25 and not that the associated increase is exactly 77.82 dollars of kfr_pooled_p25 across all values of ann_avg_job_growth_2004_2013. This is because the slope is the average increase across all points as shown by the regression line in Figure 7.4. Now that weve learned how to compute the equation for the blue regression line in Figure 7.4 and interpreted all its terms, lets take our modeling one step further. This time after fitting the model using the lm(), lets get something called the regression table using the summary() function: # Fit regression model: mob_model &lt;- lm(kfr_pooled_p25 ~ ann_avg_job_growth_2004_2013, data = atlas_mob_jobs) # Get regression results: summary(mob_model) ## ## Call: ## lm(formula = kfr_pooled_p25 ~ ann_avg_job_growth_2004_2013, data = atlas_mob_jobs) ## ## Residuals: ## Min 1Q Median 3Q Max ## -35002 -5437 -719 4656 69932 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 34276.3 31.2 1097 &lt;0.0000000000000002 ## ann_avg_job_growth_2004_2013 7782.8 409.2 19 &lt;0.0000000000000002 ## ## (Intercept) *** ## ann_avg_job_growth_2004_2013 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8110 on 70230 degrees of freedom ## (3046 observations deleted due to missingness) ## Multiple R-squared: 0.00512, Adjusted R-squared: 0.00511 ## F-statistic: 362 on 1 and 70230 DF, p-value: &lt;0.0000000000000002 Note how we took the output of the model fit saved in mob_model and used it as an input to the subsequent summary() function. The raw output of the summary() function above gives lots of information about the regression model that we wont cover in this introductory course (e.g., Multiple R-squared, F-statistic, etc.). We will only consider the Coefficients section of the output. We can print these relevant results only by accessing the coefficients object stored in the summary results. summary(mob_model)$coefficients Table 7.3: Linear regression table Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 34276 31.2 1097 0 ann_avg_job_growth_2004_2013 7783 409.2 19 0 For now since we are only using regression as an exploratory data analysis tool, we will only focus on the Estimate column that contains the estimates of the intercept and slope for the best fit line for our data. The remaining three columns refer to statistical concepts known as standard errors, t-statistics, and p-values. They are all interrelated, but we will briefly only talk about p-values at the end of this Chapter. For now, we can skip that.. Learning check (LC5.2) Fit a new simple linear regression using lm(kfr_pooled_p25 ~ poor_share2010, data = atlas_mob_jobs) where poor_share2010 is the new explanatory variable \\(x\\). Get information about the best-fitting line from the regression table by applying the summary() function. How do the regression results match up with the results from your exploratory data analysis above? Investigating the residuals of these linear regressions is usually a good idea, but I will leave that for your Econometric classes. If you want to see how to explore the residuals in R, you can look here. 7.2 Related topics 7.2.1 Correlation coefficient Lets re-plot Figure 7.1, but now consider a broader range of correlation coefficient values in Figure 7.5. Figure 7.5: Different Correlation Coefficients As we suggested in Subsection 7.1.1, interpreting coefficients that are not close to the extreme values of -1 and 1 can be subjective. To help develop your sense of correlation coefficients, we suggest you play the following 80s-style video game called Guess the correlation at http://guessthecorrelation.com/. 7.2.2 Correlation is not necessarily causation Youll note throughout this chapter weve been very cautious in making statements of the associated effect of explanatory variables on the outcome variables, for example our statement from Subsection 7.1.2 that for every increase of 1 unit in ann_avg_job_growth_2004_2013, there is an associated increase of, on average, 7782.823 units of kfr_pooled_p25. We say this because we are careful not to make causal statements. So while average job growth rate ann_avg_job_growth_2004_2013 is positively correlated with teaching kfr_pooled_p25, it does not imply that it directly cause mobility to increase. For example, lets say a neighbrhood has their ann_avg_job_growth_2004_2013 increase, but only after taking steps to try to boost public investment in the area. Does this mean that they will suddenly be a better neighborhood for kids of low income parents? Maybe? Here is another example, a not-so-great medical doctor goes through their medical records and finds that patients who slept with their shoes on tended to wake up more with headaches. So this doctor declares Sleeping with shoes on cause headaches! Figure 7.6: Does sleeping with shoes on cause headaches? However as some of you might have guessed, if someone is sleeping with their shoes on its probably because they are intoxicated. Furthermore, drinking more tends to cause more hangovers, and hence more headaches. In this instance, alcohol is whats known as a confounding/lurking variable. It lurks behind the scenes, confounding or making less apparent, the causal effect (if any) of sleeping with shoes on with waking up with a headache. We can summarize this notion in Figure 7.7 with a causal graph where: Y: Is an outcome variable, here waking up with a headache. X: Is a treatment variable whose causal effect we are interested in, here sleeping with shoes on. Figure 7.7: Causal graph. So for example, many such studies use regression modeling where the outcome variable is set to Y and the explanatory/predictor variable is X, much as youve started learning how to do in this chapter. However, Figure 7.7 also includes a third variable with arrows pointing at both X and Y. Z: Is a confounding variable that affects both X &amp; Y, thus confounding their relationship. So as we said, alcohol will both cause people to be more likely to sleep with their shoes on as well as more likely to wake up with a headache. Thus when evaluating what causes one to wake up with a headache, its hard to tease out the effect of sleeping with shoes on versus just the alcohol. Thus our model needs to also use Z as an explanatory/predictor variable as well, in other words our doctor needs to take into account who had been drinking the night before. Well start covering multiple regression models that allows us to incorporate more than one variable in the next chapter. Establishing causation is a tricky problem and frequently takes either carefully designed experiments or methods to control for the effects of potential confounding variables. Both these approaches attempt either to remove all confounding variables or take them into account as best they can, and only focus on the behavior of an outcome variable in the presence of the levels of the other variable(s). Be careful as you read studies to make sure that the writers arent falling into this fallacy of correlation implying causation. If you spot one, you may want to send them a link to Spurious Correlations. 7.3 Two numerical explanatory variables Lets first consider a multiple regression model with two numerical explanatory variables. We will continue with our previous example coming from the atlas data. In this section, well fit a regression model where we have A numerical outcome variable \\(y\\), the mobility measure for children of parents in the percentile 25th. Two explanatory variables: One numerical explanatory variable \\(x_1\\), average job growth rate between 2003 and 2014. Another numerical explanatory variable \\(x_2\\), the percentage of the population below the poverty line in 2010. 7.3.1 Exploratory data analysis Lets do something similar to what we did before, but include the variable of the share of the population below the poverty line in 2010 poor_share2010 in the data that we keep track off, and call it atlas_mob_jobs_poor. atlas_mob_jobs_poor &lt;- atlas %&gt;% select(tract, county, state, kfr_pooled_p25, ann_avg_job_growth_2004_2013, poor_share2010) Recall the three common steps in an exploratory data analysis we saw in Subsection 7.1.1: Looking at the raw data values. Computing summary statistics. Creating data visualizations. Let us know skim the data with the skim() function: atlas_mob_jobs_poor %&gt;% select(kfr_pooled_p25, ann_avg_job_growth_2004_2013, poor_share2010) %&gt;% skim() Skim summary statistics n obs: 73278 n variables: 2  Variable type:numeric  variable missing complete n mean sd p0 p25 p50 p75 p100 kfr_pooled_p25 1267 0.983 34443. 8169. 0 28973 33733. 39167. 105732. ann_avg_job_growth_2004_2013 2614 0.964 0.0153 0.0762 -0.607 -0.0189 0.00850 0.0410 1.34 poor_share2010 345 0.995 0.151 0.127 0 0.0591 0.116 0.205 1 Observe the summary statistics for the min and max of the variable poor_share2010 are 0 and 1, respectively. This should be a flag for you that the share is once more measured as 0 being 0% and 1 being 100%. Note that the average share of population living below the poverty line in 2010 is 15.1%, and that 25% of neighborhoods have a poverty share of 5.91% or less, while 75% of neighborhoods had a poverty share of 20.5% or less. Since our outcome variable kfr_pooled_p25 and the explanatory variables ann_avg_job_growth_2004_2013 and poor_share2010 are numerical, we can compute the correlation coefficient between the different possible pairs of these variables. First, we can run the cor() command as seen in Subsection 7.1.1 twice, once for each explanatory variable: cor(atlas_mob_jobs_poor$kfr_pooled_p25, atlas_mob_jobs_poor$ann_avg_job_growth_2004_2013, use = &quot;pairwise.complete.obs&quot;) cor(atlas_mob_jobs_poor$kfr_pooled_p25, atlas_mob_jobs_poor$poor_share2010, use = &quot;pairwise.complete.obs&quot;) Or we can simultaneously compute them by returning a correlation matrix which we display in Table 7.4. We can read off the correlation coefficient for any pair of variables by looking them up in the appropriate row/column combination. atlas_mob_jobs_poor %&gt;% select(kfr_pooled_p25, ann_avg_job_growth_2004_2013, poor_share2010) %&gt;% cor() Table 7.4: Correlation coefficients between mobility, job growth and poverty rate. kfr_pooled_p25 ann_avg_job_growth_2004_2013 poor_share2010 kfr_pooled_p25 1.000 0.072 -0.544 ann_avg_job_growth_2004_2013 0.072 1.000 -0.063 poor_share2010 -0.544 -0.063 1.000 For example, the correlation coefficient of: kfr_pooled_p25 with itself is 1 as we would expect based on the definition of the correlation coefficient. kfr_pooled_p25 with ann_avg_job_growth_2004_2013 is 0.072. This indicates a very weak positive linear relationship. kfr_pooled_p25 with poor_share2010 is -0.544. This is suggestive of a strong negative linear relationship. At least, much stronger than the one between kfr_pooled_p25 and ann_avg_job_growth_2004_2013. As an added bonus, we can read off the correlation coefficient between the two explanatory variables of ann_avg_job_growth_2004_2013 and poor_share2010 as -0.063. Lets visualize the relationship of the outcome variable with each of the two explanatory variables in two separate plots in Figure 7.8. ggplot(atlas_mob_jobs_poor, aes(x = ann_avg_job_growth_2004_2013, y = kfr_pooled_p25)) + geom_point(alpha = 0.2) + labs(x = &quot;Average Job Growth 2003-2014&quot;, y = &quot;Mobility for Children of Parents in P25 (in $)&quot;, title = &quot;Mobility and job growth&quot;) + geom_smooth(method = &quot;lm&quot;, se = FALSE) ggplot(atlas_mob_jobs_poor, aes(x = poor_share2010, y = kfr_pooled_p25)) + geom_point(alpha = 0.2) + labs(x = &quot;Proverty Share in 2010&quot;, y = &quot;Mobility for Children of Parents in P25 (in $)&quot;, title = &quot;Mobility and poverty share&quot;) + geom_smooth(method = &quot;lm&quot;, se = FALSE) ## `geom_smooth()` using formula &#39;y ~ x&#39; ## `geom_smooth()` using formula &#39;y ~ x&#39; Figure 7.8: Relationship between mobility and job growth/poverty share. Observe there is a weak positive relationship between mobility and job growth: as job growth increases so also does mobility but in a fairly flat rate. This is consistent with the weak positive correlation coefficient of 0.072 we computed earlier. In the case of poverty share, the strong negative relationship appear here, which depicts the -0.544 correlation coefficient that we estimated above. However, the two plots in Figure 7.8 only focus on the relationship of the outcome variable with each of the two explanatory variables separately. To visualize the joint relationship of all three variables simultaneously, we need a 3-dimensional (3D) scatterplot as seen in Figure 7.9. Because there are many datapoints in the atlas data, I subset this to data for the state of North Carolina (state==37). These points in the atlas_mob_jobs_poor data frame are marked with a blue point where The numerical outcome variable \\(y\\) kfr_pooled_p25 is on the vertical axis The two numerical explanatory variables, \\(x_1\\) poor_share2010 and \\(x_2\\) ann_avg_job_growth_2004_2013, are on the two axes that form the bottom plane. Figure 7.9: 3D scatterplot and regression plane. Furthermore, we also include the regression plane. Recall from above that regression lines are best-fitting in that of all possible lines we can draw through a cloud of points, the regression line minimizes the sum of squared residuals. This concept also extends to models with two numerical explanatory variables. The difference is instead of a best-fitting line, we now have a best-fitting plane that similarly minimizes the sum of squared residuals. Head to here to open an interactive version of similar plane (using different data) in your browser. 7.3.2 Regression plane Lets now fit a regression model and get the regression table corresponding to the regression plane in Figure 7.9. Well consider a model fit with a formula of the form y ~ x1 + x2, where x1 and x2 represent our two explanatory variables ann_avg_job_growth_2004_2013 and poor_share2010. Just like we did in Chapter 7, lets get the regression table for this model using our two-step process and display the results in Table 7.5. We first fit the linear regression model using the lm(y ~ x1 + x2, data) function and save it in mob_model2. We get the regression table by applying the summary() function to mob_model2. # Fit regression model: mob_model2 &lt;- lm(kfr_pooled_p25 ~ ann_avg_job_growth_2004_2013 + poor_share2010, data = atlas_mob_jobs_poor) # Get regression table: summary(mob_model2)$coefficients Table 7.5: Multiple regression table Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 39714 41.4 959.1 0 ann_avg_job_growth_2004_2013 3684 345.4 10.7 0 poor_share2010 -36371 214.1 -169.8 0 Lets interpret the three values in the Estimate column. First, the Intercept value is -$39,714. This intercept represents the mobility for children of parents in p25 in neighborhoods in which ann_avg_job_growth_2004_2013 is 0% and poor_share2010 of 0%. In our data however, the intercept has limited practical interpretation since no neighborhood had ann_avg_job_growth_2004_2013 or poor_share2010 values of 0%. Rather, the intercept is used to situate the regression plane in 3D space. Second, the ann_avg_job_growth_2004_2013 value is $3,684. Taking into account all the other explanatory variables in our model, if you increase job growth in a neighborhood from 0% to 100% (ann_avg_job_growth_2004_2013 from 0 to 1 unit), there is an associated increase of on average $3,684 in economic mobility for p25. Notice once more, that this is equivalent to saying that increasing job growth by 1% has an associated average increase of mobility of $36.84 - which definitely seems like a small amount. Just as we did in Subsection 7.1.2, we are cautious to not imply causality as we saw in Subsection 7.2.2 that correlation is not necessarily causation. We do this merely stating there was an associated increase. Furthermore, we preface our interpretation with the statement taking into account all the other explanatory variables in our model. Here, by all other explanatory variables we mean poor_share2010. We do this to emphasize that we are now jointly interpreting the associated effect of multiple explanatory variables in the same model at the same time. Third, poor_share2010 = -$36,371. Taking into account all the other explanatory variables in our model, for every increase of one unit in the variable poor_share2010, in other words decreasing the share poor in a neighborhood from 0% to 100%, there is an associated decrease of on average $36,371 in mobility. Once more, this is equivalent to saying that a reduction of 1% in the share poor is associated with an increase of $363.71 in economic mobility. Putting these results together, the equation of the regression plane that gives us fitted values \\(\\widehat{y}\\) = \\(\\widehat{\\text{mobility}}\\) is: \\[ \\begin{aligned} \\widehat{y} &amp;= b_0 + b_1 \\cdot x_1 + b_2 \\cdot x_2\\\\ \\widehat{\\text{mobility}} &amp;= b_0 + b_{\\text{avg job growth}} \\cdot \\text{avg job growth} + b_{\\text{pov rate}} \\cdot \\text{pov rate}\\\\ &amp;= 39714 + 3684 \\cdot\\text{avg job growth} - 36371 \\cdot\\text{pov rate} \\end{aligned} \\] Recall in the right-hand plot of Figure 7.8 that when plotting the relationship between kfr_pooled_p25 and poor_share2010 in isolation, there appeared to be a positive relationship of. In the last discussed multiple regression however, when jointly modeling the relationship between kfr_pooled_p25, ann_avg_job_growth_2004_2013, and poor_share2010, there appears to be a weaker positive relationship of kfr_pooled_p25 and ann_avg_job_growth_2004_2013 as evidenced by the slope for ann_avg_job_growth_2004_2013 of $3,684 versus the initial $7,783. 7.4 How to read p-values Now that you know what the coefficients in a regression table mean, lets briefly discuss now how to asses if the relationship you estimated (the coefficients) estimated with sufficient precision or not. Lets recap our latest regression table. We had this: summary(mob_model2)$coefficients ## Estimate Std. Error t value ## (Intercept) 39714 41.4 959.1 ## ann_avg_job_growth_2004_2013 3684 345.4 10.7 ## poor_share2010 -36371 214.1 -169.8 ## Pr(&gt;|t|) ## (Intercept) 0.0000000000000000000000000000 ## ann_avg_job_growth_2004_2013 0.0000000000000000000000000152 ## poor_share2010 0.0000000000000000000000000000 We will now focus on the last column of these tables - the one that says Pr(&gt;|t|). These numbers are called p-values. This is a big topic and I will not do it justice. In fact, I will be very loose in this explanation as this is not a course in statistics, but I do want to give you the intuition for how to read it. If you want to dig more, please see here and here. In essence, when we run a regression like the one above, we usually have at hand a sample of observations out of a universe. For example, in the atlas instance, we do not have information on all the neighborhoods in the US, although we do have most of them in the data. But that means that we could have ended with a different set of observations (or neighborhoods) in an alternative world that is slightly different from the one we actually see. To assess how much of an issue this is, we use p-values. I will not get into any details of how you can calculate p-values, but the idea is fairly simple: a p-value is the probability that you would observe data that is as extreme as the data you do if, in fact, the true slope (or coefficient) was zero. Put another way, if we imagine that there is absolutely no real relationship between our variable(s) \\(x\\) and our outcome \\(y\\), how often would we find data that looks like this or even more extreme in the apparent relationship? The answer is the p-value. Why is that useful? Because if the p-value is sufficiently low it is a very simple way to assess our confidence in the slope we are estimating. Important! p-values: probability that you would observe data that is as extreme as the data you do if, in fact, the true slope (or coefficient) were zero. interpretation: rule of thumb is to say that numbers below 0.05 are evidence that the slope is not produced by random chance. 7.4.1 An Example Lets present our latest regression table for the multivariate case once more. We had this table: summary(mob_model2)$coefficients ## Estimate Std. Error t value ## (Intercept) 39714 41.4 959.1 ## ann_avg_job_growth_2004_2013 3684 345.4 10.7 ## poor_share2010 -36371 214.1 -169.8 ## Pr(&gt;|t|) ## (Intercept) 0.0000000000000000000000000000 ## ann_avg_job_growth_2004_2013 0.0000000000000000000000000152 ## poor_share2010 0.0000000000000000000000000000 Notice that the p-values for all three estimates (the intercept, and our coefficients for ann_avg_job_growth_2004_2013 and poor_share2010) are 0. What does that mean? Well, that means that it would be very very unlikely that we could get data like the one we have (or more extreme) if the intercept was 0, or if the coefficient of ann_avg_job_growth_2004_2013 were 0, or if the coefficient for poor_share2010 was 0. That is, in a loose way, we believe these estimates are not just coincidence. If those numbers would be higher than 0.05 we should perhaps trust those numbers a bit less, as there would be a higher probability that they were produced almost by chance. 7.5 Conclusion 7.5.1 Additional resources As we suggested in Subsection 7.1.1, interpreting coefficients that are not close to the extreme values of -1, 0, and 1 can be somewhat subjective. To help develop your sense of correlation coefficients, we suggest you play the following 80s-style video game called Guess the correlation at http://guessthecorrelation.com/. "],["rmarkdown.html", "Chapter 8 RMarkdown / Quarto output 8.1 How to create an output file 8.2 Add new code 8.3 Add a title 8.4 Add an image 8.5 Submit your work", " Chapter 8 RMarkdown / Quarto output How do you put normal writing, analysis, code and output into one file in a simple way? Enter RMarkdown/Quarto! Throughout this semester you will deliver your projects in the form of a RMarkdown or Quarto file. There are several cool options when you use this type of output, but I will only briefly describe what you need submit your work. You are encourage to dig deeper if you want to here for RMarkdown and here for Quarto. I will focus here on Quarto for practical purposes. RMarkdown and Quarto are almost the same thing for our purposes. Quarto is a new tool that the folks from RStudio developed, but it uses pretty much the same format as RMarkdown, so I will use them interchangably. 8.1 How to create an output file There are several options when creating a Quarto file. You would go to the New file symbol (white page with green plus), and select the Quarto Document option Then you select the title for the project you want to create, and add your name for the authorship. Leave all other default options unchanged, including the HTML option. Then click Create You should have a new Quarto document now. Save it a name by clicking on the floppy disk. Now just try to render the file. This will create the final output in a readable fashion. To do that, click on Render (it as a blue arrow on the top). 8.1.1 Error when rendering file Maybe you clicked on Render and got this message in the **Background Jobs tab, by the console: Error in loadNamespace(x) : there is no package called jsonlite Calls: .main ... loadNamespace -&gt; withRestarts -&gt; withOneRestart -&gt; doWithOneRestart Execution halted R installation: Version: 4.2.1 Path: /opt/R/4.2.1/lib/R LibPaths: - /cloud/lib/x86_64-pc-linux-gnu-library/4.2 - /opt/R/4.2.1/lib/R/library rmarkdown: (None) The rmarkdown package is not available in this R installation. Install with install.packages(&quot;rmarkdown&quot;) If that is the case, its because the package rmarkdown is not installed. So go ahead and install it by running: install.packages(&quot;rmarkdown&quot;) Now click on Render once more. It should work, and you should see a new html window with your document. Once you Render you should see a new pop up window with the output html site, in which this example, would be something like this: You will also this in the tab Background Jobs: Notice the text after Browse at:. That is an address that you can use to watch the final output if you cannot see it anymore. 8.2 Add new code You add code by adding chunks of code. You do that by clicking on Insert, then Code Chunk, and then R. Also, notice that if you add to the first line of the chunk a line like this #| echo:false, then when you render you will not see the code, but only the output of the code. This is convenient when the output is too long or distracting, or when you dont want to show the code necessarily. 8.3 Add a title If you want to add a new title, just type in the main text the following ## (notice the space), and that will give you a Header (or title). You can get subtitles by typing three instead of two pound symbols: ###. 8.4 Add an image What if you want to add an image? For example, you might have downloaded an image like this from the Opportunity Atlas website: How do you add it? Well, you need to do it in two steps. The first, is to upload the image to the directory. To do that, go to the Files tab, and click on Upload, and select your image: You should now see your image in the Files tab. Now, you can add the image to the document by clicking on the image symbol on the top (close to the Insert option). Then browse and select the image you just uploaded, add a caption for it, and add it. You should then see it in the main document. 8.5 Submit your work There is no need to submit anything for Project 1. I will check the RStudio site on Thursday morning, and check your progress on project 1. "],["appendixA.html", "A Statistical Background A.1 Basic statistical terms", " A Statistical Background A.1 Basic statistical terms A.1.1 Mean The mean, also known as (AKA) the average, is the most commonly reported measure of center. It is commonly called the average though this term can be a little ambiguous. The mean is the sum of all of the data elements divided by how many elements there are. If we have \\(n\\) data points, the mean is given by: \\[\\overline{x} = \\frac{x_1 + x_2 + \\cdots + x_n}{n}\\] Note that you will often see shorthand notation for a sum of numbers using \\(\\sum\\) notation. For example, we could rewrite the formula for \\(\\bar{x}\\) as: \\[\\bar{x} = \\frac{x_1 + x_2 + \\cdots + x_n}{n}=\\frac{\\sum_{i = 1}^n x_i}{n}\\] Weve simply replaced the subscript on each \\(x\\) with a generic index \\(i\\), and use the \\(\\sum\\) notation to indicate we are summing \\(x\\)s with indices (i.e. subscripts) that go from 1 to \\(n\\). When summing numbers in statistics, were almost always dealing with indices that start with the value 1 and go up to a value equal to a sample size (e.g. \\(n\\)), so often you will see an even more shorthand version, where its assumed youre summing from \\(i = 1\\) to \\(i = n\\): \\[\\bar{x} = \\frac{\\sum x_i}{n}\\] A.1.2 Median The median is calculated by first sorting a variables data from smallest to largest. After sorting the data, the middle element in the list is the median. If the middle falls between two values, then the median is the mean of those two values. A.1.3 Standard deviation We will next discuss the standard deviation of a sample dataset pertaining to one variable. The formula can be a little intimidating at first but it is important to remember that it is essentially a measure of how far to expect a given data value is from its mean: \\[Standard \\, deviation = \\sqrt{\\frac{(x_1 - \\overline{x})^2 + (x_2 - \\overline{x})^2 + \\cdots + (x_n - \\overline{x})^2}{n - 1}} = \\sqrt{\\frac{\\sum_{i= 1}^n(x_i - \\overline{x})^2 }{n - 1}}\\] A.1.4 Five-number summary The five-number summary consists of five values: minimum, first quartile AKA 25th percentile, second quartile AKA median AKA 50th percentile, third quartile AKA 75th, and maximum. The quartiles are calculated as first quartile (\\(Q_1\\)): the median of the first half of the sorted data third quartile (\\(Q_3\\)): the median of the second half of the sorted data The interquartile range is defined as \\(Q_3 - Q_1\\) and is a measure of how spread out the middle 50% of values is. The five-number summary is not influenced by the presence of outliers in the ways that the mean and standard deviation are. It is, thus, recommended for skewed datasets. A.1.5 Percentiles or rank Just as you can sort the values of a variable form smallest to larges and find the middle element to get the median, you are dividing all the values into two groups with the same number of observations. If you do that but instead of dividing into 2 groups, you divide it into 4 groups, you get each of the quartiles. If you divide the sorted observations into 100 groups of equal number of observations, you get the percentiles. Depending on the application, the percentile can be read as the rank in the distribution. A.1.6 Distribution The distribution of a variable/dataset corresponds to generalizing patterns in the dataset. It often shows how frequently elements in the dataset appear. It shows how the data varies and gives some information about where a typical element in the data might fall. Distributions are most easily seen through data visualization. A.1.7 Outliers Outliers correspond to values in the dataset that fall far outside the range of ordinary values. In regards to a boxplot (by default), they correspond to values below \\(Q_1 - (1.5 * IQR)\\) or above \\(Q_3 + (1.5 * IQR)\\). Note that these terms (aside from Distribution) only apply to quantitative variables. "],["project1.html", "Project 1 Stories from the Atlas: Describing Data using Maps, Regressions, and Correlations Instructions Data Description Cheatsheat commands", " Project 1 Stories from the Atlas: Describing Data using Maps, Regressions, and Correlations Posted: Friday, August 25, 2022 Part 1: Due at midnight on Wednesday, September 7, 2022 Part 2: Due at midnight on Wednesday, September 14, 2022 The Opportunity Atlas is a freely available interactive mapping tool that traces the roots of outcomes such as poverty and incarceration back to the neighborhoods in which children grew up. Policymakers, journalists, and the public have begun to explore the Opportunity Atlas, casting new light on the geography of upward mobility in communities across the country. As an example, see Jasmine Garsds analysis for the New York City neighborhood of Brownsville in Brooklyn. In this first empirical project, you will use the Opportunity Atlas mapping tool and the underlying data to describe equality of opportunity in your hometown and across the United States. (If you grew up outside the United States, you may select a community in which you have spent some time, such as San Antonio, TX.) The end product will be a short narrative (or story) in which you describe what you have learned from the Atlas, in which you will weave in the narrative along with the data analysis. Below is a list of specific analyses and questions that your narrative must address. The document should contain references, graphs, and maps. This project focuses on the following methods for descriptive data analysis. (The later empirical projects you will do in this class will be focused on causal inference and prediction). Data visualization. Maps are a powerful way to present descriptive statistics for data with a geographic component. You will use maps to display upward mobility statistics for the Census tracts in your hometown. Regression and correlation analysis. You will use linear regressions and correlation coefficients to quantify the statistical relationship between upward mobility and potential explanatory variables. The R data file that you will use in this assignment, atlas.rds, contains an extract of the Opportunity Atlas data. It is also merged on several other variables, which you may use for the correlational analysis. You can load the data by using the usual line: atlas &lt;- readRDS(gzcon(url(&quot;https://raw.githubusercontent.com/jrm87/ECO3253_repo/master/data/atlas.rds&quot;))) Instructions You will work on RStudio Cloud for this project. Write the narrative within a Quarto/RMarkdown file in the project1 tab in RStudio Cloud. The deliverables for Part 1 and Part 2 are the following: Part 1: points 1 to 5 Part 2: points 1 to 10 - this should include any feedback you received for Part 1 Notice that Part 2 includes Part 1 I created a Section to help you get up to speed with Quarto/RMarkdown here. The Quarto/RMarkdown file is where you will write your narrative, run all your analysis, and the output of each code block should be visible. The output should include references, graphs, maps, and tables. Specific questions to address in your narrative Start by looking up the city where you grew up on the Opportunity Atlas. Zoom in to the Census tracts around your home. Figure 1 in your narrative should be a map of the Census tracts in your hometown from the Opportunity Atlas. Examples for Milwaukee, WI (where Professor Chetty grew up) and Los Angeles, CA (discussed in Lecture 1) are shown on the next page. The text of your narrative should describe what you see, and what data are being visualized. Examine the patterns for a number of different groups (e.g., lowest income children, high income children) and outcomes (e.g., earnings in adulthood, incarceration rates). Only choose one or two of these to include in your narrative. (To answer this question, read the Opportunity Atlas manuscript) What period do the data you are analyzing come from? Are you concerned that the neighborhoods you are studying may have changed for kids now growing up there? What evidence do Chetty et al. (2018) provide suggesting that such changes are or are not important? What type of data could you use to test whether your neighborhood has changed in recent years? Now turn to the atlas.rds data set. How does average upward mobility, pooling races and genders, for children with parents at the 25th percentile (kfr pooled_p25) in your home Census tract compare to mean (population-weighted, using count_pooled) upward mobility in your state and in the U.S. overall? Do kids where you grew up have better or worse chances of climbing the income ladder than the average child in America? Hint: The Opportunity Atlas website will give you the tract, county, and state FIPS codes for your home address. For example, searching for Lynwood Road, Verona, New Jersey will display Tract 34013021000, Verona, NJ. The first two digits refer to the state code, the next three digits refer to the county code, and the last 6 digits refer to the tract code. In R, you can list these observations with the function filter() as follows (assuming you called the data as atlas as in Table 2). If you only want to see kfr_pooled_p25: atlas_lynwood&lt;-atlas%&gt;% filter(state == 34 &amp; county == 013 &amp; tract == 021000) atlas_lynwood%&gt;% select(kfr_pooled_p25) Or to see all the variables for that tract: View(atlas_lynwood) See the Cheat-sheet Section below for further help. What is the standard deviation of upward mobility (population-weighted) in your home county? Is it larger or smaller than the standard deviation across tracts in your state? Across tracts in the country? What do you learn from these comparisons? Now lets turn to downward mobility: repeat questions (3) and (4) looking at children who start with parents at the 75th and 100th percentiles. How do the patterns differ? Using a linear regression, estimate the relationship between outcomes of children at the 25th and 75th percentile for the Census tracts in your home county. Generate a scatter plot to visualize this regression. Do areas where children from low-income families do well generally have better outcomes for those from high-income families, too? Next, examine whether the patterns you have looked at above are similar by race. If there is not enough racial heterogeneity in the area of interest (i.e., data is missing for most racial groups), then choose a different area to examine. Using the Census tracts in your home county, can you identify any covariates which help explain some of the patterns you have identified above? Some examples of covariates you might examine include housing prices, income inequality, fraction of children with single parents, job density, etc. For 2 or 3 of these, report estimated correlation coefficients along with their 95% confidence intervals. Open question: formulate a hypothesis for why you see the variation in upward mobility for children who grew up in the Census tracts near your home and provide correlational evidence testing that hypothesis. For this question, many covariates have been provided to you in the atlas.rds file, which are described in the Table under the Data Description section. Putting together all the analyses you did above, what have you learned about the determinants of economic opportunity where you grew up? Identify one or two key lessons or takeaways that you might discuss with a policymaker or journalist if asked about your hometown. Mention any important caveats to your conclusions; for example, can we conclude that the variable you identified as a key predictor in the question above has a causal effect (i.e., changing it would change upward mobility) based on that analysis? Why or why not? Figure A.1: Household Income in Adulthood for Children Raised in Low-Income Households in Milwaukee, WI Notes: This figure shows household income at ages 31-37 for low income children who grew up in Census tracts near Milwaukee, WI. The image was saved from www.opportunity-atlas.org by first searching for Milwaukee, WI and then clicking on the download as image button. Figure A.2: Incarceration Rates for Black Men Raised in the Lowest-Income Households in Los Angeles, CA Notes: This figure is from the non-technical summary of the Opportunity Atlas and was discussed in Section @ref(lec1_geomobility). Data Description The data consist of n = 73,278 U.S. Census tracts. For more details on the construction of the variables included in this data set, please see Chetty, Raj, John Friedman, Nathaniel Hendren, Maggie R. Jones, and Sonya R. Porter. 2018. The Opportunity Atlas: Mapping the Childhood Roots of Social Mobility., NBER Working Paper No. 25147. Table 1 Definitions of Variables in atlas.rds Variable name Label Obs. (1) (2) (3) 1. Geographic identifiers tract Tract FIPS Code (6-digit) 2010 73,278 county County FIPS Code (3-digit) 73,278 state State FIPS Code (2-digit) 73,278 cz Commuting Zone Identifier (1990 Definition) 72,473 2. Characteristics of Census tracts hhinc_mean2000 Mean Household Income 2000 72,302 mean_commutetime2000 Average Commute Time of Working Adults in 2000 72,313 frac_coll_plus2010 Fraction of Residents with a College Degree or More in 2010 72,993 frac_coll_plus2000 Fraction of Residents with a College Degree or More in 2000 72,343 foreign_share2010 Share of Population Born Outside the U.S. 72,279 med_hhinc2016 Median Household Income in 2016 72,763 med_hhinc1990 Median Household Income in 1999 72,313 popdensity2000 Population Density (per square mile) in 2000 72,469 poor_share2010 Poverty Rate 2010 72,933 poor_share2000 Poverty Rate 2000 72,315 poor_share1990 Poverty Rate 1990 72,323 share_black2010 Share black 2010 73,111 share_hisp2010 Share Hispanic 2010 73,111 share_asian2010 Share Asian 2010 71,945 share_black2000 Share black 2000 72,368 share_white2000 Share white 2000 72,368 share_hisp2000 Share Hispanic 2000 72,368 share_asian2000 Share Asian 2000 71,050 gsmn_math_g3_2013 Average School District Level Standardized Test Scores in 3rd Grade in 2013 72,090 rent_twobed2015 Average Rent for Two-Bedroom Apartment in 2015 56,607 singleparent_share2010 Share of Single-Headed Households with Children 2010 72,564 singleparent_share1990 Share of Single-Headed Households with Children 1990 72,196 singleparent_share2000 Share of Single-Headed Households with Children 2000 72,285 traveltime15_2010 Share of Working Adults w/ Commute Time of 15 Minutes Or Less in 2010 72,939 emp2000 Employment Rate 2000 72,344 mail_return_rate2010 Census Form Rate Return Rate 2010 72,547 ln_wage_growth_hs_grad Log wage growth for HS Grad., 2005-2014 51,635 jobs_total_5mi_2015 Number of Primary Jobs within 5 Miles in 2015 72,311 jobs_highpay_5mi_2015 Number of High-Paying (&gt;USD40,000 annually) Jobs within 5 Miles in 2015 72,311 nonwhite_share2010 Share of People who are not white 2010 73,111 popdensity2010 Population Density (per square mile) in 2010 73,194 ann_avg_job_growth_2004_2013 Average Annual Job Growth Rate 2004-2013 70,664 job_density_2013 Job Density (in square miles) in 2013 72,463 3. Measures of Upward Mobility from the Opportunity Atlas kfr_pooled_p25 Household income ($) at age 31-37 for children with parents at the 25th percentile of the national income distribution 72,011 kfr_pooled_p75 Household income ($) at age 31-37 for children with parents at the 75th percentile of the national income distribution 72,012 kfr_pooled_p100 Household income ($) at age 31-37 for children with parents at the 100th percentile of the national income distribution 71,968 kfr_natam_p25 Household income ($) at age 31-37 for Native American children with parents at the 25th percentile of the national income distribution 1,733 kfr_natam_p75 Household income ($) at age 31-37 for Native American children with parents at the 75th percentile of the national income distribution 1,728 kfr_natam_p100 Household income ($) at age 31-37 for Native American children with parents at the 100th percentile of the national income distribution 1,594 kfr_asian_p25 Household income ($) at age 31-37 for Asian children with parents at the 25th percentile of the national income distribution 15,434 kfr_asian_p75 Household income ($) at age 31-37 for Asian children with parents at the 75th percentile of the national income distribution 15,360 kfr_asian_p100 Household income ($) at age 31-37 for Asian children with parents at the 100th percentile of the national income distribution 13,480 kfr_black_p25 Household income ($) at age 31-37 for Black children with parents at the 25th percentile of the national income distribution 34,086 kfr_black_p75 Household income ($) at age 31-37 for Black children with parents at the 75th percentile of the national income distribution 34,049 kfr_black_p100 Household income ($) at age 31-37 for Black children with parents at the 100th percentile of the national income distribution 32,536 kfr_hisp_p25 Household income ($) at age 31-37 for Hispanic children with parents at the 25th percentile of the national income distribution 37,611 kfr_hisp_p75 Household income ($) at age 31-37 for Hispanic children with parents at the 75th percentile of the national income distribution 37,579 kfr_hisp_p100 Household income ($) at age 31-37 for Hispanic children with parents at the 100th percentile of the national income distribution 35,987 kfr_white_p25 Household income ($) at age 31-37 for white children with parents at the 25th percentile of the national income distribution 67,978 kfr_white_p75 Household income ($) at age 31-37 for white children with parents at the 75th percentile of the national income distribution 67,968 kfr_white_p100 Household income ($) at age 31-37 for white children with parents at the 100th percentile of the national income distribution 67,627 3. Counts of number of children under 18 in 2000 (to calculate weighted summary statistics) count_pooled Count of all children 72,451 count_white Count of White children 72,451 count_black Count of Black children 72,451 count_asian Count of Asian children 72,451 count_hisp Count of Hispanic children 72,451 count_natam Count of Native American children 72,451 Cheatsheat commands R command Description Here I present a summary of the commands you could use to work on this project. There are two important issues you should keep in mind while reading this: Notice that whenever you see yvar this is not a real variable. It is only a place holder for the appropriate variable that you decide to analyze or use. For example, if you want to see the mean across neighborhoods of the average household income as measured in 2000, you would not do mean(atlas$yvar, na.rm=TRUE) but mean(atlas$hhinc_mean2000, na.rm=TRUE). Important! yvar is not a real variable. You should replace it for the appropriate variable in your code. The data atlas has missing information for some neighborhoods for some variables. These are called NA or missing. Most R functions do not like that you include missings in the function, because R does not know what to do with that. What is 5+NA ? NA !! So, for many of these functions, we will explicitely tell R to ignore NAs. That is what the option na.rm=TRUE does. It does not not exist for every function, but it does for most of the ones we will use here. Important! Careful with missing values (also called NA)! We will use na.rm=TRUE as an option for several functions to tell R to ignore the missings. Unweighted summary statistics summary(atlas$yvar) mean(atlas$yvar, na.rm=TRUE) sd(atlas$yvar, na.rm=TRUE) Load package If you wanted to install and open the package Hmisc (which you will need to calculate the weighted statistics), run: install.packages(&quot;Hmisc&quot;) library(Hmisc) Weighted summary statistics You can weight means or other statistics. In our case, we want to use the population weighted statistics in several cases. That is, we want to put more weight on the value of a tract in which more people live than in another with lower population. Recall that the population variable is count_pooled. Weighted mean: wtd.mean(atlas$yvar, atlas$count_pooled) Weighted standard deviation: sqrt(wtd.var(atlas$yvar, atlas$count_pooled)) Subset observations State level: If you want to select a subset of observations, you can add the rule for selecting those observations, and the filter function. Here we subset the observations for the State of Wisconsin, and called the resulting dataset atlas_wisconsin. atlas_wisconsin &lt;- atlas %&gt;% filter(state == 55) County level: We can do the same but now for a specific county, adding an extra rule after a comma , or &amp;. Here we subset the observations for Milwaukee County in Wisconsin: atlas_milwaukee &lt;- atlas %&gt;% filter(state == 55 &amp; county == 079) Standardize variables You can standardize variables by substracting the mean and dividing by the standard deviation. Let us say that you want to standardize only considering the variables in Milwaukee, then you can do this: atlas_milwaukee&lt;- atlas_milwaukee %&gt;% mutate(x_std=(xvar - mean(atlas_milwaukee$xvar, na.rm=TRUE))/sd(atlas_milwaukee$xvar, na.rm=TRUE)) As an example, lets say you want to standardize both the measure of mobility and the annual job growth for the data in Wisconsin: atlas_wisconsin&lt;- atlas_wisconsin %&gt;% mutate(kfr_pooled_p25_std=(kfr_pooled_p25 - mean(atlas_wisconsin$kfr_pooled_p25, na.rm=TRUE))/sd(atlas_wisconsin$kfr_pooled_p25, na.rm=TRUE), ann_avg_job_growth_2004_2013_std=(ann_avg_job_growth_2004_2013 - mean(atlas_wisconsin$ann_avg_job_growth_2004_2013, na.rm=TRUE))/sd(atlas_wisconsin$ann_avg_job_growth_2004_2013, na.rm=TRUE)) Run regression I have written a whole section explaining regression in more detail: Section 7. Please see that for further details. But here is a quick help. Simple linear regression Lets say you want to run a simple regression of variable yvar on variable xvar1 for the county of Milwaukee. We will save the results of that regression in an object call mod1 (we could give it any name). Then you would do this: mod1 &lt;- lm(yvar~xvar1, data = atlas_milwaukee) To see what the outcome of the regression is, you would use the function summary and apply it to our new object mod1, like this: summary(mod1) As an example, we could regress the mobility of children from parents in the 25th percentile (kfr_pooled_p25) on the average annual job growth rate between 2004 and 2013 (ann_avg_job_growth_2004_2013). To do that, we would run: mod1 &lt;- lm(kfr_pooled_p25~ann_avg_job_growth_2004_2013, data = atlas_milwaukee) Multivariate linear regression You might want to understand the relationship between yvar and variable xvar1 while holding fixed another variable xvar2 for neighborhoods only in Milwaukee. You can do this: mod2 &lt;- lm(yvar~xvar1+xvar2 + xvar3, data = atlas_milwaukee) How to read the regression output? To simplify the interpretation, lets run a regression where you use the standardize both the measure of mobility and the annual job growth for the data in Wisconsin: mod3 &lt;- lm(kfr_pooled_p25_std ~ ann_avg_job_growth_2004_2013_std, data = atlas_wisconsin) summary(mod3) ## ## Call: ## lm(formula = kfr_pooled_p25_std ~ ann_avg_job_growth_2004_2013_std, ## data = atlas_wisconsin) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.829 -0.569 0.035 0.684 5.128 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.00000978 0.02681564 0.00 1.000 ## ann_avg_job_growth_2004_2013_std -0.05131843 0.02679889 -1.91 0.056 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.999 on 1386 degrees of freedom ## (6 observations deleted due to missingness) ## Multiple R-squared: 0.00264, Adjusted R-squared: 0.00192 ## F-statistic: 3.67 on 1 and 1386 DF, p-value: 0.0557 You should focus here on interpreting the coefficient for ann_avg_job_growth_2004_2013_std. You should pay attention to both the magnitude of the number, and the sign. In this case, you would read it like this: In Wisconsin, increasing one standard deviation the average annual job growth rate is correlated with an reduction of 0.05 standard deviations in the economic mobility for children with parents in the 25th percertile. Plotting the linear relationship You need to load the ggplot2 package (which should be installed already). library(ggplot2) Suppose you want to visually see the linear relationship between two variables in the atlas_milwaukee dataset that we filtered above. Then, you can do this: ggplot(data = atlas_milwaukee) + geom_point(aes(x = xvar1, y = yvar)) + geom_smooth(aes(x = xvar1, y = yvar), method = &quot;lm&quot;, se = F) The function geom_smooth() adds the line that you calculated above for mod1, where you ran mod1 &lt;- lm(yvar~xvar1, data = atlas_milwaukee). For more, see Section 5. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
