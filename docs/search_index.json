[["index.html", "Economics of Public and Social Issues ECO 3253, Fall 2022 About About me Organization of this site Data-driven course Topics to be covered Statistical tools Economic Concepts Projects FAQs", " Economics of Public and Social Issues ECO 3253, Fall 2022 Jonathan Moreno-Medina 2022-08-29 About Hey! Welcome to ECO 3253! In this course we will see how we can use data to understand and solve current and important economic problems! You will get a sense of what is the research frontier in applied economics and social science. These include topics like equality of opportunity and mobility, education, innovation and entrepreneurship, health care, climate change, and crime. How will we get there? By doing 3 things: covering the topics with a focus towards using data that can help answer these questions understanding the intuition for how to use data to answer these questions (basics of statistical analysis) using computational tools to help us with the statistical analysis (basics of R) About me Let me briefly tell you about me: I am Colombian. Did my PhD in Economics at Duke University, a Masters in Economics at Université Catholique de Louvain in Belgium, and an undergraduate in (you guessed it!) Economics at Universidad Nacional de Colombia. I am an applied (micro)economist. What is applied microeconomics? Well, first what I do not work on: macroeconomics (inflation, general unemployment, GDP growth, and so on). The micro part just means I tend to focus on specific markets (housing and media, being the main ones), and the applied just means I use data all the time in my research. Which leads me to my next point. Organization of this site In this site I will put the materials we cover in the lectures so you can refer to it later on your own. I will divide this book into two main parts: economic content, and the tools. The tools are both statistical (correlations, means, distributions, etc) and computational (R). For the first week of class, you shall see two headers on the table of content on the left: one for each. These will get filled up as the semester continues. There is also an Appendix, which you will be able to see on the bottom of the navigation panel, where I will post complementary material where you could brush up several statistical concepts, for example. Lastly, I will also leave a link to the projects you will work on this semester at the end of the list on the left. Data-driven course We will work with data throughout the semester! This will not just be a theoretical course. We will work with real world data, and we will try to make sense of it. We will need theoretical to make sense of the world for sure (either coming from economics or statistics), but we are looking at those tools always eyeing real world instances. The course here presented is partly based on the course by Prof. Raj Chetty at Harvard. As we will see later, I also rely on some material on the book ModernDive for teaching the basics of our statistical software R. Just to give you a sense of how much economics has became a much more empirical discipline in later years, below is the number of articles in leading journals that are data-drive in a way. (#fig:fig_econJournals)Source: Mamermesh (JEL 2013) Fortunately, the same types of skills that are used to solve private market issues using data work to tackle challenges like growing inequality and climate change. In order to achieve that goal, the idea of this class is to introduce a broad range of topics, methods, and real-world applications of these sorts of ideas. Fundamentally, we want to start from the questions that motivate the methods we teach in economics and social science, rather than the traditional approach, which is to do the reverse. Topics to be covered The plan for what we will covered in the semester includes: Geography of Upward Mobility in America Causal Effects of Neighborhoods and Characteristics of High-Mobility Areas Historical and International Evidence on the Drivers of Inequality and Mobility Upward Mobility, Innovation, and Growth Higher Education and Upward Mobility Primary education Teachers and Charter Schools Racial Disparities in Economic Opportunity Improving judicial decisions Immigration Political Economy Income taxation Savings and wealth Housing markets and COVID Intro to air and water pollution, and externalities Discount rates, external validity You can check the Schedule here, or for more details, please check the syllabus on Blackboard. Statistical tools Although we are going to take a topic-oriented focus in this class, we will cover the basics of several methods that will help us make sense of the data. These methods include: Descriptive Data Analysis: correlation, regression Experiments: randomization, non-compliance Quasi-Experiments: regression discontinuity, difference-in-differences Machine Learning: prediction, overfitting, cross-validation R as our statistical software Economic Concepts We will cover and make use if several economic concepts you probably learned in your intro classes, but we will see several instances of how to use it practice. These include: Effects of price incentives Supply and demand Competitive equilibrium Adverse selection Behavioral economics vs. rational models Projects A big part of the course will be the projects you will do during the semester. You will work on 4 projects through the semester. These are more involved than most homeworks you have probably worked up to now. The good news is that they involve doing economics! You will get hands-on experience working with real data on real problems. The main recommendation is to start working on this projects early! I cannot emphasize this enough. There are several moving parts to these projects, and you need to plan in advance your work so you can try, fail, come back to it, and so on. If you try to work on these projects just the night before the deadline, that will not leave much room for experimenting, and trying. Given than several of these tools might be new, you need to give yourself time to try more than once. Important! Do not work on these projects just the day before! They are relatively involved, so give yourself enough time. For more details, please check the syllabus on Blackboard. FAQs Do I need to know how to program? No! I will give you the basic tools to understand and do basic analysis in R even if you have no background in this sort of things. Do I need to have taken econometrics? No! You will have a bit of a head start if you have, but you do not have to have taken the econometrics course to be successful in this class. I will give you some of the basic conceptual frameworks for how we think about statistical analysis and causal inference. Do I need to know statistics? Basic statistics is definitely recommended. We will have plenty of opportunity to brush up some of those concepts throughout the course, though. You can find a refresher in the Appendix Where can I find more details about this class? You can read more in the syllabus uploaded to Blackboard. "],["schedule.html", "Updated Schedule", " Updated Schedule I will keep this page with the updated schedule for the semester. You can find these readings on Blackboard. Week Session Topic Required reading Method Deliverable 1: 08/22 &amp; 08/24 1 Introduction to the course 2 Geography of Upward Mobility in America Chetty, Friedman, Hendren, Jones and Porter (2018)- Non-technical summary Correlation, regression 2: 08/29 &amp; 08/31 3 Intro to R and data 4 Intro to visualization and RMarkdown (homeworks) First short report (In class) 3: 09/05 &amp; 09/07 - Labor Day  No class 5 Intro to Causal Effects of Neighborhoods Bergman, Chetty, DeLuca, Hendren, Katz and Palmer (2019) [Non-technical summary] Experiments (RCTs) Project 1  Part 1 4: 09/12 &amp; 09/14 6 Characteristics of High-Mobility Areas Bergman, Chetty, DeLuca, Hendren, Katz and Palmer (2019) [Introduction] Quasi-experiments 7 Historical and International Evidence onthe Drivers of Inequality and Mobility Cost-benefit analysis Project 1  Part 2 5: 09/19 &amp; 09/21 8 Upward Mobility, Innovation, and Growth Bian, Leslie and Cimpian (2017) Propensity score reweighting 9 Higher Education and Upward Mobility Dynarski, Libassi, Michelmore and Owen (2018) Regression discontinuity 6: 09/26 &amp; 09/28 10 Higher Education and Upward Mobility II 11 Review for midterm Project 2 7: 10/03 &amp; 10/05 12 Midterm 13 Solution review 8: 10/10 &amp; 10/12 14 Primary education Chetty, Friedman and Rockoff (2011) [Non-technical summary] Experiments 15 Teachers and Charter Schools Event study designs, competitive equilibrium 9: 10/17 &amp; 10/19 16 Racial Disparities in Economic Opportunity Bertrand, and Mullainathan (2004) Dynamic models and steady states 17 Improving judicial decisions Kleinberg, Lakkaraju, Leskovec, Ludwig and Mullainathan (2017) Machine learning, implicit bias Project 3 10: 10/24 &amp; 10/26 18 Implementing a simple machine learning model in R 19 Immigration Clemens (2011) Welfare analysis 11: 10/31 &amp; 11/02 20 Political Economy 21 Income taxation Diamond and Saez (2011) Supply and demand; synthetic control 12: 11/07&amp; 11/09 22 Savings and wealth Behavioral economics 23 Regression Project 4  Part 1 13: 11/14 &amp; 11/16 24 Housing markets and COVID Glaeser, Edward, Kominers, Luca and Naik (2018) Machine learning 25 Intro to air and water pollution, and externalities Moore, Obradovich, Lehner and Baylis (2019) Difference in difference Project 4  Part 2 14: 11/21 &amp; 11/28 26 Discount rates, external validity Regression discontinuity 27 15: 11/28 &amp; 11/30 28 29 Review session 16: 12/05 -12-09 - Final Exam Week - "],["lec1_geomobility.html", "Chapter 1 Upward Mobility in the US 1.1 Parental and children income rank 1.2 Interpreting the regression line 1.3 Geographic Variation in Upward Mobility by Commuting Zone 1.4 Local Area Variation in Upward Mobility: Los Angeles, CA", " Chapter 1 Upward Mobility in the US We will first dive into a relatively recent (from 2020) called The Opportunity Atlas: Mapping the Childhood Roots of Social Mobility., by Chetty, Friedman, Hendren, Jones and Porter. The previous link should take you to Blackboard where you will be able to download it. The main question in the paper is the following: how do childrens chances of moving up vary across areas in America? To answer it, the authors need to measure upward mobility across the country. How do they do that? They will construct a database called the Opportunity Atlas. How do they measure upward mobility separately by geographic area in the United States? They take data from the 2000 and 2010 Censuses, and link that to information from federal income tax returns. They also use tax return data from 1989 to 2015. Linking those datasets yields information on essentially every American between 1989 and 2015, including how much they are earning, where they live, the dependents they have, and other information, year by year. In that dataset, they want to study economic opportunity across generations. But that requires knowing in the data who is the children of whom. In order to link parents to their children, they use information from dependency claims on tax returns. (In order to receive a tax deduction, parents must enter their childs Social Security Number on their tax returns.) Theyre able to use this information to link 99% of kids in America back to their parents, thereby generating an intergenerational sample where you can study income inequality and mobility across generations. There is a lot you can learn about the US and economic mobility with that sort of data! At the end, they end up with an 8-billion row dataset which covers 20.5 million children born between 1978 and 1983, representing 96% of our target population. They analyze children born during those particular years because we need the children to be old enough that we can measure their earnings reliably. Theyre interested in people who were born in the U.S. or are authorized immigrants who came to the U.S. in childhood. They are focusing on authorized immigrants because these datasets dont go a great job of covering undocumented immigrants. Note here that this is a limitation for the study only in the sense that it does not necessarily paint a picture of how economic mobility looks for undocumented immigrants. Additionally, the number is not 100% because there are some kids who you cant link to their parents and people you cant link the census form to the tax form. How do they measure parents and childrens incomes in tax data? They do so by measuring incomes using information from the anonymized tax return data. For parents, they use average income between 1994 and 2000 reported on Form 1040, the main tax return in the U.S. Similarly, for kids, we measure average income in 2014 and 2015, the last two years of the data theyworked with. That is when the children are in their mid-30s. Using this information, theyre going to focus on percentile ranks in the national distribution. What that means, concretely, is that they rank kids relative to all the other kids born in the same year, and parents relative to all other parents. They are comparing kids to other kids of the same age. Then likewise, they compare parents to other parents. The reason is that because they want to adjust for the fact that as people grow older, their incomes tend to rise. 1.1 Parental and children income rank The chart below was constructed using data for kids who were raised in the Chicago metro area, which consists of Chicago and the surrounding suburbs. Figure 1.1: Source: Chetty, Hendren, Kline, Saez (2014) Lets interpret the figure. On the x-axis, it shows the parent rank in the national income distribution. There are a hundred dots here, one corresponding to each percentile of the distribution (see here for a refresher on what the percentiles are). Then in each of those hundred bins, were plotting the average ranking of the child in the national income distribution. Now as you go to the right, youre looking at kids from richer and richer families, and you see that theres a very strong upward-sloping pattern. This reflects the simple fact that if you were born to a richer family in America, you yourself tend to be richer in adulthood. Now lets find the line that fits that data most accurately using a method called regression. Then Im going to focus on the value of this line, called the predicted value, at the 25th percentile of the parent income distribution. There is a lot of information contained in each dot in the graph, but by focusing on the value of this line we can construct a digestible single statistic (i.e., a number) summarizing what upward mobility looks like in each place. 1.2 Interpreting the regression line In Chicago, on average, kids who start out in families at the 25th percentile end up at the 40th percentile. Kids growing up in low-income families in Chicago, roughly speaking, earn about $30,000, on average, when theyre adults. We cant directly use the value of the dot on the above chart at the 25th percentile. Instead we use a regression line. This is because there is noise and random variation in the data, specifically with smaller samples of people. When working with small samples, it starts to become very important to fit that regression line. That is, we need to use the discipline of a statistical model. Thats the core idea of statistical models, to take the underlying data and represent it in a way that is more stable. 1.2.1 Percentiles The conversion to percentiles is very important here. If we did this analysis in dollars, that relationship is very far from linear. It is very curved, which makes it harder to fit systematically with a statistical model. To construct the Opportunity Atlas, we fit line like this to the kids who grew up in every different census tract in America. 1.2.2 What is a tract? A Census tract is a small definition of a neighborhood that the Census Bureau has created. There are 70,000 Census tracts in America, each of which has about 4,200 people. In order to handle children who might have moved while they were kids, we weigh children by the fraction of their childhood that they spent in each area. 1.3 Geographic Variation in Upward Mobility by Commuting Zone The map below plots average household earnings of children who grew up in low-income families. The map presents this statistic separately for each of the 741 commuting zones (CZs) in the United States. CZs are aggregations of counties based on commuting patterns that are similar to metro areas but cover the entire United States. Figure 1.2: Source: Chetty, Friedman, Hendren, Jones and Porter (2018) Note that the map shows household income in dollars, but the underlying statistic is based on the predicted percentile rank defined earlier. The ranks have been converted to dollars because its more intuitive and concrete. In the map, blue colors depict areas with high levels of upward mobility and red colors depict areas with low levels of upward mobility. The map shows broad geographic variation. One of the most interesting features of this map is that the highest upward mobility areas in America are the Great Plains, the rural parts of the country in the center of the country. Charlotte is one of the cities in America with the highest rates of job growth in the United States. Yet, somehow remarkably, for low-income kids who grow up in Charlotte, they do not have very good chances of moving up. The map shows that in the current generation, there are some parts of America where kids chances of moving up still look fantasticactually better than any other country in the world. Then theres some places, like in much of the industrial Midwest, where your odds of climbing up look worse than any country for which we currently have data. America is a land of tremendous variability in opportunity. 1.3.1 Adjustments for cost of living This map shows nominal incomes, meaning it does not adjust for differences in cost of living. You can redraw this map, adjusting for differences in cost of living. When you do that, you get a map that looks almost identical to the one that Im showing you here. To put it more precisely, the correlation between that data and these data is .9, meaning that it looks essentially the same. Were focusing specifically here on kids growing up in low-income families. If you look at kids growing up in middle-class families, its broadly similar. If you look at kids growing up in high-income families, you see that theres significantly less variation across areas for kids growing up in very-high-income families. 1.4 Local Area Variation in Upward Mobility: Los Angeles, CA This geographic variation in upward mobility is not just about broad regional variation, but its actually about extremely local variation. We can use the Opportunity Atlas to visualize the data. Figure 1.3: Incarceration Rates for Black Men Raised in the Lowest-Income Households in Los Angeles, CA The Opportunity Atlas starts out with the national map of the same statistics by commuting zone that we were looking at before. However, it allows us to zoom in to areas of specific interest. Let us focus on one particular example: Nickerson Gardens in Los Angeles, CA, which is a public housing project in Watts. Lets look at black men growing up in the lowest-income families in the bottom 1% of the income distribution, which is actually representative of the incomes of the families living in this public housing project. The average household income of black men who grew up in the poorest families in Watts is just $3,300 a year. It has to be the case that lots of people are basically not working at all. You can see that in a very direct way in these data because were able to look not just at income, but a variety of other outcomes, including incarceration. Focusing on incarceration rates, you will see a really shocking and disturbing statistic about the United States, and this area in particular, which is that 44% of the black men who grew up in these lowest-income families are incarcerated on a single day, the date of the 2010 census. If you go down to Compton, you see incarceration rates of 6.2%, which is a factor of 10 smaller than the 44% that we were seeing in Watts for black men growing up in low-income families. Compton is a different neighborhood than Watts, its not exactly the same, but I dont think anybody from L.A. would have predicted that Compton would have drastically different outcomes like this from Watts. That shows you that you can go two miles away and just have a dramatically different picture in terms of what kids life trajectories look like. We see that in the stark example here within Los Angeles, but we see that sort of thing more broadly across the United States. "],["getting-started.html", "Chapter 2 Very Brief Intro to Data in R 2.1 Why R again? 2.2 Key concepts before we start: 2.3 Lets open R! 2.4 How do I code in R? 2.5 In Class Exercise 2.6 What are R packages? 2.7 Hands-on exercise! 2.8 Conclusion", " Chapter 2 Very Brief Intro to Data in R 2.1 Why R again? 2.1.1 Why are we learning R? I wanted to learn about economics of public and social issues This class is about social issues in economics. But what are those social issues? Economic mobility and inequality Effects of education on wages and inequality Criminal justice system outcomes Pollution and climate change and so on How can we know if going to school increases wages? Or if economic mobility is low or high? We need to analyze data! We can do that analysis by hand but that would be very time consuming. Or we can use a super calculator with amazing capabilities to explore data, maps, etc: enter R and R Studio. This course is not about teaching you all about R! We will only cover the very basics so you can jump into doing some empirical analysis by yourself. You will be able to expand much more on the tools briefly described here in other, more advanced, courses in the Economics sequence. Important! For the vast majority of exercises in our course, I will give you all the code you will have to run. So, its not like you need to write anything from scratch! I do want you to get a basic understanding of what we will be doing when we run those lines. 2.1.2 Ok, but why R? R is free and open source! R has a vibrant online community! R is very flexible and powerful  adaptable to nearly any task ( e.g., correlations, econometrics, spatial data analysis, machine learning, web scraping, data cleaning, website building, teaching.) Employers like R over alternatives 2.1.3 Added benefits of learning R Employers hire people that knows R. Again, we will only cover the essentials, but maybe you want to keep this in mind as you go along with your studies. 2.2 Key concepts before we start: Before we can start exploring data in R, there are some key concepts to understand first: What are R and RStudio? How do I code in R? What are R packages? Well introduce these concepts in upcoming Sections 2.2.1-2.6. Then well introduce our first data set: data on the economic mobility for all neighborhoods across the US in the atlas dataset. 2.2.1 What are R and RStudio? For much of this book, we will assume that you are using R via RStudio. First time users often confuse the two. At its simplest: R is like a cars engine. RStudio is like a cars dashboard. R: Engine RStudio: Dashboard More precisely, R is a programming language that runs computations while RStudio is an integrated development environment (IDE) that provides an interface by adding many convenient features and tools. So just as having access to a speedometer, rearview mirrors, and a navigation system makes driving much easier, using RStudios interface makes using R much easier as well. 2.2.2 R and RStudio: In your computer or in the cloud? To use R and RStudio, you can: install it in your computer (see the book) or  run it on someone elses computer (the cloud!). We will do that, so you dont have to worry about installations. In this course I wont require you to install R and RStudio in your own computer. Instead, we will use the cloud! The Department of Economics is graciously paying for your accounts this semester. If you would still like to install R and RStudio in your own computer, please follow the instructions in Section 1.1.1 in this link. Of course, you are encouraged to experiment in your own machine as well. Notice, however, that you should carry out the assignments and projects in RStudio Cloud. Important! Homework and projects should be carried in RStudio Cloud (not your personal computer). 2.3 Lets open R! 2.3.1 RStudio in the cloud Lets jump in! You should have received a link for you to access RStudio in the cloud. The Department of Economics is paying for us to use this service for this class. Remember that all the code is running on someones computer. In this case, it is running on a computer owned by the folks at RStudio. Receive link with invitation You should have received an email with a link inviting you to join RStudio Cloud. Once you click on the link, you should land in a page like this: Then you should fill out the information for your account. Please use your utsa email account (the account you should have received the invitation to). Once you click on Sign Up, it will show this: Then you need to go back to your email and verify it. Then log back in again! Inside RStudio Cloud select ECO3253 Once you have logged back in, you should land in the main page, which should look like this: Click on the left where it says ECO3253. That is where you will see all the material and projects for this class. Select the appropriate project you want to work on Once you are in the ECO3253 tab, you should see a list of individual projects we will work on throughout the semester. It should look something like this: For our first class on R you will select the link that says introRClass. That is it! Now we are ready to get to work! 2.3.2 Using R via RStudio Recall our car analogy from above. Much as we dont drive a car by interacting directly with the engine but rather by interacting with elements on the cars dashboard, we wont be using R directly but rather we will use RStudios interface. After you open RStudio Cloud and follow the previous instructions, you should see a panel like the following: Note the three panes, which are three panels dividing the screen: The Console pane, the Files pane, and the Environment pane. Over the course of this chapter, youll come to learn what purpose each of these panes serve. 2.4 How do I code in R? Now that youre set up with RStudio Cloud, you are probably asking yourself OK. Now how do I use R? The first thing to note as that unlike other statistical software programs like Excel, STATA, or SAS that provide point and click interfaces, R is an interpreted language, meaning you have to enter in R commands written in R code. In other words, you have to code/program in R. Note that well use the terms coding and programming interchangeably in this book. While it is not required to be a seasoned coder/computer programmer to use R, there is still a set of basic programming concepts that R users need to understand. Consequently, while this course is not a book on programming, you will still learn just enough of these basic programming concepts needed to explore and analyze data effectively. 2.4.1 Basic programming concepts and terminology We now introduce some basic programming concepts and terminology. Instead of asking you to learn all these concepts and terminology right now, well guide you so that youll learn by doing. Note that in this book we will always use a different font to distinguish regular text from computer_code. The best way to master these topics is, in our opinions, learning by doing and lots of repetition. Basics: Console: Where you enter in commands. Running code: The act of telling R to perform an action by giving it commands in the console. Objects: Where values are saved in R. In order to do useful and interesting things in R, we will want to assign a name to an object. For example we could do the following assignments: x &lt;- 44 - 20 and three &lt;- 3. This would allow us to run x + three which would return 27. Data types: Integers, doubles/numerics, logicals, and characters. Vectors: A series of values. These are created using the c() function, where c() stands for combine or concatenate. For example: c(6, 11, 13, 31, 90, 92). Factors: Categorical data are represented in R as factors. Data frames: Data frames are like rectangular spreadsheets: they are representations of datasets in R where the rows correspond to observations and the columns correspond to variables that describe the observations. Well cover data frames later in Section 2.7.1. Conditionals: Testing for equality in R using == (and not = which is typically used for assignment). Ex: 2 + 1 == 3 compares 2 + 1 to 3 and is correct R code, while 2 + 1 = 3 will return an error. Boolean algebra: TRUE/FALSE statements and mathematical operators such as &lt; (less than), &lt;= (less than or equal), and != (not equal to). Logical operators: &amp; representing and as well as | representing or. Ex: (2 + 1 == 3) &amp; (2 + 1 == 4) returns FALSE since both clauses are not TRUE (only the first clause is TRUE). On the other hand, (2 + 1 == 3) | (2 + 1 == 4) returns TRUE since at least one of the two clauses is TRUE. Functions, also called commands: Functions perform tasks in R. They take in inputs called arguments and return outputs. You can either manually specify a functions arguments or use the functions default values. This list is by no means an exhaustive list of all the programming concepts and terminology needed to become a savvy R user; such a list would be so large it wouldnt be very useful, especially for novices. Rather, we feel this is a minimally viable list of programming concepts and terminology you need to know before getting started. Im confident you can learn the rest as you go. Remember that your mastery of all of these concepts and terminology will build as you practice more and more.1 2.5 In Class Exercise Try running the following commands in the console. What do you see for each one? 2*3 2*pi log(10) exp(2) sqrt(25) 3==3 3==4 3&lt;=4 3!=4 x &lt;- c(1,3,2,5)# this is called a &#39;vector&#39; x #what do you see? x &lt;- c(1,6,2) x #now what do you see? y &lt;- c(1,4,3) # USE ARROW! length(x) length(y) x+y #write this write this again 2.5.1 Errors, warnings, and messages Noticed the last thing that appeared in the console when you wrote write this again? It had scary red letters. It is an example of something that intimidates new R and RStudio users: how it reports errors, warnings, and messages. R reports errors, warnings, and messages in a glaring red font, which makes it seem like it is scolding you. However, seeing red text in the console is not always bad. R will show red text in the console pane in three different situations: Errors: When the red text is a legitimate error, it will be prefaced with Error in and try to explain what went wrong. Generally when theres an error, the code will not run. For example, well see in Subsection 2.6.3 if you see Error in ggplot(...) : could not find function \"ggplot\", it means that the ggplot() function is not accessible because the package that contains the function (ggplot2) was not loaded with library(ggplot2). Thus you cannot use the ggplot() function without the ggplot2 package being loaded first. Warnings: When the red text is a warning, it will be prefaced with Warning: and R will try to explain why theres a warning. Generally your code will still work, but with some caveats. For example, you will see in Chapter 3 if you create a scatterplot based on a dataset where one of the values is missing, you will see this warning: Warning: Removed 1 rows containing missing values (geom_point). R will still produce the scatterplot with all the remaining values, but it is warning you that one of the points isnt there. Messages: When the red text doesnt start with either Error or Warning, its just a friendly message. Youll see these messages when you load R packages in the upcoming Subsection 2.6.2 or when you read data saved in spreadsheet files with the read_csv() function as youll see in Chapter ??. These are helpful diagnostic messages and they dont stop your code from working. Additionally, youll see these messages when you install packages too using install.packages(). Important! When you see red text in the console, dont panic! Just check out what it could be. Remember, when you see red text in the console, dont panic. It doesnt necessarily mean anything is wrong. Rather: If the text starts with Error, figure out whats causing it. Think of errors as a red traffic light: something is wrong! If the text starts with Warning, figure out if its something to worry about. For instance, if you get a warning about missing values in a scatterplot and you know there are missing values, youre fine. If thats surprising, look at your data and see whats missing. Think of warnings as a yellow traffic light: everything is working fine, but watch out/pay attention. Otherwise the text is just a message. Read it, wave back at R, and thank it for talking to you. Think of messages as a green traffic light: everything is working fine. 2.5.2 Tips on learning to code Learning to code/program is very much like learning a foreign language, it can be very daunting and frustrating at first. Such frustrations are very common and it is very normal to feel discouraged as you learn. However just as with learning a foreign language, if you put in the effort and are not afraid to make mistakes, anybody can learn. Here are a few useful tips to keep in mind as you learn to program: Remember that computers are not actually that smart: You may think your computer or smartphone are smart, but really people spent a lot of time and energy designing them to appear smart. Rather you have to tell a computer everything it needs to do. Furthermore the instructions you give your computer cant have any mistakes in them, nor can they be ambiguous in any way. Take the copy, paste, and tweak approach: Especially when learning your first programming language, it is often much easier to taking existing code that you know works and modify it to suit your ends, rather than trying to write new code from scratch. We call this the copy, paste, and tweak approach. So early on, we suggest not trying to write code from memory, but rather take existing examples we have provided you, then copy, paste, and tweak them to suit your goals. Dont be afraid to play around! The best way to learn to code is by doing: Rather than learning to code for its own sake, we feel that learning to code goes much smoother when you have a goal in mind or when you are working on a particular project, like analyzing data that you are interested in. Practice is key: Just as the only method to improving your foreign language skills is through practice, practice, and practice; so also the only method to improving your coding is through practice, practice, and practice. Dont worry however; well give you plenty of opportunities to do so! 2.6 What are R packages? Another point of confusion with many new R users is the idea of an R package. R packages extend the functionality of R by providing additional functions, data, and documentation. They are written by a world-wide community of R users and can be downloaded for free from the internet. For example, among the many packages we will use in this book are the ggplot2 package for data visualization which we will cover later (or you can check here) or the dplyr package for data wrangling (again, we will cover later, but check this if you want to know more). A good analogy for R packages is they are like apps you can download onto a mobile phone: R: A new phone R Packages: Apps you can download So R is like a new mobile phone: while it has a certain amount of features when you use it for the first time, it doesnt have everything. R packages are like the apps you can download onto your phone from Apples App Store or Androids Google Play. Lets continue this analogy by considering the Instagram app for editing and sharing pictures. Say you have purchased a new phone and you would like to share a recent photo you have taken on Instagram. You need to: Install the app: Since your phone is new and does not include the Instagram app, you need to download the app from either the App Store or Google Play. You do this once and youre set. You might do this again in the future any time there is an update to the app. Open the app: After youve installed Instagram, you need to open the app. Once Instagram is open on your phone, you can then proceed to share your photo with your friends and family. The process is very similar for using an R package. You need to: Install the package: This is like installing an app on your phone. Most packages are not installed by default when you install R and RStudio. Thus if you want to use a package for the first time, you need to install it first. Once youve installed a package, you likely wont install it again unless you want to update it to a newer version. Load the package: Loading a package is like opening an app on your phone. Packages are not loaded by default when you start RStudio on your computer; you need to load each package you want to use every time you start RStudio. Lets now show you how to perform these two steps for the ggplot2 package for data visualization. 2.6.1 Package installation For the most part, in RStudio Cloud, I will pre-install the packages you are going to need to use. But just in case you also want to work on your own machine, or install your own packages, here I explain that a bit more. There are two ways to install an R package. For example, to install the ggplot2 package: Easy way: In the Files pane of RStudio: Click on the Packages tab Click on Install Type the name of the package under Packages (separate multiple with space or comma): In this case, type ggplot2 Click Install Slightly harder way: An alternative but slightly less convenient way to install a package is by typing install.packages(\"ggplot2\") in the Console pane of RStudio and hitting enter. Note you must include the quotation marks. Much like an app on your phone, you only have to install a package once. However, if you want to update an already installed package to a newer verions, you need to re-install it by repeating the above steps. Learning check (LC2.1) Repeat the above installing steps, but for the dplyr, and knitr packages. This will install the earlier mentioned dplyr package, and the knitr package for writing reports in R. 2.6.2 Package loading Recall that after youve installed a package, you need to load it, in other words open it. We do this by using the library() command. For example, to load the ggplot2 package, run the following code in the Console pane. What do we mean by run the following code? Either type or copy &amp; paste the following code into the Console pane and then hit the enter key. library(ggplot2) If after running the above code, a blinking cursor returns next to the &gt; prompt sign, it means you were successful and the ggplot2 package is now loaded and ready to use. If however, you get a red error message that reads Error in library(ggplot2) : there is no package called ggplot2  it means that you didnt successfully install it. In that case, go back to the previous subsection Package installation and install it. Learning check (LC2.2) Load the dplyr, and knitr packages as well by repeating the above steps. 2.6.3 Package use One extremely common mistake new R users make when wanting to use particular packages is that they forget to load them first by using the library() command we just saw. Remember: you have to load each package you want to use every time you start RStudio. If you dont first load a package, but attempt to use one of its features, youll see an error message similar to: Error: could not find function R is telling you that you are trying to use a function in a package that has not yet been loaded. Almost all new users forget do this when starting out, and it is a little annoying to get used to. However, youll remember with pratice. 2.7 Hands-on exercise! 2.7.1 Explore your first dataset: economic mobility in the US Lets put everything weve learned so far into practice and start exploring some real data! These spreadsheet-type datasets are called data frames in R; we will focus on working with data saved as data frames throughout this course. Step 1: Load all the packages needed for this exercise (assuming youve already installed them). library(dplyr) library(tibble) atlas &lt;- readRDS(gzcon(url(&quot;https://raw.githubusercontent.com/jrm87/ECO3253_repo/master/data/atlas.rds&quot;))) atlas&lt;-tibble(atlas) 2.7.2 Economic mobility data The Opportunity Atlas is a freely available interactive mapping tool that traces the roots of outcomes such as poverty and incarceration back to the neighborhoods in which children grew up. The atlas dataset we loaded has the underlying data to describe equality of opportunity across the 73,278 different neighborhoods in the United States. Lets unpack these data a bit more! 2.7.3 atlas data frame We will begin by exploring the atlas data frame we just loaded to get an idea of its structure. Run the following code in your console (either by typing it or cutting &amp; pasting it): it loads the atlas dataset into your Console. Note depending on the size of your monitor, the output may vary slightly. atlas ## # A tibble: 73,278 x 62 ## tract county state cz czname hhinc_mean2000 mean_commutetime2000 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 20100 1 1 11101 Montgomery 68639. 26.2 ## 2 20200 1 1 11101 Montgomery 57243. 24.8 ## 3 20300 1 1 11101 Montgomery 75648. 25.3 ## 4 20400 1 1 11101 Montgomery 74852. 23.0 ## 5 20500 1 1 11101 Montgomery 96175. 26.2 ## 6 20600 1 1 11101 Montgomery 68096. 21.6 ## 7 20700 1 1 11101 Montgomery 65182. 23.2 ## 8 20801 1 1 11101 Montgomery 76874. 30.3 ## 9 20802 1 1 11101 Montgomery 77310. 30.7 ## 10 20900 1 1 11101 Montgomery 66234. 36.4 ## # ... with 73,268 more rows, and 55 more variables: frac_coll_plus2010 &lt;dbl&gt;, ## # frac_coll_plus2000 &lt;dbl&gt;, foreign_share2010 &lt;dbl&gt;, med_hhinc2016 &lt;dbl&gt;, ## # med_hhinc1990 &lt;dbl&gt;, popdensity2000 &lt;dbl&gt;, poor_share2010 &lt;dbl&gt;, ## # poor_share2000 &lt;dbl&gt;, poor_share1990 &lt;dbl&gt;, share_black2010 &lt;dbl&gt;, ## # share_hisp2010 &lt;dbl&gt;, share_asian2010 &lt;dbl&gt;, share_black2000 &lt;dbl&gt;, ## # share_white2000 &lt;dbl&gt;, share_hisp2000 &lt;dbl&gt;, share_asian2000 &lt;dbl&gt;, ## # gsmn_math_g3_2013 &lt;dbl&gt;, rent_twobed2015 &lt;dbl&gt;, ... Lets unpack this output: A tibble: 73,278 x 62: A tibble is a kind of data frame used in R. This particular data frame has 73,278 rows (one for each neighborhood) 62 columns corresponding to 62 variables describing each observation (e.g. neighborhood in this case) tract county state cz czname hhinc_mean2000 mean_commutetime2000 ... are different columns, in other words variables, of this data frame. We then have the first 10 rows of observations corresponding to 10 neighborhoods. ... with 73,268 more rows, and 52 more variables: indicating to us that 73,268 more rows of data and 52 more variables could not fit in this screen. Unfortunately, this output does not allow us to explore the data very well. Lets look at different tools to explore data frames. 2.7.4 Exploring data frames Among the many ways of getting a feel for the data contained in a data frame such as atlas, we present three functions that take as their argument, in other words their input, the data frame in question. We also include a fourth method for exploring one particular column of a data frame: Using the View() function built for use in RStudio. We will use this the most. Using the glimpse() function, which is included in the dplyr package. Using the $ operator to view a single variable in a data frame. 1. View(): Run View(atlas) in your Console in RStudio, either by typing it or cutting &amp; pasting it into the Console pane, and explore this data frame in the resulting pop-up viewer. You should get into the habit of always Viewing any data frames that come your way. Note the capital V in View. R is case-sensitive so youll receive an error is you run view(atlas) instead of View(atlas). Learning check (LC2.3) What does any ONE row in this atlas dataset refer to? A. Data on an neighborhood B. Data on a state C. Data on an person D. Data on multiple neighborhood By running View(atlas), we see the different variables listed in the columns and we see that there are different types of variables. Some of the variables like poor_share2010, hhinc_mean2000, and share_hisp2010 are what we will call quantitative variables. These variables are numerical in nature. Other variables, like tract are categorical: they are just names (even if they have numbers). For example tract represents a Tract FIPS Code, that is, a 6-digit code assigned by the census folks to each neighborhood in 2010. Note that if you look in the leftmost column of the View(atlas) output, you will see a column of numbers. These are the row numbers of the dataset. If you glance across a row with the same number, say row 5, you can get an idea of what each row corresponds to. 2. glimpse(): The second way to explore a data frame is using the glimpse() function included in the dplyr package. Thus, you can only use the glimpse() function after youve loaded the dplyr package. This function provides us with an alternative method for exploring a data frame: glimpse(atlas) ## Rows: 73,278 ## Columns: 62 ## $ tract &lt;dbl&gt; 20100, 20200, 20300, 20400, 20500, 20600,~ ## $ county &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3,~ ## $ state &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,~ ## $ cz &lt;dbl&gt; 11101, 11101, 11101, 11101, 11101, 11101,~ ## $ czname &lt;chr&gt; &quot;Montgomery&quot;, &quot;Montgomery&quot;, &quot;Montgomery&quot;,~ ## $ hhinc_mean2000 &lt;dbl&gt; 68639, 57243, 75648, 74852, 96175, 68096,~ ## $ mean_commutetime2000 &lt;dbl&gt; 26.2, 24.8, 25.3, 23.0, 26.2, 21.6, 23.2,~ ## $ frac_coll_plus2010 &lt;dbl&gt; 0.2544, 0.2672, 0.1642, 0.2527, 0.3751, 0~ ## $ frac_coll_plus2000 &lt;dbl&gt; 0.1565, 0.1469, 0.2244, 0.2305, 0.3212, 0~ ## $ foreign_share2010 &lt;dbl&gt; 0.00995, 0.01634, 0.02710, 0.01508, 0.046~ ## $ med_hhinc2016 &lt;dbl&gt; 66000, 41107, 51250, 52704, 52463, 63750,~ ## $ med_hhinc1990 &lt;dbl&gt; 27375, 19000, 29419, 37891, 41516, 29000,~ ## $ popdensity2000 &lt;dbl&gt; 195.72, 566.38, 624.20, 713.80, 529.93, 4~ ## $ poor_share2010 &lt;dbl&gt; 0.1050, 0.1476, 0.0804, 0.0632, 0.0596, 0~ ## $ poor_share2000 &lt;dbl&gt; 0.1268, 0.2271, 0.0766, 0.0455, 0.0368, 0~ ## $ poor_share1990 &lt;dbl&gt; 0.0989, 0.1983, 0.1140, 0.0679, 0.0547, 0~ ## $ share_black2010 &lt;dbl&gt; 0.1192, 0.5650, 0.1980, 0.0467, 0.1397, 0~ ## $ share_hisp2010 &lt;dbl&gt; 0.02301, 0.03456, 0.02579, 0.01938, 0.032~ ## $ share_asian2010 &lt;dbl&gt; 0.004707, 0.002304, 0.004744, 0.003648, 0~ ## $ share_black2000 &lt;dbl&gt; 0.0755, 0.6221, 0.1491, 0.0259, 0.0601, 0~ ## $ share_white2000 &lt;dbl&gt; 0.897, 0.355, 0.820, 0.938, 0.897, 0.799,~ ## $ share_hisp2000 &lt;dbl&gt; 0.00625, 0.00846, 0.01647, 0.02217, 0.015~ ## $ share_asian2000 &lt;dbl&gt; 0.003644, 0.003171, 0.003893, 0.007288, 0~ ## $ gsmn_math_g3_2013 &lt;dbl&gt; 2.76, 2.76, 2.76, 2.76, 2.76, 2.76, 2.76,~ ## $ rent_twobed2015 &lt;dbl&gt; NA, 907, 583, 713, 923, 765, 645, 532, 67~ ## $ singleparent_share2010 &lt;dbl&gt; 0.1139, 0.4885, 0.2281, 0.2275, 0.2597, 0~ ## $ singleparent_share1990 &lt;dbl&gt; 0.1812, 0.3525, 0.1259, 0.1268, 0.0744, 0~ ## $ singleparent_share2000 &lt;dbl&gt; 0.251, 0.393, 0.245, 0.191, 0.168, 0.289,~ ## $ traveltime15_2010 &lt;dbl&gt; 0.2730, 0.1520, 0.2055, 0.3507, 0.2505, 0~ ## $ emp2000 &lt;dbl&gt; 0.567, 0.493, 0.579, 0.597, 0.661, 0.643,~ ## $ mail_return_rate2010 &lt;dbl&gt; 83.5, 81.3, 79.5, 83.5, 77.3, 82.8, 83.2,~ ## $ ln_wage_growth_hs_grad &lt;dbl&gt; 0.03823, 0.08931, -0.17774, -0.07231, -0.~ ## $ jobs_total_5mi_2015 &lt;dbl&gt; 10109, 9948, 10387, 12933, 12933, 9193, 1~ ## $ jobs_highpay_5mi_2015 &lt;dbl&gt; 3396, 3328, 3230, 3635, 3635, 3052, 3389,~ ## $ nonwhite_share2010 &lt;dbl&gt; 0.1627, 0.6111, 0.2476, 0.0812, 0.2162, 0~ ## $ popdensity2010 &lt;dbl&gt; 504.8, 1682.2, 1633.4, 1780.0, 2446.3, 11~ ## $ ann_avg_job_growth_2004_2013 &lt;dbl&gt; -0.00677, -0.00425, 0.01422, -0.01984, 0.~ ## $ job_density_2013 &lt;dbl&gt; 92.133, 971.318, 340.920, 207.386, 800.27~ ## $ kfr_natam_p25 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N~ ## $ kfr_natam_p75 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N~ ## $ kfr_natam_p100 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N~ ## $ kfr_asian_p25 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N~ ## $ kfr_asian_p75 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N~ ## $ kfr_asian_p100 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N~ ## $ kfr_black_p25 &lt;dbl&gt; 26819, 18138, 20515, 12883, 26594, 19108,~ ## $ kfr_black_p75 &lt;dbl&gt; 45926, 33842, 34133, 40334, 42575, 26062,~ ## $ kfr_black_p100 &lt;dbl&gt; 84690, 60512, 56516, 105250, 72565, 35737~ ## $ kfr_hisp_p25 &lt;dbl&gt; NA, NA, NA, 26363, 17234, NA, NA, NA, 329~ ## $ kfr_hisp_p75 &lt;dbl&gt; NA, NA, NA, 67532, 44642, NA, NA, NA, 655~ ## $ kfr_hisp_p100 &lt;dbl&gt; NA, NA, NA, NA, 93976, NA, NA, NA, 147274~ ## $ kfr_pooled_p25 &lt;dbl&gt; 27621, 22303, 28215, 33331, 34633, 23583,~ ## $ kfr_pooled_p75 &lt;dbl&gt; 51531, 46650, 50754, 52337, 57007, 47735,~ ## $ kfr_pooled_p100 &lt;dbl&gt; 78922, 74225, 76055, 72586, 81792, 75188,~ ## $ kfr_white_p25 &lt;dbl&gt; 30328, 42189, 33670, 34181, 39540, 27835,~ ## $ kfr_white_p75 &lt;dbl&gt; 50820, 54239, 51579, 52848, 58699, 51198,~ ## $ kfr_white_p100 &lt;dbl&gt; 75126, 66646, 71991, 74330, 80415, 80144,~ ## $ count_pooled &lt;dbl&gt; 519, 530, 960, 1123, 1867, 994, 772, 632,~ ## $ count_white &lt;dbl&gt; 457, 173, 774, 1033, 1626, 756, 630, 523,~ ## $ count_black &lt;dbl&gt; 42, 336, 151, 40, 137, 198, 111, 89, 290,~ ## $ count_asian &lt;dbl&gt; 3, 1, 1, 6, 13, 2, 1, 1, 5, 0, 0, 0, 0, 3~ ## $ count_hisp &lt;dbl&gt; 4, 5, 21, 37, 39, 19, 14, 9, 29, 17, 7, 3~ ## $ count_natam &lt;dbl&gt; 6, 1, 2, 0, 8, 2, 9, 1, 5, 4, 8, 3, 20, 8~ We see that glimpse() will give you the first few entries of each variable in a row after the variable. In addition, the data type of the variable is given immediately after each variables name inside &lt; &gt;. Here, int and dbl refer to integer and double, which are computer coding terminology for quantitative/numerical variables. In contrast, chr refers to character, which is computer terminology for text data. Text data, such as the czname (the name of the metro area), are categorical variables. 3. $ operator Lastly, the $ operator allows us to explore a single variable within a data frame. For example, run the following in your console atlas$tract We used the $ operator to extract only the tract variable and return it as a vector of length 73,278. We will only be occasionally exploring data frames using this operator, instead favoring the View() and glimpse() functions. 2.8 Conclusion Weve given you what we feel are the most essential concepts to know before you can start exploring data in R. There is much more to explore in R but this is a great place to get started! 2.8.1 Additional resources If you want to dive more and feel you could benefit from a more detailed introduction, check this short book: Getting used to R, RStudio, and R Markdown short book. It has screencast recordings that you can follow along and pause as you learn. Furthermore, there is an introduction to R Markdown, a tool used for reproducible research in R. We will see more about that in the next class. If you truly insist on getting more information, you can check this link explaining some of the basics: https://rstudio-education.github.io/hopr/basics.html ;but again, this is not required or expected. "],["viz.html", "Chapter 3 Data Visualization 3.1 The Grammar of Graphics 3.2 Three Important Graphs - 3.3 Scatterplots 3.4 Histograms 3.5 Barplots 3.6 Conclusion", " Chapter 3 Data Visualization This chapter is based in big part on the chapter on visualization by the folks at Northwestern. Please be check that link if you want to dive into how to make even cooler graphs than the ones we will cover here. We will learn basic tools to visualize our data. By visualizing our data, we gain valuable insights that we couldnt initially see from just looking at the raw data in spreadsheet form. We will use the ggplot2 package as it provides an easy way to customize your plots. ggplot2 is rooted in the data visualization theory known as The Grammar of Graphics (wilkinson2005?). At the most basic level, graphics/plots/charts (we use these terms interchangeably in this guide) provide a nice way for us to get a sense for how quantitative variables compare in terms of their center (where the values tend to be located) and their spread (how they vary around the center). Graphics should be designed to emphasize the findings and insight you want your audience to understand. This does however require a balancing act. On the one hand, you want to highlight as many meaningful relationships and interesting findings as possible; on the other you dont want to include so many as to overwhelm your audience. As we will see, plots/graphics also help us to identify patterns in our data. We will see that a common extension of these ideas is to compare the distribution of one quantitative variable (i.e., what the spread of a variable looks like or how the variable is distributed in terms of its values) as we go across the levels of a different categorical variable. Needed data and packages Lets load all the packages and data needed for this chapter (this assumes youve already installed them). Read Section 2.6 for information on how to install and load R packages. As before, we will load the atlas dataset as well. We will also plot some of the information in the package gapminder for some of our examples. atlas &lt;- readRDS(gzcon(url(&quot;https://raw.githubusercontent.com/jrm87/ECO3253_repo/master/data/atlas.rds&quot;))) library(ggplot2) library(dplyr) library(gapminder) 3.1 The Grammar of Graphics We begin with a discussion of a theoretical framework for data visualization known as The Grammar of Graphics, which serves as the foundation for the ggplot2 package. Think of how we construct sentences in English to form sentences by combining different elements, like nouns, verbs, particles, subjects, objects, etc. However, we cant just combine these elements in any arbitrary order; we must do so following a set of rules known as a linguistic grammar. Similarly to a linguistic grammar, The Grammar of Graphics define a set of rules for constructing statistical graphics by combining different types of layers. This grammar was created by Leland Wilkinson (wilkinson2005?) and has been implemented in a variety of data visualization software including R. 3.1.1 Components of the Grammar In short, the grammar tells us that: A statistical graphic is a mapping of data variables to aesthetic attributes of geometric objects. Specifically, we can break a graphic into the following three essential components: data: the data set composed of variables that we map. geom: the geometric object in question. This refers to the type of object we can observe in a plot. For example: points, lines, and bars. aes: aesthetic attributes of the geometric object. For example, x-position, y-position, color, shape, and size. Each assigned aesthetic attribute can be mapped to a variable in our data set. You might be wondering why we wrote the terms data, geom, and aes in a computer code type font. Well see very shortly that well specify the elements of the grammar in R using these terms. However, lets first break down the grammar with an example unrelated to our mobility data, but worry not! We will return to the atlas data. First 3.1.2 Gapminder data In February 2006, a statistician named Hans Rosling gave a TED talk titled The best stats youve ever seen where he presented global economic, health, and development data from the website gapminder.org. For example, for the 142 countries included from 2007, lets consider only the first 6 countries when listed alphabetically in Table 3.1. Table 3.1: Gapminder 2007 Data: First 6 of 142 countries Country Continent Life Expectancy Population GDP per Capita Afghanistan Asia 43.8 31889923 975 Albania Europe 76.4 3600523 5937 Algeria Africa 72.3 33333216 6223 Angola Africa 42.7 12420476 4797 Argentina Americas 75.3 40301927 12779 Australia Oceania 81.2 20434176 34435 Each row in this table corresponds to a country in 2007. For each row, we have 5 columns: Country: Name of country. Continent: Which of the five continents the country is part of. (Note that Americas includes countries in both North and South America and that Antarctica is excluded.) Life Expectancy: Life expectancy in years. Population: Number of people living in the country. GDP per Capita: Gross domestic product (in US dollars). Now consider Figure 3.1, which plots this data for all 142 countries in the data. Figure 3.1: Life Expectancy over GDP per Capita in 2007 Lets view this plot through the grammar of graphics: The data variable GDP per Capita gets mapped to the x-position aesthetic of the points. The data variable Life Expectancy gets mapped to the y-position aesthetic of the points. The data variable Population gets mapped to the size aesthetic of the points. The data variable Continent gets mapped to the color aesthetic of the points. Well see shortly that data corresponds to the particular data frame where our data is saved and a data variable corresponds to a particular column in the data frame. Furthermore, the type of geometric object considered in this plot are points. That being said, while in this example we are considering points, graphics are not limited to just points. Other plots involve lines while others involve bars. Lets summarize the three essential components of the Grammar in Table 3.2. Table 3.2: Summary of Grammar of Graphics for this plot data variable aes geom GDP per Capita x point Life Expectancy y point Population size point Continent color point 3.1.3 Other components There are other components of the Grammar of Graphics we can control as well. As you start to delve deeper into the Grammar of Graphics, youll start to encounter these topics more frequently. In this book however, well keep things simple and only work with the two additional components listed below: faceting breaks up a plot into small multiples corresponding to the levels of another variable (Section ??) position adjustments for barplots (Section 3.5) Other more complex components like scales and coordinate systems are left for a more advanced text such as R for Data Science (rds2016?). Generally speaking, the Grammar of Graphics allows for a high degree of customization of plots and also a consistent framework for easily updating and modifying them. 3.1.4 ggplot2 package In this book, we will be using the ggplot2 package for data visualization, which is an implementation of the Grammar of Graphics for R (R-ggplot2?). As we noted earlier, a lot of the previous section was written in a computer code type font. This is because the various components of the Grammar of Graphics are specified in the ggplot() function included in the ggplot2 package, which expects at a minimum as arguments (i.e. inputs): The data frame where the variables exist: the data argument. The mapping of the variables to aesthetic attributes: the mapping argument which specifies the aesthetic attributes involved. After weve specified these components, we then add layers to the plot using the + sign. The most essential layer to add to a plot is the layer that specifies which type of geometric object we want the plot to involve: points, lines, bars, and others. Other layers we can add to a plot include layers specifying the plot title, axes labels, visual themes for the plots, and facets (which well see in Section ??). Lets now put the theory of the Grammar of Graphics into practice. 3.2 Three Important Graphs - In order to keep things simple, we will only focus on 3 types of graphics in this section, each with a commonly given name. scatterplots histograms barplots We will discuss some variations of these plots, but with this basic repertoire of graphics in your toolbox you can visualize a wide array of different variable types. Note that certain plots are only appropriate for categorical variables and while others are only appropriate for quantitative variables. Youll want to quiz yourself often as we go along on which plot makes sense a given a particular problem or data set. 3.3 Scatterplots The simplest of the figrue we will cover are scatterplots, also called bivariate plots. They allow you to visualize the relationship between two numerical variables. While you may already be familiar with scatterplots, lets view them through the lens of the Grammar of Graphics. Specifically, we will visualize the relationship across neighborhoods between the following two numerical variables in the atlas data frame: kfr_pooled_p25: upward mobility for children with parents on the percentile 25 on the horizontal y axis med_hhinc2016: median household income in 2016 on the vertical x axis 3.3.1 Scatterplots via geom_point Lets now go over the code that will create the desired scatterplot, keeping in mind our discussion on the Grammar of Graphics in Section 3.1. Well be using the ggplot() function included in the ggplot2 package. ggplot(data = atlas, mapping = aes(x = med_hhinc2016, y = kfr_pooled_p25)) + geom_point() Lets break this down piece-by-piece: Within the ggplot() function, we specify two of the components of the Grammar of Graphics as arguments (i.e. inputs): The data frame to be atlas by setting data = atlas. The aesthetic mapping by setting aes(x = med_hhinc2016, y = kfr_pooled_p25). Specifically: the variable med_hhinc2016 maps to the x position aesthetic the variable kfr_pooled_p25 maps to the y position aesthetic We add a layer to the ggplot() function call using the + sign. The layer in question specifies the third component of the grammar: the geometric object. In this case the geometric object are points, set by specifying geom_point(). After running the above code, youll notice two outputs: a warning message and the graphic shown in Figure 3.2. Lets first unpack the warning message: y axis ## Warning: Removed 1372 rows containing missing values (geom_point). Figure 3.2: Median Household Income in 2016 vs Mobility for Children with Parents in Percentile 25 After running the above code, R returns a warning message alerting us to the fact that 1372 rows were ignored due to them being missing. For 1372 rows either the value for med_hhinc2016 or kfr_pooled_p25 or both were missing (recorded in R as NA), and thus these rows were ignored in our plot. Turning our attention to the resulting scatterplot in Figure 3.2, we see that a positive relationship exists between med_hhinc2016 and kfr_pooled_p25: as the median income level of the neighborhood increases, the mobility for children of parents in the percentile 25 tend to also increase. Before we continue, lets consider a few more notes on the layers in the above code that generated the scatterplot: Note that the + sign comes at the end of lines, and not at the beginning. Youll get an error in R if you put it at the beginning. When adding layers to a plot, you are encouraged to start a new line after the + so that the code for each layer is on a new line. As we add more and more layers to plots, youll see this will greatly improve the legibility of your code. To stress the importance of adding layers in particular the layer specifying the geometric object, consider Figure 3.3 where no layers are added. A not very useful plot! ggplot(data = atlas, mapping = aes(x = med_hhinc2016, y = kfr_pooled_p25)) Figure 3.3: Plot with No Layers Learning check (LC3.1) What are some other features of the plot that stand out to you? (LC3.2) Create a new scatterplot using different variables in the atlas data frame by modifying the example above. 3.3.2 Over-plotting Sometimes you end up with a large mass of points, which can cause some confusion as it is hard to tell the true number of points that are plotted. This is the result of a phenomenon called overplotting. As one may guess, this corresponds to values being plotted on top of each other over and over again. It is often difficult to know just how many values are plotted in this way when looking at a basic scatterplot as we have here. The main methods to address the issue of overplotting is: By adjusting the transparency of the points. Changing the transparency The main way of addressing overplotting is by changing the transparency of the points by using the alpha argument in geom_point(). By default, this value is set to 1. We can change this to any value between 0 and 1, where 0 sets the points to be 100% transparent and 1 sets the points to be 100% opaque. Note how the following code is identical to the code in Section 3.3 that created the scatterplot with overplotting, but with alpha = 0.2 added to the geom_point(): ggplot(data = atlas, mapping = aes(x = med_hhinc2016, y = kfr_pooled_p25))+ geom_point(alpha = 0.2) Figure 3.4: Delay scatterplot with alpha=0.2 The key feature to note in Figure 3.4 is that the transparency of the points is cumulative: areas with a high-degree of overplotting are darker, whereas areas with a lower degree are less dark. Note furthermore that there is no aes() surrounding alpha = 0.2. This is because we are not mapping a variable to an aesthetic attribute, but rather merely changing the default setting of alpha. In fact, youll receive an error if you try to change the second line above to read geom_point(aes(alpha = 0.2)). Learning check (LC3.3) Why is setting the alpha argument value useful with scatterplots? What further information does it give you that a regular scatterplot cannot? 3.3.3 Summary Scatterplots display the relationship between two numerical variables. They are among the most commonly used plots because they can provide an immediate way to see the trend in one variable versus another. However, if you try to create a scatterplot where either one of the two variables is not numerical, you might get strange results. Be careful! With medium to large data sets, you may need to play around with the different modifications one can make to a scatterplot. This tweaking is often a fun part of data visualization, since youll have the chance to see different relationships come about as you make subtle changes to your plots. 3.4 Histograms Lets consider the kfr_pooled_p25 variable in the atlas data frame once again, but now we want to understand how the values of kfr_pooled_p25 distribute. In other words, for economic mobility for children from parents in the percentile 25: What are the smallest and largest values? What is the center value? How do the values spread out? What are frequent and infrequent values? One way to visualize this distribution of this single variable kfr_pooled_p25 is to plot what is know as a histogram. A histogram is a plot that visualizes the distribution of a numerical value as follows: We first cut up the x-axis into a series of bins, where each bin represents a range of values. For each bin, we count the number of observations that fall in the range corresponding to that bin. Then for each bin, we draw a bar whose height marks the corresponding count. Lets drill-down on an example of a histogram, shown in Figure 3.5. Figure 3.5: Example histogram. Observe that there are six bins of equal width between $ 30,000 and $ 60,000, thus we have three bins of width $ 5,000 each: one bin for the 30-35k range, and so on, until the bin for the 55-60k range. Since: The bin for the 30-35k range has a height of around 9000, this histogram is telling us that around 9000 neighborhoods in the US have an average mobility measure for children of parents in the percentile 25th of between $30,000 and $35,000. The remaining bins all have a similar interpretation. 3.4.1 Histograms via geom_histogram Lets now present the ggplot() code to plot your first histogram! Unlike with scatterplots and linegraphs, there is now only one variable being mapped in aes(): the single numerical variable temp. The y-aesthetic of a histogram gets computed for you automatically. Furthermore, the geometric object layer is now a geom_histogram() ggplot(data = atlas, mapping = aes(x = kfr_pooled_p25)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 1267 rows containing non-finite values (stat_bin). Figure 3.6: Histogram of mobility for children of p25 in the US. Lets unpack the messages R sent us first. The first message is telling us that the histogram was constructed using bins = 30, in other words 30 equally spaced bins. This is known in computer programming as a default value; unless you override this default number of bins with a number you specify, R will choose 30 by default. Well see in the next section how to change this default number of bins. The second message is telling us once again that there are some missing values: that because one row has a missing NA value for kfr_pooled_p25, it was omitted from the histogram. R is just giving us a friendly heads up that this was the case. Nows lets unpack the resulting histogram in Figure 3.6. Observe that values less than $12,000 as well as values above $80,000 are rather rare. However, because of the large number of bins, its hard to get a sense for which range of temperatures is covered by each bin; everything is one giant amorphous blob. So lets add white vertical borders demarcating the bins by adding a color = \"white\" argument to geom_histogram(): ggplot(data = atlas, mapping = aes(x = kfr_pooled_p25)) + geom_histogram(color = &quot;white&quot;) Figure 3.7: Histogram of mobility for children of p25 in the US with white borders. We can now better associate ranges of mobility to each of the bins. We can also vary the color of the bars by setting the fill argument. Run colors() to see all 657 possible choice of colors! ggplot(data = atlas, mapping = aes(x = kfr_pooled_p25)) + geom_histogram(color = &quot;white&quot;, fill = &quot;steelblue&quot;) Figure 3.8: Histogram of mobility for children of p25 in the US with white borders. 3.4.2 Adjusting the bins Lets now adjust the number of bins in our histogram in one of two methods: By adjusting the number of bins via the bins argument to geom_histogram(). By adjusting the width of the bins via the binwidth argument to geom_histogram(). Using the first method, we have the power to specify how many bins we would like to cut the x-axis up in. As mentioned in the previous section, the default number of bins is 30. We can override this default, to say 40 bins, as follows: ggplot(data = atlas, mapping = aes(x = kfr_pooled_p25)) + geom_histogram(bins = 40, color = &quot;white&quot;) Figure 3.9: Histogram with 40 bins. Using the second method, instead of specifying the number of bins, we specify the width of the bins by using the binwidth argument in the geom_histogram() layer. For example, lets set the width of each bin to be 10°F. ggplot(data = atlas, mapping = aes(x = kfr_pooled_p25)) + geom_histogram(binwidth = 4000, color = &quot;white&quot;) Figure 3.10: Histogram with binwidth 10. Learning check (LC3.4) What does changing the number of bins from 30 to 40 tell us about the distribution of mobility? (LC3.5) Would you classify the distribution of mobility as symmetric or skewed? (LC3.6) What would you guess is the center value in this distribution? Why did you make that choice? (LC3.7) Is this data spread out greatly from the center or is it close? Why? 3.4.3 Summary Histograms, unlike scatterplots, present information on only a single numerical variable. Specifically, they are visualizations of the distribution of the numerical variable in question. 3.5 Barplots Histograms are tools to visualize the distribution of numerical variables. Another common task is visualize the distribution of a categorical variable. This is a simpler task, as we are simply counting different categories, also known as levels, of a categorical variable. Often the best way to visualize these different counts, also known as frequencies, is with a barplot (also known as a barchart). One complication, however, is how your data is represented: is the categorical variable of interest pre-counted or not? For example, run the following code that manually creates two data frames representing a collection of fruit: 3 apples and 2 oranges. fruits &lt;- data_frame( fruit = c(&quot;apple&quot;, &quot;apple&quot;, &quot;orange&quot;, &quot;apple&quot;, &quot;orange&quot;) ) fruits_counted &lt;- data_frame( fruit = c(&quot;apple&quot;, &quot;orange&quot;), number = c(3, 2) ) We see both the fruits and fruits_counted data frames represent the same collection of fruit. Whereas fruits just lists the fruit individually ## # A tibble: 5 x 1 ## fruit ## &lt;chr&gt; ## 1 apple ## 2 apple ## 3 orange ## 4 apple ## 5 orange  fruits_counted has a variable count which represents pre-counted values of each fruit. ## # A tibble: 2 x 2 ## fruit number ## &lt;chr&gt; &lt;dbl&gt; ## 1 apple 3 ## 2 orange 2 Depending on how your categorical data is represented, youll need to use add a different geom layer to your ggplot() to create a barplot, as we now explore. 3.5.1 Barplots via geom_bar or geom_col Lets generate barplots using these two different representations of the same basket of fruit: 3 apples and 2 oranges. Using the fruits data frame where all 5 fruits are listed individually in 5 rows, we map the fruit variable to the x-position aesthetic and add a geom_bar() layer. ggplot(data = fruits, mapping = aes(x = fruit)) + geom_bar() Figure 3.11: Barplot when counts are not pre-counted However, using the fruits_counted data frame where the fruit have been pre-counted, we map the fruit variable to the x-position aesthetic as with geom_bar(), but we also map the count variable to the y-position aesthetic, and add a geom_col() layer. ggplot(data = fruits_counted, mapping = aes(x = fruit, y = number)) + geom_col() Figure 3.12: Barplot when counts are pre-counted Compare the barplots in Figures 3.11 and 3.12. They are identical because they reflect count of the same 5 fruit. However depending on how our data is saved, either pre-counted or not, we must add a different geom layer. When the categorical variable whose distribution you want to visualize is: Is not pre-counted in your data frame: use geom_bar(). Is pre-counted in your data frame, use geom_col() with the y-position aesthetic mapped to the variable that has the counts. Lets now go back to the atlas data frame and visualize the distribution of the categorical variable state. In other words, lets visualize the number of neighborhoodsin the data belonging to each state. Recall from Section 2.7.4 when you first explored the atlas data frame you saw that each row corresponds to a neighborhood. In other words the atlas data frame is more like the fruits data frame than the fruits_counted data frame above, and thus we should use geom_bar() instead of geom_col() to create a barplot. Much like a geom_histogram(), there is only one variable in the aes() aesthetic mapping: the variable state gets mapped to the x-position. ggplot(data = atlas, mapping = aes(x = state)) + geom_bar() Figure 3.13: Number of neighborhoods by State in the atlas data using geom_bar Observe in Figure 3.13 that state 06, which is California, has the most number of neighborhoods in the data. If you dont know which State FIPS code correspond to which State, you can see it here. For example: TX is State 48, while NC is State 37. Learning check (LC3.8) Why are histograms inappropriate for visualizing categorical variables? (LC3.9) What is the difference between histograms and barplots? (LC3.10) How many neighborhoods are there in Texas in the atlas data? 3.5.2 Summary Barplots are the preferred way of displaying the distribution of a categorical variable, or in other words the frequency with which the different categories called levels occur. They are easy to understand and make it easy to make comparisons across levels. When trying to visualize two categorical variables, you have many options: stacked barplots, side-by-side barplots, and faceted barplots. Depending on what aspect of the joint distribution you are trying to emphasize, you will need to make a choice between these three types of barplots. 3.6 Conclusion 3.6.1 Summary table Lets recap all five of the three main figures in Table ?? summarizing their differences. Using these 5NG, youll be able to visualize the distributions and relationships of variables contained in a wide array of datasets. 3.6.2 Argument specification Run the following two segments of code. First this: ggplot(data = atlas, mapping = aes(x = state)) + geom_bar() then this: ggplot(atlas, aes(x = state)) + geom_bar() Youll notice that that both code segments create the same barplot, even though in the second segment we omitted the data = and mapping = code argument names. This is because the ggplot() by default assumes that the data argument comes first and the mapping argument comes second. So as long as you specify the data frame in question first and the aes() mapping second, you can omit the explicit statement of the argument names data = and mapping =. Going forward for the rest of this book, all ggplot() will be like the second segment above: with the data = and mapping = explicit naming of the argument omitted and the default ordering of arguments respected. 3.6.3 Additional resources If you want to further unlock the power of the ggplot2 package for data visualization, you can check out RStudios Data Visualization with ggplot2 cheatsheet. This cheatsheet summarizes much more than what weve discussed in this chapter, in particular the many more than the 3 geom geometric objects we covered in this Chapter, while providing quick and easy to read visual descriptions. You can access this cheatsheet by going to the RStudio Menu Bar -&gt; Help -&gt; Cheatsheets -&gt; Data Visualization with ggplot2: Figure 3.14: Data Visualization with ggplot2 cheatsheat "],["wrangling.html", "Chapter 4 Data Wrangling 4.1 The pipe operator: %&gt;% 4.2 filter rows 4.3 mutate existing variables 4.4 Other verbs 4.5 Conclusion", " Chapter 4 Data Wrangling So far in our journey, weve seen how to look at data saved in data frames using the glimpse() and View() functions in Chapter 2 on and how to create data visualizations using the ggplot2 package in Chapter 3. In particular we studied what we term the the following three graphs: scatterplots via geom_point() histograms via geom_histogram() barplots via geom_bar() or geom_col() We created these visualizations using the Grammar of Graphics, which maps variables in a data frame to the aesthetic attributes of one the above 3 geometric objects. We can also control other aesthetic attributes of the geometric objects such as the size and color as seen in the Gapminder data example in Figure 3.1. Recall however in Section ?? we discussed that for two of our visualizations we needed transformed/modified versions of existing data frames. Recall for example the scatterplot of economic mobility only for neighborhoods in Wisconsin. In order to create this visualization, we needed to first pare down the atlas data frame to a new data frame atlas_wisconsin. consisting of only state == 55 neighborhoods using the filter() function. atlas_wisconsin &lt;- atlas %&gt;% filter(state == 55) ggplot(data = atlas_wisconsin, mapping = aes(x = med_hhinc2016, y = kfr_pooled_p25)) + geom_point() In this chapter, well introduce two very useful functions from the dplyr package that will allow you to take a data frame and filter() its existing rows to only pick out a subset of them. For example, the atlas_wisconsin data frame above. mutate() its existing columns/variables to create new ones. For example, convert dollars to log dollars. There are other functions we will not cover here, but that could be really useful for you to know if you ever get to work with data outside this course: summarize(), group_by(), arrange() and join(). I will leave those to learn on your own if you want to. You can check this very good explanation. Back to our functions! Notice how we used computer code font to describe the actions we want to take on our data frames. This is because the dplyr package for data wrangling that well introduce in this chapter has intuitively verb-named functions that are easy to remember. Well start by introducing the pipe operator %&gt;%, which allows you to combine multiple data wrangling verb-named functions into a single sequential chain of actions. Needed packages Lets load all the packages needed for this chapter (this assumes youve already installed them). If needed, read Section 2.6 for information on how to install and load R packages. We will also load the atlas data once more. library(dplyr) library(ggplot2) atlas &lt;- readRDS(gzcon(url(&quot;https://raw.githubusercontent.com/jrm87/ECO3253_repo/master/data/atlas.rds&quot;))) 4.1 The pipe operator: %&gt;% Before we start data wrangling, lets first introduce a very nifty tool that gets loaded along with the dplyr package: the pipe operator %&gt;%. Say you would like to perform a hypothetical sequence of operations on a hypothetical data frame x using hypothetical functions f(), g(), and h(): Take x then Use x as an input to a function f() then Use the output of f(x) as an input to a function g() then Use the output of g(f(x)) as an input to a function h() One way to achieve this sequence of operations is by using nesting parentheses as follows: h(g(f(x))) The above code isnt so hard to read since we are applying only three functions: f(), then g(), then h(). However, you can imagine that this can get progressively harder and harder to read as the number of functions applied in your sequence increases. This is where the pipe operator %&gt;% comes in handy. %&gt;% takes one output of one function and then pipes it to be the input of the next function. Furthermore, a helpful trick is to read %&gt;% as then. For example, you can obtain the same output as the above sequence of operations as follows: x %&gt;% f() %&gt;% g() %&gt;% h() You would read this above sequence as: Take x then Use this output as the input to the next function f() then Use this output as the input to the next function g() then Use this output as the input to the next function h() So while both approaches above would achieve the same goal, the latter is much more human-readable because you can read the sequence of operations line-by-line. But what are the hypothetical x, f(), g(), and h()? Throughout this chapter on data wrangling: The starting value x will be a data frame. For example: atlas. The sequence of functions, here f(), g(), and h(), will be a sequence of any number of the 6 data wrangling verb-named functions we listed in the introduction to this chapter. For example: filter(state == 55). The result will be the transformed/modified data frame that you want. For example: a data frame consisting of only the subset of rows in atlas corresponding to neighborhoods in the State of Wisconsin. Much like when adding layers to a ggplot() using the + sign at the end of lines, you form a single chain of data wrangling operations by combining verb-named functions into a single sequence with pipe operators %&gt;% at the end of lines. So continuing our example involving neighborhoods in Wisconsin, we form a chain using the pipe operator %&gt;% and save the resulting data frame in atlas_wisconsin: atlas_wisconsin &lt;- atlas %&gt;% filter(state == 55) Keep in mind, there are many more advanced data wrangling functions than just the 2 listed in the introduction to this chapter; youll see some examples of these in Section 4.4. However, just with these 2 verb-named functions youll be able to perform a broad array of data wrangling tasks for the rest of this course. 4.2 filter rows Figure 4.1: Diagram of The filter() function here works much like the Filter option in Microsoft Excel; it allows you to specify criteria about the values of a variable in your dataset and then filters out only those rows that match that criteria. We begin by focusing only on neighborhoods from the state of North Carolina. The state code (or airport code) for Portland, Oregon is 37. Run the following and look at the resulting spreadsheet to ensure that only neighborhoods from North Carolina are chosen here: atlas_nc&lt;- atlas %&gt;% filter(state == 37) View(atlas_nc) Note the following: The ordering of the commands: Take the atlas data frame then filter the data frame so that only thosewith state equals 37 are included. We test for equality using the double equal sign == and not a single equal sign =. In other words filter(state = 37) will yield an error. This is a convention across many programming languages. If you are new to coding, youll probably forget to use the double equal sign == a few times before you get the hang of it. You can use other mathematical operations beyond just == to form criteria: &gt; corresponds to greater than &lt; corresponds to less than &gt;= corresponds to greater than or equal to &lt;= corresponds to less than or equal to != corresponds to not equal to. The ! is used in many programming languages to indicate not. Furthermore, you can combine multiple criteria together using operators that make comparisons: | corresponds to or &amp; corresponds to and To see many of these in action, lets filter atlas for all rows that: Had a mean household income over 30,000 in the year 2000 and Are in North Carolina or Massachusetts; and Had an employment rate in the year 2000 of at least 80% Run the following: atlas_filter1 &lt;- atlas %&gt;% filter(hhinc_mean2000 &gt;30000 &amp; (state ==37 | state == 25) &amp; emp2000 &gt;= 0.8) View(atlas_filter1) Note that even though colloquially speaking one might say all neighborhoods from North Carolina and Massachusetts in terms of computer operations, we really mean all neighborhoods from North Carolina or Massachusetts. For a given row in the data, state can be 37, 25, or something else, but not 37 and 25 at the same time. Furthermore, note the careful use of parentheses around the state ==37 | state == 25. We can often skip the use of &amp; and just separate our conditions with a comma. In other words the code above will return the identical output atlas_filter1 as this code below: atlas_filter1 &lt;- atlas %&gt;% filter(hhinc_mean2000 &gt;30000, (state ==37 | state == 25), emp2000 &gt;= 0.8) View(atlas_filter1) Lets present another example that uses the ! not operator to pick rows that dont match a criteria. As mentioned earlier, the ! can be read as not. Here we are filtering rows corresponding to neighborhoods that are not in North Carolina or Massachusetts. atlas_not_nc_ma &lt;- atlas %&gt;% filter(!(state ==37 | state == 25)) View(atlas_not_nc_ma) Again, note the careful use of parentheses around the (state ==37 | state == 25). If we didnt use parentheses as follows: atlas %&gt;% filter(!state ==37 | state == 25) We would be returning all neighborhoods not in 37 or those in 25, which is an entirely different resulting data frame. Now say we have a large list of airports we want to filter for, say NC (37), MA (25), FL(12) and PA(42). We could continue to use the | or operator as so: atlas_many_states &lt;- atlas %&gt;% filter(state ==37 | state == 25 | state == 12 | state == 42) View(atlas_many_states) but as we progressively include more airports, this will get unwieldy. A slightly shorter approach uses the %in% operator: atlas_many_states &lt;- atlas %&gt;% filter(state %in% c(37, 25, 12, 42)) View(atlas_many_states) What this code is doing is filtering atlas for all neighborhoods where state is in the list of airports c(37, 25, 12, 42). Recall from Chapter 2 that the c() function combines or concatenates values in a vector of values. Both outputs of atlas_many_states are the same, but as you can see the latter takes much less time to code. As a final note we point out that filter() should often be among the first verbs you apply to your data. This cleans your dataset to only those rows you care about, or put differently, it narrows down the scope of your data frame to just the observations your care about. 4.3 mutate existing variables Figure 4.2: Mutate diagram from Data Wrangling with dplyr and tidyr cheatsheet A common transformation of data is to create/compute new variables based on existing ones. For example, say you are more comfortable thinking of income in terms of the logarithm of income instead of in dollars. We will apply this to the variable hhinc_mean2000 (the mean household income in 2000). You want to implement the following formula: \\[ \\text{income} = log(\\text{income}) \\] We can apply this formula to the hhinc_mean2000 variable using the mutate() function, which takes existing variables and mutates them to create new ones. atlas &lt;- atlas %&gt;% mutate(log_hhinc_mean2000= log(hhinc_mean2000)) View(atlas) Note that we have overwritten the original atlas data frame with a new version that now includes the additional variable log_hhinc_mean2000. In other words, the mutate() command outputs a new data frame which then gets saved over the original atlas data frame. Furthermore, note how in mutate() we used log_hhinc_mean2000= log(hhinc_mean2000) to create a new variable log_hhinc_mean2000. Why did we overwrite the data frame atlas instead of assigning the result to a new data frame like atlas_new, but on the other hand why did we not overwrite hhinc_mean2000, but instead created a new variable called temp_in_C? As a rough rule of thumb, as long as you are not losing original information that you might need later, its acceptable practice to overwrite existing data frames. On the other hand, had we used mutate(hhinc_mean2000 = log(hhinc_mean2000) instead of mutate(log_hhinc_mean2000= log(hhinc_mean2000)), we would have overwritten the original variable hhinc_mean2000 and lost its values. 4.4 Other verbs Here are some other useful data wrangling verbs that might come in handy: select() only a subset of variables/columns rename() variables/columns to have new names Return only the top_n() values of a variable 4.4.1 select variables Figure 4.3: Select diagram from Data Wrangling with dplyr and tidyr cheatsheet Weve seen that the atlas data frame contains 62 different variables. You can identify the names of these 19 variables by running the glimpse() function from the dplyr package: glimpse(atlas) However, say you only need two of these variables, say state and emp2000. You can select() these two variables: atlas %&gt;% select(state, emp2000) This function makes exploring data frames with a very large number of variables easier for humans to process by restricting consideration to only those we care about, like our example with state and emp2000 above. This might make viewing the dataset using the View() spreadsheet viewer more digestible. However, as far as the computer is concerned, it doesnt care how many additional variables are in the data frame in question, so long as state and state are included. Lastly, the helper functions starts_with(), ends_with(), and contains() can be used to select variables/column that match those conditions. For example: atlas_begin_kfr &lt;- atlas %&gt;% select(starts_with(&quot;kfr&quot;)) atlas_begin_kfr 4.4.2 rename variables Another useful function is rename(), which as you may have guessed renames one column to another name. Suppose we want emp2000 and popdensity2010 to be employmentrate2000 and populationdensity2010 instead in the atlas data frame: atlas &lt;- atlas %&gt;% rename(populationdensity2010 = popdensity2010, employmentrate2000 = emp2000) glimpse(atlas) Note that in this case we used a single = sign within the rename(). This is because we are not testing for equality like we would using ==, but instead we want to assign a new variable populationdensity2010 to have the same values as popdensity2010 and then delete the variable popdensity2010. Its easy to forget if the new name comes before or after the equals sign. I usually remember this as New Before, Old After or NBOA. 4.4.3 top_n values of a variable We can also return the top n values of a variable using the top_n() function. For example, we can return a data frame of the top neighborhoods by mobility for children from parents in the percentile 25. Observe that we set the number of values to return to n = 10 and wt = kfr_pooled_p25 to indicate that we want the rows of corresponding to the top 10 values of kfr_pooled_p25. See the help file for top_n() by running ?top_n for more information. atlas %&gt;% top_n(n = 10, wt = kfr_pooled_p25) Lets further arrange() these results in descending order of kfr_pooled_p25: atlas %&gt;% top_n(n = 10, wt = kfr_pooled_p25) %&gt;% arrange(desc(kfr_pooled_p25)) You can read more about the function arrange() here, but the logic is fairly simple. We are organizing the neighborhoods in descending order in the variable kfr_pooled_p25. 4.5 Conclusion 4.5.1 Additional resources If you want to further unlock the power of the dplyr package for data wrangling, we suggest you that you check out RStudios Data Transformation with dplyr cheatsheet. This cheatsheet summarizes much more than what weve discussed in this chapter, in particular more-intermediate level and advanced data wrangling functions, while providing quick and easy to read visual descriptions. You can access this cheatsheet by going to the RStudio Menu Bar -&gt; Help -&gt; Cheatsheets -&gt; Data Transformation with dplyr: Figure 4.4: Data Transformation with dplyr cheatsheat On top of data wrangling verbs and examples we presented in this section, if youd like to see more examples of using the dplyr package for data wrangling check out Chapter 5 of Garrett Grolemund and Hadley Wickhams and Garretts book (rds2016?). "],["appendixA.html", "A Statistical Background A.1 Basic statistical terms", " A Statistical Background A.1 Basic statistical terms A.1.1 Mean The mean, also known as (AKA) the average, is the most commonly reported measure of center. It is commonly called the average though this term can be a little ambiguous. The mean is the sum of all of the data elements divided by how many elements there are. If we have \\(n\\) data points, the mean is given by: \\[\\overline{x} = \\frac{x_1 + x_2 + \\cdots + x_n}{n}\\] Note that you will often see shorthand notation for a sum of numbers using \\(\\sum\\) notation. For example, we could rewrite the formula for \\(\\bar{x}\\) as: \\[\\bar{x} = \\frac{x_1 + x_2 + \\cdots + x_n}{n}=\\frac{\\sum_{i = 1}^n x_i}{n}\\] Weve simply replaced the subscript on each \\(x\\) with a generic index \\(i\\), and use the \\(\\sum\\) notation to indicate we are summing \\(x\\)s with indices (i.e. subscripts) that go from 1 to \\(n\\). When summing numbers in statistics, were almost always dealing with indices that start with the value 1 and go up to a value equal to a sample size (e.g. \\(n\\)), so often you will see an even more shorthand version, where its assumed youre summing from \\(i = 1\\) to \\(i = n\\): \\[\\bar{x} = \\frac{\\sum x_i}{n}\\] A.1.2 Median The median is calculated by first sorting a variables data from smallest to largest. After sorting the data, the middle element in the list is the median. If the middle falls between two values, then the median is the mean of those two values. A.1.3 Standard deviation We will next discuss the standard deviation of a sample dataset pertaining to one variable. The formula can be a little intimidating at first but it is important to remember that it is essentially a measure of how far to expect a given data value is from its mean: \\[Standard \\, deviation = \\sqrt{\\frac{(x_1 - \\overline{x})^2 + (x_2 - \\overline{x})^2 + \\cdots + (x_n - \\overline{x})^2}{n - 1}} = \\sqrt{\\frac{\\sum_{i= 1}^n(x_i - \\overline{x})^2 }{n - 1}}\\] A.1.4 Five-number summary The five-number summary consists of five values: minimum, first quartile AKA 25th percentile, second quartile AKA median AKA 50th percentile, third quartile AKA 75th, and maximum. The quartiles are calculated as first quartile (\\(Q_1\\)): the median of the first half of the sorted data third quartile (\\(Q_3\\)): the median of the second half of the sorted data The interquartile range is defined as \\(Q_3 - Q_1\\) and is a measure of how spread out the middle 50% of values is. The five-number summary is not influenced by the presence of outliers in the ways that the mean and standard deviation are. It is, thus, recommended for skewed datasets. A.1.5 Percentiles or rank Just as you can sort the values of a variable form smallest to larges and find the middle element to get the median, you are dividing all the values into two groups with the same number of observations. If you do that but instead of dividing into 2 groups, you divide it into 4 groups, you get each of the quartiles. If you divide the sorted observations into 100 groups of equal number of observations, you get the percentiles. Depending on the application, the percentile can be read as the rank in the distribution. A.1.6 Distribution The distribution of a variable/dataset corresponds to generalizing patterns in the dataset. It often shows how frequently elements in the dataset appear. It shows how the data varies and gives some information about where a typical element in the data might fall. Distributions are most easily seen through data visualization. A.1.7 Outliers Outliers correspond to values in the dataset that fall far outside the range of ordinary values. In regards to a boxplot (by default), they correspond to values below \\(Q_1 - (1.5 * IQR)\\) or above \\(Q_3 + (1.5 * IQR)\\). Note that these terms (aside from Distribution) only apply to quantitative variables. "],["project1.html", "Project 1 Stories from the Atlas: Describing Data using Maps, Regressions, and Correlations Instructions Data Description Cheatsheat commands", " Project 1 Stories from the Atlas: Describing Data using Maps, Regressions, and Correlations Posted: Friday, August 25, 2022 Part 1: Due at midnight on Wednesday, September 7, 2022 Part 2: Due at midnight on Wednesday, September 14, 2022 The Opportunity Atlas is a freely available interactive mapping tool that traces the roots of outcomes such as poverty and incarceration back to the neighborhoods in which children grew up. Policymakers, journalists, and the public have begun to explore the Opportunity Atlas, casting new light on the geography of upward mobility in communities across the country. As an example, see Jasmine Garsds analysis for the New York City neighborhood of Brownsville in Brooklyn. In this first empirical project, you will use the Opportunity Atlas mapping tool and the underlying data to describe equality of opportunity in your hometown and across the United States. (If you grew up outside the United States, you may select a community in which you have spent some time, such as San Antonio, TX.) The end product will be a short narrative (or story) in which you describe what you have learned from the Atlas, in which you will weave in the narrative along with the data analysis. Below is a list of specific analyses and questions that your narrative must address. The document should contain references, graphs, and maps. This project focuses on the following methods for descriptive data analysis. (The later empirical projects you will do in this class will be focused on causal inference and prediction). Data visualization. Maps are a powerful way to present descriptive statistics for data with a geographic component. You will use maps to display upward mobility statistics for the Census tracts in your hometown. Regression and correlation analysis. You will use linear regressions and correlation coefficients to quantify the statistical relationship between upward mobility and potential explanatory variables. The R data file that you will use in this assignment, atlas.rds, contains an extract of the Opportunity Atlas data. It is also merged on several other variables, which you may use for the correlational analysis. Instructions You will work on RStudio Cloud for this project. Write the narrative within a RMarkdown file in the project1 tab in RStudio Cloud. The deliverables for Part 1 and Part 2 are the following: Part 1: points 1 to 5 Part 2: points 1 to 10 - this should include any feedback you received for Part 1 Notice that Part 2 includes Part 1 The Markdown file is where you will write your narrative, run all your analysis, and the output of each code block should be visible. The output should include references, graphs, maps, and tables. Specific questions to address in your narrative Start by looking up the city where you grew up on the Opportunity Atlas. Zoom in to the Census tracts around your home. Figure 1 in your narrative should be a map of the Census tracts in your hometown from the Opportunity Atlas. Examples for Milwaukee, WI (where Professor Chetty grew up) and Los Angeles, CA (discussed in Lecture 1) are shown on the next page. The text of your narrative should describe what you see, and what data are being visualized. Examine the patterns for a number of different groups (e.g., lowest income children, high income children) and outcomes (e.g., earnings in adulthood, incarceration rates). Only choose one or two of these to include in your narrative. (To answer this question, read the Opportunity Atlas manuscript) What period do the data you are analyzing come from? Are you concerned that the neighborhoods you are studying may have changed for kids now growing up there? What evidence do Chetty et al. (2018) provide suggesting that such changes are or are not important? What type of data could you use to test whether your neighborhood has changed in recent years? Now turn to the atlas.rds data set. How does average upward mobility, pooling races and genders, for children with parents at the 25th percentile (kfr pooled_p25) in your home Census tract compare to mean (population-weighted, using count_pooled) upward mobility in your state and in the U.S. overall? Do kids where you grew up have better or worse chances of climbing the income ladder than the average child in America? Hint: The Opportunity Atlas website will give you the tract, county, and state FIPS codes for your home address. For example, searching for Lynwood Road, Verona, New Jersey will display Tract 34013021000, Verona, NJ. The first two digits refer to the state code, the next three digits refer to the county code, and the last 6 digits refer to the tract code. In R, you can list these observations with the function filter() as follows (assuming you called the data as atlas as in Table 2). If you only want to see kfr_pooled_p25: atlas_lynwood&lt;-atlas%&gt;% filter(state == 34 &amp; county == 013 &amp; tract == 021000) atlas_lynwood%&gt;% select(kfr_pooled_p25) Or to see all the variables for that tract: View(atlas_lynwood) See the cheat-sheet Section @ref(#cheatsheetproject1) below for further help. What is the standard deviation of upward mobility (population-weighted) in your home county? Is it larger or smaller than the standard deviation across tracts in your state? Across tracts in the country? What do you learn from these comparisons? Now lets turn to downward mobility: repeat questions (3) and (4) looking at children who start with parents at the 75th and 100th percentiles. How do the patterns differ? Using a linear regression, estimate the relationship between outcomes of children at the 25th and 75th percentile for the Census tracts in your home county. Generate a scatter plot to visualize this regression. Do areas where children from low-income families do well generally have better outcomes for those from high-income families, too? Next, examine whether the patterns you have looked at above are similar by race. If there is not enough racial heterogeneity in the area of interest (i.e., data is missing for most racial groups), then choose a different area to examine. Using the Census tracts in your home county, can you identify any covariates which help explain some of the patterns you have identified above? Some examples of covariates you might examine include housing prices, income inequality, fraction of children with single parents, job density, etc. For 2 or 3 of these, report estimated correlation coefficients along with their 95% confidence intervals. Open question: formulate a hypothesis for why you see the variation in upward mobility for children who grew up in the Census tracts near your home and provide correlational evidence testing that hypothesis. For this question, many covariates have been provided to you in the atlas.rds file, which are described under the Characteristics of Census tracts header in Table 1. Putting together all the analyses you did above, what have you learned about the determinants of economic opportunity where you grew up? Identify one or two key lessons or takeaways that you might discuss with a policymaker or journalist if asked about your hometown. Mention any important caveats to your conclusions; for example, can we conclude that the variable you identified as a key predictor in the question above has a causal effect (i.e., changing it would change upward mobility) based on that analysis? Why or why not? Figure A.1: Household Income in Adulthood for Children Raised in Low-Income Households in Milwaukee, WI Notes: This figure shows household income at ages 31-37 for low income children who grew up in Census tracts near Milwaukee, WI. The image was saved from www.opportunity-atlas.org by first searching for Milwaukee, WI and then clicking on the download as image button. Figure A.2: Incarceration Rates for Black Men Raised in the Lowest-Income Households in Los Angeles, CA Notes: This figure is from the non-technical summary of the Opportunity Atlas and was discussed in Lecture 1. Data Description The data consist of n = 73,278 U.S. Census tracts. For more details on the construction of the variables included in this data set, please see Chetty, Raj, John Friedman, Nathaniel Hendren, Maggie R. Jones, and Sonya R. Porter. 2018. The Opportunity Atlas: Mapping the Childhood Roots of Social Mobility. NBER Working Paper No. 25147. Table 1 Definitions of Variables in atlas.rds Variable name Label Obs. (1) (2) (3) 1. Geographic identifiers tract Tract FIPS Code (6-digit) 2010 73,278 county County FIPS Code (3-digit) 73,278 state State FIPS Code (2-digit) 73,278 cz Commuting Zone Identifier (1990 Definition) 72,473 2. Characteristics of Census tracts hhinc_mean2000 Mean Household Income 2000 72,302 mean_commutetime2000 Average Commute Time of Working Adults in 2000 72,313 frac_coll_plus2010 Fraction of Residents with a College Degree or More in 2010 72,993 frac_coll_plus2000 Fraction of Residents with a College Degree or More in 2000 72,343 foreign_share2010 Share of Population Born Outside the U.S. 72,279 med_hhinc2016 Median Household Income in 2016 72,763 med_hhinc1990 Median Household Income in 1999 72,313 popdensity2000 Population Density (per square mile) in 2000 72,469 poor_share2010 Poverty Rate 2010 72,933 poor_share2000 Poverty Rate 2000 72,315 poor_share1990 Poverty Rate 1990 72,323 share_black2010 Share black 2010 73,111 share_hisp2010 Share Hispanic 2010 73,111 share_asian2010 Share Asian 2010 71,945 share_black2000 Share black 2000 72,368 share_white2000 Share white 2000 72,368 share_hisp2000 Share Hispanic 2000 72,368 share_asian2000 Share Asian 2000 71,050 gsmn_math_g3_2013 Average School District Level Standardized Test Scores in 3rd Grade in 2013 72,090 rent_twobed2015 Average Rent for Two-Bedroom Apartment in 2015 56,607 singleparent_share2010 Share of Single-Headed Households with Children 2010 72,564 singleparent_share1990 Share of Single-Headed Households with Children 1990 72,196 singleparent_share2000 Share of Single-Headed Households with Children 2000 72,285 traveltime15_2010 Share of Working Adults w/ Commute Time of 15 Minutes Or Less in 2010 72,939 emp2000 Employment Rate 2000 72,344 mail_return_rate2010 Census Form Rate Return Rate 2010 72,547 ln_wage_growth_hs_grad Log wage growth for HS Grad., 2005-2014 51,635 jobs_total_5mi_2015 Number of Primary Jobs within 5 Miles in 2015 72,311 jobs_highpay_5mi_2015 Number of High-Paying (&gt;USD40,000 annually) Jobs within 5 Miles in 2015 72,311 nonwhite_share2010 Share of People who are not white 2010 73,111 popdensity2010 Population Density (per square mile) in 2010 73,194 ann_avg_job_growth_2004_2013 Average Annual Job Growth Rate 2004-2013 70,664 job_density_2013 Job Density (in square miles) in 2013 72,463 3. Measures of Upward Mobility from the Opportunity Atlas kfr_pooled_p25 Household income ($) at age 31-37 for children with parents at the 25th percentile of the national income distribution 72,011 kfr_pooled_p75 Household income ($) at age 31-37 for children with parents at the 75th percentile of the national income distribution 72,012 kfr_pooled_p100 Household income ($) at age 31-37 for children with parents at the 100th percentile of the national income distribution 71,968 kfr_natam_p25 Household income ($) at age 31-37 for Native American children with parents at the 25th percentile of the national income distribution 1,733 kfr_natam_p75 Household income ($) at age 31-37 for Native American children with parents at the 75th percentile of the national income distribution 1,728 kfr_natam_p100 Household income ($) at age 31-37 for Native American children with parents at the 100th percentile of the national income distribution 1,594 kfr_asian_p25 Household income ($) at age 31-37 for Asian children with parents at the 25th percentile of the national income distribution 15,434 kfr_asian_p75 Household income ($) at age 31-37 for Asian children with parents at the 75th percentile of the national income distribution 15,360 kfr_asian_p100 Household income ($) at age 31-37 for Asian children with parents at the 100th percentile of the national income distribution 13,480 kfr_black_p25 Household income ($) at age 31-37 for Black children with parents at the 25th percentile of the national income distribution 34,086 kfr_black_p75 Household income ($) at age 31-37 for Black children with parents at the 75th percentile of the national income distribution 34,049 kfr_black_p100 Household income ($) at age 31-37 for Black children with parents at the 100th percentile of the national income distribution 32,536 kfr_hisp_p25 Household income ($) at age 31-37 for Hispanic children with parents at the 25th percentile of the national income distribution 37,611 kfr_hisp_p75 Household income ($) at age 31-37 for Hispanic children with parents at the 75th percentile of the national income distribution 37,579 kfr_hisp_p100 Household income ($) at age 31-37 for Hispanic children with parents at the 100th percentile of the national income distribution 35,987 kfr_white_p25 Household income ($) at age 31-37 for white children with parents at the 25th percentile of the national income distribution 67,978 kfr_white_p75 Household income ($) at age 31-37 for white children with parents at the 75th percentile of the national income distribution 67,968 kfr_white_p100 Household income ($) at age 31-37 for white children with parents at the 100th percentile of the national income distribution 67,627 3. Counts of number of children under 18 in 2000 (to calculate weighted summary statistics) count_pooled Count of all children 72,451 count_white Count of White children 72,451 count_black Count of Black children 72,451 count_asian Count of Asian children 72,451 count_hisp Count of Hispanic children 72,451 count_natam Count of Native American children 72,451 Cheatsheat commands R command Description Here I present a summary of the commands you could use to work on this project. There are two important issues you should keep in mind while reading this: Notice that whenever you see yvar this is not a real variable. It is only a place holder for the appropriate variable that you decide to analyze or use. For example, if you want to see the mean across neighborhoods of the average household income as measured in 2000, you would not do mean(atlas$yvar, na.rm=TRUE) but mean(atlas$hhinc_mean2000, na.rm=TRUE). Important! yvar is not a real variable. You should replace it for the appropriate variable in your code. The data atlas has missing information for some neighborhoods for some variables. These are called NA or missing. Most R functions do not like that you include missings in the function, because R does not know what to do with that. What is 5+NA ? NA !! So, for many of these functions, we will explicitely tell R to ignore NAs. That is what the option na.rm=TRUE does. It does not not exist for every function, but it does for most of the ones we will use here. Important! Careful with missing values (also called NA)! We will use na.rm=TRUE as an option for several functions to tell R to ignore the missings. Unweighted summary statistics summary(atlas$yvar) mean(atlas$yvar, na.rm=TRUE) sd(atlas$yvar, na.rm=TRUE) Load package If you wanted to install the package Hmisc (which you will need), run: library(Hmisc) Weighted summary statistics You can weight means or other statistics. In our case, we want to use the population weighted statistics in several cases. That is, we want to put more weight on the value of a tract in which more people live than in another with lower population. Recall that the population variable is count_pooled. Weighted mean: wtd.mean(atlas$yvar, atlas$count_pooled) Weighted standard deviation: sqrt(wtd.var(atlas$yvar, atlas$count_pooled)) Subset observations State level: If you want to select a subset of observations, you can add the rule for selecting those observations, and the filter function. Here we subset the observations for the State of Wisconsin, and called the resulting dataset atlas_wisconsin. atlas_wisconsin &lt;- atlas %&gt;% filter(state == 55) County level: We can do the same but now for a specific county, adding an extra rule after a comma , or &amp;. Here we subset the observations for Milwaukee County in Wisconsin: atlas_milwaukee &lt;- atlas %&gt;% filter(state == 55 &amp; county == 079) Standardize variables You can standardize variables by substracting the mean and dividing by the standard deviation. Let us say that you want to standardize only considering the variables in Milwaukee, then you can do this: atlas_milwaukee&lt;- atlas_milwaukee %&gt;% mutate(x_std=(xvar - mean(atlas_milwaukee$xvar, na.rm=TRUE))/sd(atlas_milwaukee$xvar, na.rm=TRUE)) As an example, lets say you want to standardize both the measure of mobility and the annual job growth for the data in Wisconsin: atlas_wisconsin&lt;- atlas_wisconsin %&gt;% mutate(kfr_pooled_p25_std=(kfr_pooled_p25 - mean(atlas_wisconsin$kfr_pooled_p25, na.rm=TRUE))/sd(atlas_wisconsin$kfr_pooled_p25, na.rm=TRUE), ann_avg_job_growth_2004_2013_std=(ann_avg_job_growth_2004_2013 - mean(atlas_wisconsin$ann_avg_job_growth_2004_2013, na.rm=TRUE))/sd(atlas_wisconsin$ann_avg_job_growth_2004_2013, na.rm=TRUE)) Run regression Simple linear regression Lets say you want to run a simple regression of variable yvar on variable xvar1 for the county of Milwaukee. We will save the results of that regression in an object call mod1 (we could give it any name). Then you would do this: mod1 &lt;- lm(yvar~xvar1, data = atlas_milwaukee) To see what the outcome of the regression is, you would use the function summary and apply it to our new object mod1, like this: summary(mod1) As an example, we could regress the mobility of children from parents in the 25th percentile (kfr_pooled_p25) on the average annual job growth rate between 2004 and 2013 (ann_avg_job_growth_2004_2013). To do that, we would run: mod1 &lt;- lm(kfr_pooled_p25~ann_avg_job_growth_2004_2013, data = atlas_milwaukee) Multivariate linear regression You might want to understand the relationship between yvar and variable xvar1 while holding fixed another variable xvar2 for neighborhoods only in Milwaukee. You can do this: mod2 &lt;- lm(yvar~xvar1+xvar2 + xvar3, data = atlas_milwaukee) How to read the regression output? To simplify the interpretation, lets run a regression where you use the standardize both the measure of mobility and the annual job growth for the data in Wisconsin: mod3 &lt;- lm(kfr_pooled_p25_std ~ ann_avg_job_growth_2004_2013_std, data = atlas_wisconsin) summary(mod3) ## ## Call: ## lm(formula = kfr_pooled_p25_std ~ ann_avg_job_growth_2004_2013_std, ## data = atlas_wisconsin) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.829 -0.569 0.035 0.684 5.128 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.00000978 0.02681564 0.00 1.000 ## ann_avg_job_growth_2004_2013_std -0.05131843 0.02679889 -1.91 0.056 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.999 on 1386 degrees of freedom ## (6 observations deleted due to missingness) ## Multiple R-squared: 0.00264, Adjusted R-squared: 0.00192 ## F-statistic: 3.67 on 1 and 1386 DF, p-value: 0.0557 You should focus here on interpreting the coefficient for ann_avg_job_growth_2004_2013_std. You should pay attention to both the magnitude of the number, and the sign. In this case, you would read it like this: In Wisconsin, increasing one standard deviation the average annual job growth rate is correlated with an reduction of 0.05 standard deviations in the economic mobility for children with parents in the 25th percertile. Plotting the linear relationship You need to load the ggplot2 package (which should be installed already). library(ggplot2) Suppose you want to visually see the linear relationship between two variables in the atlas_milwaukee dataset that we filtered above. Then, you can do this: ggplot(data = atlas_milwaukee) + geom_point(aes(x = xvar1, y = yvar)) + geom_smooth(aes(x = xvar1, y = yvar), method = &quot;lm&quot;, se = F) The function geom_smooth() adds the line that you calculated above for mod1, where you ran mod1 &lt;- lm(yvar~xvar1, data = atlas_milwaukee) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
