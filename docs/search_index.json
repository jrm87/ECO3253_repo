[["index.html", "Economics of Public and Social Issues ECO 3253, Fall 2022 About About me Organization of this site Data-driven course Topics to be covered Statistical tools Economic Concepts Projects FAQs", " Economics of Public and Social Issues ECO 3253, Fall 2022 Jonathan Moreno-Medina 2022-08-25 About Hey! Welcome to ECO 3253! In this course we will see how we can use data to understand and solve current and important economic problems! You will get a sense of what is the research frontier in applied economics and social science. These include topics like equality of opportunity and mobility, education, innovation and entrepreneurship, health care, climate change, and crime. How will we get there? By doing 3 things: covering the topics with a focus towards using data that can help answer these questions understanding the intuition for how to use data to answer these questions (basics of statistical analysis) using computational tools to help us with the statistical analysis (basics of R) About me Let me briefly tell you about me: I am Colombian. Did my PhD in Economics at Duke University, a Masters in Economics at Université Catholique de Louvain in Belgium, and an undergraduate in (you guessed it!) Economics at Universidad Nacional de Colombia. I am an applied (micro)economist. What is applied microeconomics? Well, first what I do not work on: macroeconomics (inflation, general unemployment, GDP growth, and so on). The micro part just means I tend to focus on specific markets (housing and media, being the main ones), and the applied just means I use data all the time in my research. Which leads me to my next point. Organization of this site In this site I will put the materials we cover in the lectures so you can refer to it later on your own. I will divide this book into two main parts: economic content, and the tools. The tools are both statistical (correlations, means, distributions, etc) and computational (R). For the first week of class, you shall see two headers on the table of content on the left: one for each. These will get filled up as the semester continues. There is also an Appendix, which you will be able to see on the bottom of the navigation panel, where I will post complementary material where you could brush up several statistical concepts, for example. Lastly, I will also leave a link to the projects you will work on this semester at the end of the list on the left. Data-driven course We will work with data throughout the semester! This will not just be a theoretical course. We will work with real world data, and we will try to make sense of it. We will need theoretical to make sense of the world for sure (either coming from economics or statistics), but we are looking at those tools always eyeing real world instances. The course here presented is partly based on the course by Prof. Raj Chetty at Harvard. As we will see later, I also rely on some material on the book ModernDive for teaching the basics of our statistical software R. Just to give you a sense of how much economics has became a much more empirical discipline in later years, below is the number of articles in leading journals that are data-drive in a way. (#fig:fig_econJournals)Source: Mamermesh (JEL 2013) Fortunately, the same types of skills that are used to solve private market issues using data work to tackle challenges like growing inequality and climate change. In order to achieve that goal, the idea of this class is to introduce a broad range of topics, methods, and real-world applications of these sorts of ideas. Fundamentally, we want to start from the questions that motivate the methods we teach in economics and social science, rather than the traditional approach, which is to do the reverse. Topics to be covered The plan for what we will covered in the semester includes: Geography of Upward Mobility in America Causal Effects of Neighborhoods and Characteristics of High-Mobility Areas Historical and International Evidence on the Drivers of Inequality and Mobility Upward Mobility, Innovation, and Growth Higher Education and Upward Mobility Primary education Teachers and Charter Schools Racial Disparities in Economic Opportunity Improving judicial decisions Immigration Political Economy Income taxation Savings and wealth Housing markets and COVID Intro to air and water pollution, and externalities Discount rates, external validity You can check the Schedule here, or for more details, please check the syllabus on Blackboard. Statistical tools Although we are going to take a topic-oriented focus in this class, we will cover the basics of several methods that will help us make sense of the data. These methods include: Descriptive Data Analysis: correlation, regression Experiments: randomization, non-compliance Quasi-Experiments: regression discontinuity, difference-in-differences Machine Learning: prediction, overfitting, cross-validation R as our statistical software Economic Concepts We will cover and make use if several economic concepts you probably learned in your intro classes, but we will see several instances of how to use it practice. These include: Effects of price incentives Supply and demand Competitive equilibrium Adverse selection Behavioral economics vs. rational models Projects A big part of the course will be the projects you will do during the semester. You will work on 4 projects through the semester. These are more involved than most homeworks you have probably worked up to now. The good news is that they involve doing economics! You will get hands-on experience working with real data on real problems. The main recommendation is to start working on this projects early! I cannot emphasize this enough. There are several moving parts to these projects, and you need to plan in advance your work so you can try, fail, come back to it, and so on. If you try to work on these projects just the night before the deadline, that will not leave much room for experimenting, and trying. Given than several of these tools might be new, you need to give yourself time to try more than once. Important! Do not work on these projects just the day before! They are relatively involved, so give yourself enough time. For more details, please check the syllabus on Blackboard. FAQs Do I need to know how to program? No! I will give you the basic tools to understand and do basic analysis in R even if you have no background in this sort of things. Do I need to have taken econometrics? No! You will have a bit of a head start if you have, but you do not have to have taken the econometrics course to be successful in this class. I will give you some of the basic conceptual frameworks for how we think about statistical analysis and causal inference. Do I need to know statistics? Basic statistics is definitely recommended. We will have plenty of opportunity to brush up some of those concepts throughout the course, though. You can find a refresher in the Appendix Where can I find more details about this class? You can read more in the syllabus uploaded to Blackboard. "],["schedule.html", "Updated Schedule", " Updated Schedule I will keep this page with the updated schedule for the semester. You can find these readings on Blackboard. Week Session Topic Required reading Method Deliverable 1: 08/22 &amp; 08/24 1 Introduction to the course 2 Geography of Upward Mobility in America Chetty, Friedman, Hendren, Jones and Porter (2018)- Non-technical summary Correlation, regression 2: 08/29 &amp; 08/31 3 Intro to R and data 4 Intro to visualization and RMarkdown (homeworks) First short report (In class) 3: 09/05 &amp; 09/07 - Labor Day  No class 5 Intro to Causal Effects of Neighborhoods Bergman, Chetty, DeLuca, Hendren, Katz and Palmer (2019) [Non-technical summary] Experiments (RCTs) Project 1  Part 1 4: 09/12 &amp; 09/14 6 Characteristics of High-Mobility Areas Bergman, Chetty, DeLuca, Hendren, Katz and Palmer (2019) [Introduction] Quasi-experiments 7 Historical and International Evidence onthe Drivers of Inequality and Mobility Cost-benefit analysis Project 1  Part 2 5: 09/19 &amp; 09/21 8 Upward Mobility, Innovation, and Growth Bian, Leslie and Cimpian (2017) Propensity score reweighting 9 Higher Education and Upward Mobility Dynarski, Libassi, Michelmore and Owen (2018) Regression discontinuity 6: 09/26 &amp; 09/28 10 Higher Education and Upward Mobility II 11 Review for midterm Project 2 7: 10/03 &amp; 10/05 12 Midterm 13 Solution review 8: 10/10 &amp; 10/12 14 Primary education Chetty, Friedman and Rockoff (2011) [Non-technical summary] Experiments 15 Teachers and Charter Schools Event study designs, competitive equilibrium 9: 10/17 &amp; 10/19 16 Racial Disparities in Economic Opportunity Bertrand, and Mullainathan (2004) Dynamic models and steady states 17 Improving judicial decisions Kleinberg, Lakkaraju, Leskovec, Ludwig and Mullainathan (2017) Machine learning, implicit bias Project 3 10: 10/24 &amp; 10/26 18 Implementing a simple machine learning model in R 19 Immigration Clemens (2011) Welfare analysis 11: 10/31 &amp; 11/02 20 Political Economy 21 Income taxation Diamond and Saez (2011) Supply and demand; synthetic control 12: 11/07&amp; 11/09 22 Savings and wealth Behavioral economics 23 Regression Project 4  Part 1 13: 11/14 &amp; 11/16 24 Housing markets and COVID Glaeser, Edward, Kominers, Luca and Naik (2018) Machine learning 25 Intro to air and water pollution, and externalities Moore, Obradovich, Lehner and Baylis (2019) Difference in difference Project 4  Part 2 14: 11/21 &amp; 11/28 26 Discount rates, external validity Regression discontinuity 27 15: 11/28 &amp; 11/30 28 29 Review session 16: 12/05 -12-09 - Final Exam Week - "],["upward-mobility-in-the-us.html", "Chapter 1 Upward Mobility in the US 1.1 Parental and children income rank 1.2 Interpreting the regression line 1.3 Geographic Variation in Upward Mobility by Commuting Zone 1.4 Local Area Variation in Upward Mobility: Los Angeles, CA", " Chapter 1 Upward Mobility in the US We will first dive into a relatively recent (from 2020) called The Opportunity Atlas: Mapping the Childhood Roots of Social Mobility., by Chetty, Friedman, Hendren, Jones and Porter. The previous link should take you to Blackboard where you will be able to download it. The main question in the paper is the following: how do childrens chances of moving up vary across areas in America? To answer it, the authors need to measure upward mobility across the country. How do they do that? They will construct a database called the Opportunity Atlas. How do they measure upward mobility separately by geographic area in the United States? They take data from the 2000 and 2010 Censuses, and link that to information from federal income tax returns. They also use tax return data from 1989 to 2015. Linking those datasets yields information on essentially every American between 1989 and 2015, including how much they are earning, where they live, the dependents they have, and other information, year by year. In that dataset, they want to study economic opportunity across generations. But that requires knowing in the data who is the children of whom. In order to link parents to their children, they use information from dependency claims on tax returns. (In order to receive a tax deduction, parents must enter their childs Social Security Number on their tax returns.) Theyre able to use this information to link 99% of kids in America back to their parents, thereby generating an intergenerational sample where you can study income inequality and mobility across generations. There is a lot you can learn about the US and economic mobility with that sort of data! At the end, they end up with an 8-billion row dataset which covers 20.5 million children born between 1978 and 1983, representing 96% of our target population. They analyze children born during those particular years because we need the children to be old enough that we can measure their earnings reliably. Theyre interested in people who were born in the U.S. or are authorized immigrants who came to the U.S. in childhood. They are focusing on authorized immigrants because these datasets dont go a great job of covering undocumented immigrants. Note here that this is a limitation for the study only in the sense that it does not necessarily paint a picture of how economic mobility looks for undocumented immigrants. Additionally, the number is not 100% because there are some kids who you cant link to their parents and people you cant link the census form to the tax form. How do they measure parents and childrens incomes in tax data? They do so by measuring incomes using information from the anonymized tax return data. For parents, they use average income between 1994 and 2000 reported on Form 1040, the main tax return in the U.S. Similarly, for kids, we measure average income in 2014 and 2015, the last two years of the data theyworked with. That is when the children are in their mid-30s. Using this information, theyre going to focus on percentile ranks in the national distribution. What that means, concretely, is that they rank kids relative to all the other kids born in the same year, and parents relative to all other parents. They are comparing kids to other kids of the same age. Then likewise, they compare parents to other parents. The reason is that because they want to adjust for the fact that as people grow older, their incomes tend to rise. 1.1 Parental and children income rank The chart below was constructed using data for kids who were raised in the Chicago metro area, which consists of Chicago and the surrounding suburbs. Figure 1.1: Source: Chetty, Hendren, Kline, Saez (2014) Lets interpret the figure. On the x-axis, it shows the parent rank in the national income distribution. There are a hundred dots here, one corresponding to each percentile of the distribution (see here for a refresher on what the percentiles are). Then in each of those hundred bins, were plotting the average ranking of the child in the national income distribution. Now as you go to the right, youre looking at kids from richer and richer families, and you see that theres a very strong upward-sloping pattern. This reflects the simple fact that if you were born to a richer family in America, you yourself tend to be richer in adulthood. Now lets find the line that fits that data most accurately using a method called regression. Then Im going to focus on the value of this line, called the predicted value, at the 25th percentile of the parent income distribution. There is a lot of information contained in each dot in the graph, but by focusing on the value of this line we can construct a digestible single statistic (i.e., a number) summarizing what upward mobility looks like in each place. 1.2 Interpreting the regression line In Chicago, on average, kids who start out in families at the 25th percentile end up at the 40th percentile. Kids growing up in low-income families in Chicago, roughly speaking, earn about $30,000, on average, when theyre adults. We cant directly use the value of the dot on the above chart at the 25th percentile. Instead we use a regression line. This is because there is noise and random variation in the data, specifically with smaller samples of people. When working with small samples, it starts to become very important to fit that regression line. That is, we need to use the discipline of a statistical model. Thats the core idea of statistical models, to take the underlying data and represent it in a way that is more stable. 1.2.1 Percentiles The conversion to percentiles is very important here. If we did this analysis in dollars, that relationship is very far from linear. It is very curved, which makes it harder to fit systematically with a statistical model. To construct the Opportunity Atlas, we fit line like this to the kids who grew up in every different census tract in America. 1.2.2 What is a tract? A Census tract is a small definition of a neighborhood that the Census Bureau has created. There are 70,000 Census tracts in America, each of which has about 4,200 people. In order to handle children who might have moved while they were kids, we weigh children by the fraction of their childhood that they spent in each area. 1.3 Geographic Variation in Upward Mobility by Commuting Zone The map below plots average household earnings of children who grew up in low-income families. The map presents this statistic separately for each of the 741 commuting zones (CZs) in the United States. CZs are aggregations of counties based on commuting patterns that are similar to metro areas but cover the entire United States. Figure 1.2: Source: Chetty, Friedman, Hendren, Jones and Porter (2018) Note that the map shows household income in dollars, but the underlying statistic is based on the predicted percentile rank defined earlier. The ranks have been converted to dollars because its more intuitive and concrete. In the map, blue colors depict areas with high levels of upward mobility and red colors depict areas with low levels of upward mobility. The map shows broad geographic variation. One of the most interesting features of this map is that the highest upward mobility areas in America are the Great Plains, the rural parts of the country in the center of the country. Charlotte is one of the cities in America with the highest rates of job growth in the United States. Yet, somehow remarkably, for low-income kids who grow up in Charlotte, they do not have very good chances of moving up. The map shows that in the current generation, there are some parts of America where kids chances of moving up still look fantasticactually better than any other country in the world. Then theres some places, like in much of the industrial Midwest, where your odds of climbing up look worse than any country for which we currently have data. America is a land of tremendous variability in opportunity. 1.3.1 Adjustments for cost of living This map shows nominal incomes, meaning it does not adjust for differences in cost of living. You can redraw this map, adjusting for differences in cost of living. When you do that, you get a map that looks almost identical to the one that Im showing you here. To put it more precisely, the correlation between that data and these data is .9, meaning that it looks essentially the same. Were focusing specifically here on kids growing up in low-income families. If you look at kids growing up in middle-class families, its broadly similar. If you look at kids growing up in high-income families, you see that theres significantly less variation across areas for kids growing up in very-high-income families. 1.4 Local Area Variation in Upward Mobility: Los Angeles, CA This geographic variation in upward mobility is not just about broad regional variation, but its actually about extremely local variation. We can use the Opportunity Atlas to visualize the data. The Opportunity Atlas starts out with the national map of the same statistics by commuting zone that we were looking at before. However, it allows us to zoom in to areas of specific interest. Let us focus on one particular example: Nickerson Gardens in Los Angeles, CA, which is a public housing project in Watts. Lets look at black men growing up in the lowest-income families in the bottom 1% of the income distribution, which is actually representative of the incomes of the families living in this public housing project. The average household income of black men who grew up in the poorest families in Watts is just $3,300 a year. It has to be the case that lots of people are basically not working at all. You can see that in a very direct way in these data because were able to look not just at income, but a variety of other outcomes, including incarceration. Focusing on incarceration rates, you will see a really shocking and disturbing statistic about the United States, and this area in particular, which is that 44% of the black men who grew up in these lowest-income families are incarcerated on a single day, the date of the 2010 census. If you go down to Compton, you see incarceration rates of 6.2%, which is a factor of 10 smaller than the 44% that we were seeing in Watts for black men growing up in low-income families. Compton is a different neighborhood than Watts, its not exactly the same, but I dont think anybody from L.A. would have predicted that Compton would have drastically different outcomes like this from Watts. That shows you that you can go two miles away and just have a dramatically different picture in terms of what kids life trajectories look like. We see that in the stark example here within Los Angeles, but we see that sort of thing more broadly across the United States. "],["getting-started.html", "Chapter 2 Very Brief Intro to Data in R 2.1 Why R again? 2.2 Key concepts before we start: 2.3 Lets open R! 2.4 How do I code in R? 2.5 In Class Exercise 2.6 What are R packages? 2.7 Hands-on exercise! 2.8 Conclusion", " Chapter 2 Very Brief Intro to Data in R 2.1 Why R again? 2.1.1 Why are we learning R? I wanted to learn about economics of public and social issues This class is about social issues in economics. But what are those social issues? Economic mobility and inequality Effects of education on wages and inequality Criminal justice system outcomes Pollution and climate change and so on How can we know if going to school increases wages? Or if economic mobility is low or high? We need to analyze data! We can do that analysis by hand but that would be very time consuming. Or we can use a super calculator with amazing capabilities to explore data, maps, etc: enter R and R Studio. This course is not about teaching you all about R! We will only cover the very basics so you can jump into doing some empirical analysis by yourself. You will be able to expand much more on the tools briefly described here in other, more advanced, courses in the Economics sequence. Important! For the vast majority of exercises in our course, I will give you all the code you will have to run. So, its not like you need to write anything from scratch! I do want you to get a basic understanding of what we will be doing when we run those lines. 2.1.2 Ok, but why R? R is free and open source! R has a vibrant online community! R is very flexible and powerful  adaptable to nearly any task ( e.g., correlations, econometrics, spatial data analysis, machine learning, web scraping, data cleaning, website building, teaching.) Employers like R over alternatives 2.1.3 Added benefits of learning R Employers hire people that knows R. Again, we will only cover the essentials, but maybe you want to keep this in mind as you go along with your studies. 2.2 Key concepts before we start: Before we can start exploring data in R, there are some key concepts to understand first: What are R and RStudio? How do I code in R? What are R packages? Well introduce these concepts in upcoming Sections 2.2.1-2.6. Then well introduce our first data set: data on the economic mobility for all neighborhoods across the US in the atlas dataset. 2.2.1 What are R and RStudio? For much of this book, we will assume that you are using R via RStudio. First time users often confuse the two. At its simplest: R is like a cars engine. RStudio is like a cars dashboard. R: Engine RStudio: Dashboard More precisely, R is a programming language that runs computations while RStudio is an integrated development environment (IDE) that provides an interface by adding many convenient features and tools. So just as having access to a speedometer, rearview mirrors, and a navigation system makes driving much easier, using RStudios interface makes using R much easier as well. 2.2.2 R and RStudio: In your computer or in the cloud? To use R and RStudio, you can: install it in your computer (see the book) or  run it on someone elses computer (the cloud!). We will do that, so you dont have to worry about installations. In this course I wont require you to install R and RStudio in your own computer. Instead, we will use the cloud! The Department of Economics is graciously paying for your accounts this semester. If you would still like to install R and RStudio in your own computer, please follow the instructions in Section 1.1.1 in this link. Of course, you are encouraged to experiment in your own machine as well. Notice, however, that you should carry out the assignments and projects in RStudio Cloud. Important! Homework and projects should be carried in RStudio Cloud (not your personal computer). 2.3 Lets open R! 2.3.1 RStudio in the cloud Lets jump in! You should have received a link for you to access RStudio in the cloud. The Department of Economics is paying for us to use this service for this class. Remember that all the code is running on someones computer. In this case, it is running on a computer owned by the folks at RStudio. Receive link with invitation You should have received an email with a link inviting you to join RStudio Cloud. Once you click on the link, you should land in a page like this: Then you should fill out the information for your account. Please use your utsa email account (the account you should have received the invitation to). Once you click on Sign Up, it will show this: Then you need to go back to your email and verify it. Then log back in again! Inside RStudio Cloud select ECO3253 Once you have logged back in, you should land in the main page, which should look like this: Click on the left where it says ECO3253. That is where you will see all the material and projects for this class. Select the appropriate project you want to work on Once you are in the ECO3253 tab, you should see a list of individual projects we will work on throughout the semester. It should look something like this: For our first class on R you will select the link that says introRClass. That is it! Now we are ready to get to work! 2.3.2 Using R via RStudio Recall our car analogy from above. Much as we dont drive a car by interacting directly with the engine but rather by interacting with elements on the cars dashboard, we wont be using R directly but rather we will use RStudios interface. After you open RStudio Cloud and follow the previous instructions, you should see a panel like the following: Note the three panes, which are three panels dividing the screen: The Console pane, the Files pane, and the Environment pane. Over the course of this chapter, youll come to learn what purpose each of these panes serve. 2.4 How do I code in R? Now that youre set up with RStudio Cloud, you are probably asking yourself OK. Now how do I use R? The first thing to note as that unlike other statistical software programs like Excel, STATA, or SAS that provide point and click interfaces, R is an interpreted language, meaning you have to enter in R commands written in R code. In other words, you have to code/program in R. Note that well use the terms coding and programming interchangeably in this book. While it is not required to be a seasoned coder/computer programmer to use R, there is still a set of basic programming concepts that R users need to understand. Consequently, while this course is not a book on programming, you will still learn just enough of these basic programming concepts needed to explore and analyze data effectively. 2.4.1 Basic programming concepts and terminology We now introduce some basic programming concepts and terminology. Instead of asking you to learn all these concepts and terminology right now, well guide you so that youll learn by doing. Note that in this book we will always use a different font to distinguish regular text from computer_code. The best way to master these topics is, in our opinions, learning by doing and lots of repetition. Basics: Console: Where you enter in commands. Running code: The act of telling R to perform an action by giving it commands in the console. Objects: Where values are saved in R. In order to do useful and interesting things in R, we will want to assign a name to an object. For example we could do the following assignments: x &lt;- 44 - 20 and three &lt;- 3. This would allow us to run x + three which would return 27. Data types: Integers, doubles/numerics, logicals, and characters. Vectors: A series of values. These are created using the c() function, where c() stands for combine or concatenate. For example: c(6, 11, 13, 31, 90, 92). Factors: Categorical data are represented in R as factors. Data frames: Data frames are like rectangular spreadsheets: they are representations of datasets in R where the rows correspond to observations and the columns correspond to variables that describe the observations. Well cover data frames later in Section 2.7.1. Conditionals: Testing for equality in R using == (and not = which is typically used for assignment). Ex: 2 + 1 == 3 compares 2 + 1 to 3 and is correct R code, while 2 + 1 = 3 will return an error. Boolean algebra: TRUE/FALSE statements and mathematical operators such as &lt; (less than), &lt;= (less than or equal), and != (not equal to). Logical operators: &amp; representing and as well as | representing or. Ex: (2 + 1 == 3) &amp; (2 + 1 == 4) returns FALSE since both clauses are not TRUE (only the first clause is TRUE). On the other hand, (2 + 1 == 3) | (2 + 1 == 4) returns TRUE since at least one of the two clauses is TRUE. Functions, also called commands: Functions perform tasks in R. They take in inputs called arguments and return outputs. You can either manually specify a functions arguments or use the functions default values. This list is by no means an exhaustive list of all the programming concepts and terminology needed to become a savvy R user; such a list would be so large it wouldnt be very useful, especially for novices. Rather, we feel this is a minimally viable list of programming concepts and terminology you need to know before getting started. Im confident you can learn the rest as you go. Remember that your mastery of all of these concepts and terminology will build as you practice more and more.1 2.5 In Class Exercise Try running the following commands in the console. What do you see for each one? 2*3 2*pi log(10) exp(2) sqrt(25) 3==3 3==4 3&lt;=4 3!=4 x &lt;- c(1,3,2,5)# this is called a &#39;vector&#39; x #what do you see? x &lt;- c(1,6,2) x #now what do you see? y &lt;- c(1,4,3) # USE ARROW! length(x) length(y) x+y #write this write this again 2.5.1 Errors, warnings, and messages Noticed the last thing that appeared in the console when you wrote write this again? It had scary red letters. It is an example of something that intimidates new R and RStudio users: how it reports errors, warnings, and messages. R reports errors, warnings, and messages in a glaring red font, which makes it seem like it is scolding you. However, seeing red text in the console is not always bad. R will show red text in the console pane in three different situations: Errors: When the red text is a legitimate error, it will be prefaced with Error in and try to explain what went wrong. Generally when theres an error, the code will not run. For example, well see in Subsection 2.6.3 if you see Error in ggplot(...) : could not find function \"ggplot\", it means that the ggplot() function is not accessible because the package that contains the function (ggplot2) was not loaded with library(ggplot2). Thus you cannot use the ggplot() function without the ggplot2 package being loaded first. Warnings: When the red text is a warning, it will be prefaced with Warning: and R will try to explain why theres a warning. Generally your code will still work, but with some caveats. For example, you will see in Chapter ?? if you create a scatterplot based on a dataset where one of the values is missing, you will see this warning: Warning: Removed 1 rows containing missing values (geom_point). R will still produce the scatterplot with all the remaining values, but it is warning you that one of the points isnt there. Messages: When the red text doesnt start with either Error or Warning, its just a friendly message. Youll see these messages when you load R packages in the upcoming Subsection 2.6.2 or when you read data saved in spreadsheet files with the read_csv() function as youll see in Chapter ??. These are helpful diagnostic messages and they dont stop your code from working. Additionally, youll see these messages when you install packages too using install.packages(). Important! When you see red text in the console, dont panic! Just check out what it could be. Remember, when you see red text in the console, dont panic. It doesnt necessarily mean anything is wrong. Rather: If the text starts with Error, figure out whats causing it. Think of errors as a red traffic light: something is wrong! If the text starts with Warning, figure out if its something to worry about. For instance, if you get a warning about missing values in a scatterplot and you know there are missing values, youre fine. If thats surprising, look at your data and see whats missing. Think of warnings as a yellow traffic light: everything is working fine, but watch out/pay attention. Otherwise the text is just a message. Read it, wave back at R, and thank it for talking to you. Think of messages as a green traffic light: everything is working fine. 2.5.2 Tips on learning to code Learning to code/program is very much like learning a foreign language, it can be very daunting and frustrating at first. Such frustrations are very common and it is very normal to feel discouraged as you learn. However just as with learning a foreign language, if you put in the effort and are not afraid to make mistakes, anybody can learn. Here are a few useful tips to keep in mind as you learn to program: Remember that computers are not actually that smart: You may think your computer or smartphone are smart, but really people spent a lot of time and energy designing them to appear smart. Rather you have to tell a computer everything it needs to do. Furthermore the instructions you give your computer cant have any mistakes in them, nor can they be ambiguous in any way. Take the copy, paste, and tweak approach: Especially when learning your first programming language, it is often much easier to taking existing code that you know works and modify it to suit your ends, rather than trying to write new code from scratch. We call this the copy, paste, and tweak approach. So early on, we suggest not trying to write code from memory, but rather take existing examples we have provided you, then copy, paste, and tweak them to suit your goals. Dont be afraid to play around! The best way to learn to code is by doing: Rather than learning to code for its own sake, we feel that learning to code goes much smoother when you have a goal in mind or when you are working on a particular project, like analyzing data that you are interested in. Practice is key: Just as the only method to improving your foreign language skills is through practice, practice, and practice; so also the only method to improving your coding is through practice, practice, and practice. Dont worry however; well give you plenty of opportunities to do so! 2.6 What are R packages? Another point of confusion with many new R users is the idea of an R package. R packages extend the functionality of R by providing additional functions, data, and documentation. They are written by a world-wide community of R users and can be downloaded for free from the internet. For example, among the many packages we will use in this book are the ggplot2 package for data visualization which we will cover later (or you can check here) or the dplyr package for data wrangling (again, we will cover later, but check this if you want to know more). A good analogy for R packages is they are like apps you can download onto a mobile phone: R: A new phone R Packages: Apps you can download So R is like a new mobile phone: while it has a certain amount of features when you use it for the first time, it doesnt have everything. R packages are like the apps you can download onto your phone from Apples App Store or Androids Google Play. Lets continue this analogy by considering the Instagram app for editing and sharing pictures. Say you have purchased a new phone and you would like to share a recent photo you have taken on Instagram. You need to: Install the app: Since your phone is new and does not include the Instagram app, you need to download the app from either the App Store or Google Play. You do this once and youre set. You might do this again in the future any time there is an update to the app. Open the app: After youve installed Instagram, you need to open the app. Once Instagram is open on your phone, you can then proceed to share your photo with your friends and family. The process is very similar for using an R package. You need to: Install the package: This is like installing an app on your phone. Most packages are not installed by default when you install R and RStudio. Thus if you want to use a package for the first time, you need to install it first. Once youve installed a package, you likely wont install it again unless you want to update it to a newer version. Load the package: Loading a package is like opening an app on your phone. Packages are not loaded by default when you start RStudio on your computer; you need to load each package you want to use every time you start RStudio. Lets now show you how to perform these two steps for the ggplot2 package for data visualization. 2.6.1 Package installation For the most part, in RStudio Cloud, I will pre-install the packages you are going to need to use. But just in case you also want to work on your own machine, or install your own packages, here I explain that a bit more. There are two ways to install an R package. For example, to install the ggplot2 package: Easy way: In the Files pane of RStudio: Click on the Packages tab Click on Install Type the name of the package under Packages (separate multiple with space or comma): In this case, type ggplot2 Click Install Slightly harder way: An alternative but slightly less convenient way to install a package is by typing install.packages(\"ggplot2\") in the Console pane of RStudio and hitting enter. Note you must include the quotation marks. Much like an app on your phone, you only have to install a package once. However, if you want to update an already installed package to a newer verions, you need to re-install it by repeating the above steps. Learning check (LC2.1) Repeat the above installing steps, but for the dplyr, and knitr packages. This will install the earlier mentioned dplyr package, and the knitr package for writing reports in R. 2.6.2 Package loading Recall that after youve installed a package, you need to load it, in other words open it. We do this by using the library() command. For example, to load the ggplot2 package, run the following code in the Console pane. What do we mean by run the following code? Either type or copy &amp; paste the following code into the Console pane and then hit the enter key. library(ggplot2) If after running the above code, a blinking cursor returns next to the &gt; prompt sign, it means you were successful and the ggplot2 package is now loaded and ready to use. If however, you get a red error message that reads Error in library(ggplot2) : there is no package called ggplot2  it means that you didnt successfully install it. In that case, go back to the previous subsection Package installation and install it. Learning check (LC2.2) Load the dplyr, and knitr packages as well by repeating the above steps. 2.6.3 Package use One extremely common mistake new R users make when wanting to use particular packages is that they forget to load them first by using the library() command we just saw. Remember: you have to load each package you want to use every time you start RStudio. If you dont first load a package, but attempt to use one of its features, youll see an error message similar to: Error: could not find function R is telling you that you are trying to use a function in a package that has not yet been loaded. Almost all new users forget do this when starting out, and it is a little annoying to get used to. However, youll remember with pratice. 2.7 Hands-on exercise! 2.7.1 Explore your first dataset: economic mobility in the US Lets put everything weve learned so far into practice and start exploring some real data! These spreadsheet-type datasets are called data frames in R; we will focus on working with data saved as data frames throughout this course. Step 1: Load all the packages needed for this exercise (assuming youve already installed them). library(dplyr) library(tibble) atlas &lt;- readRDS(gzcon(url(&quot;https://raw.githubusercontent.com/jrm87/ECO3253_repo/master/data/atlas.rds&quot;))) atlas&lt;-tibble(atlas) 2.7.2 Economic mobility data The Opportunity Atlas is a freely available interactive mapping tool that traces the roots of outcomes such as poverty and incarceration back to the neighborhoods in which children grew up. The atlas dataset we loaded has the underlying data to describe equality of opportunity across the 73,278 different neighborhoods in the United States. Lets unpack these data a bit more! 2.7.3 atlas data frame We will begin by exploring the atlas data frame we just loaded to get an idea of its structure. Run the following code in your console (either by typing it or cutting &amp; pasting it): it loads the atlas dataset into your Console. Note depending on the size of your monitor, the output may vary slightly. atlas ## # A tibble: 73,278 x 62 ## tract county state cz czname hhinc_mean2000 mean_commutetime2000 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 20100 1 1 11101 Montgomery 68639. 26.2 ## 2 20200 1 1 11101 Montgomery 57243. 24.8 ## 3 20300 1 1 11101 Montgomery 75648. 25.3 ## 4 20400 1 1 11101 Montgomery 74852. 23.0 ## 5 20500 1 1 11101 Montgomery 96175. 26.2 ## 6 20600 1 1 11101 Montgomery 68096. 21.6 ## 7 20700 1 1 11101 Montgomery 65182. 23.2 ## 8 20801 1 1 11101 Montgomery 76874. 30.3 ## 9 20802 1 1 11101 Montgomery 77310. 30.7 ## 10 20900 1 1 11101 Montgomery 66234. 36.4 ## # ... with 73,268 more rows, and 55 more variables: frac_coll_plus2010 &lt;dbl&gt;, ## # frac_coll_plus2000 &lt;dbl&gt;, foreign_share2010 &lt;dbl&gt;, med_hhinc2016 &lt;dbl&gt;, ## # med_hhinc1990 &lt;dbl&gt;, popdensity2000 &lt;dbl&gt;, poor_share2010 &lt;dbl&gt;, ## # poor_share2000 &lt;dbl&gt;, poor_share1990 &lt;dbl&gt;, share_black2010 &lt;dbl&gt;, ## # share_hisp2010 &lt;dbl&gt;, share_asian2010 &lt;dbl&gt;, share_black2000 &lt;dbl&gt;, ## # share_white2000 &lt;dbl&gt;, share_hisp2000 &lt;dbl&gt;, share_asian2000 &lt;dbl&gt;, ## # gsmn_math_g3_2013 &lt;dbl&gt;, rent_twobed2015 &lt;dbl&gt;, ... Lets unpack this output: A tibble: 73,278 x 62: A tibble is a kind of data frame used in R. This particular data frame has 73,278 rows (one for each neighborhood) 62 columns corresponding to 62 variables describing each observation (e.g. neighborhood in this case) tract county state cz czname hhinc_mean2000 mean_commutetime2000 ... are different columns, in other words variables, of this data frame. We then have the first 10 rows of observations corresponding to 10 neighborhoods. ... with 73,268 more rows, and 52 more variables: indicating to us that 73,268 more rows of data and 52 more variables could not fit in this screen. Unfortunately, this output does not allow us to explore the data very well. Lets look at different tools to explore data frames. 2.7.4 Exploring data frames Among the many ways of getting a feel for the data contained in a data frame such as atlas, we present three functions that take as their argument, in other words their input, the data frame in question. We also include a fourth method for exploring one particular column of a data frame: Using the View() function built for use in RStudio. We will use this the most. Using the glimpse() function, which is included in the dplyr package. Using the $ operator to view a single variable in a data frame. 1. View(): Run View(atlas) in your Console in RStudio, either by typing it or cutting &amp; pasting it into the Console pane, and explore this data frame in the resulting pop-up viewer. You should get into the habit of always Viewing any data frames that come your way. Note the capital V in View. R is case-sensitive so youll receive an error is you run view(atlas) instead of View(atlas). Learning check (LC2.3) What does any ONE row in this atlas dataset refer to? A. Data on an neighborhood B. Data on a state C. Data on an person D. Data on multiple neighborhood By running View(atlas), we see the different variables listed in the columns and we see that there are different types of variables. Some of the variables like poor_share2010, hhinc_mean2000, and share_hisp2010 are what we will call quantitative variables. These variables are numerical in nature. Other variables, like tract are categorical: they are just names (even if they have numbers). For example tract represents a Tract FIPS Code, that is, a 6-digit code assigned by the census folks to each neighborhood in 2010. Note that if you look in the leftmost column of the View(atlas) output, you will see a column of numbers. These are the row numbers of the dataset. If you glance across a row with the same number, say row 5, you can get an idea of what each row corresponds to. 2. glimpse(): The second way to explore a data frame is using the glimpse() function included in the dplyr package. Thus, you can only use the glimpse() function after youve loaded the dplyr package. This function provides us with an alternative method for exploring a data frame: glimpse(atlas) ## Rows: 73,278 ## Columns: 62 ## $ tract &lt;dbl&gt; 20100, 20200, 20300, 20400, 20500, 20600,~ ## $ county &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3,~ ## $ state &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,~ ## $ cz &lt;dbl&gt; 11101, 11101, 11101, 11101, 11101, 11101,~ ## $ czname &lt;chr&gt; &quot;Montgomery&quot;, &quot;Montgomery&quot;, &quot;Montgomery&quot;,~ ## $ hhinc_mean2000 &lt;dbl&gt; 68639, 57243, 75648, 74852, 96175, 68096,~ ## $ mean_commutetime2000 &lt;dbl&gt; 26.2, 24.8, 25.3, 23.0, 26.2, 21.6, 23.2,~ ## $ frac_coll_plus2010 &lt;dbl&gt; 0.2544, 0.2672, 0.1642, 0.2527, 0.3751, 0~ ## $ frac_coll_plus2000 &lt;dbl&gt; 0.1565, 0.1469, 0.2244, 0.2305, 0.3212, 0~ ## $ foreign_share2010 &lt;dbl&gt; 0.00995, 0.01634, 0.02710, 0.01508, 0.046~ ## $ med_hhinc2016 &lt;dbl&gt; 66000, 41107, 51250, 52704, 52463, 63750,~ ## $ med_hhinc1990 &lt;dbl&gt; 27375, 19000, 29419, 37891, 41516, 29000,~ ## $ popdensity2000 &lt;dbl&gt; 195.72, 566.38, 624.20, 713.80, 529.93, 4~ ## $ poor_share2010 &lt;dbl&gt; 0.1050, 0.1476, 0.0804, 0.0632, 0.0596, 0~ ## $ poor_share2000 &lt;dbl&gt; 0.1268, 0.2271, 0.0766, 0.0455, 0.0368, 0~ ## $ poor_share1990 &lt;dbl&gt; 0.0989, 0.1983, 0.1140, 0.0679, 0.0547, 0~ ## $ share_black2010 &lt;dbl&gt; 0.1192, 0.5650, 0.1980, 0.0467, 0.1397, 0~ ## $ share_hisp2010 &lt;dbl&gt; 0.02301, 0.03456, 0.02579, 0.01938, 0.032~ ## $ share_asian2010 &lt;dbl&gt; 0.004707, 0.002304, 0.004744, 0.003648, 0~ ## $ share_black2000 &lt;dbl&gt; 0.0755, 0.6221, 0.1491, 0.0259, 0.0601, 0~ ## $ share_white2000 &lt;dbl&gt; 0.897, 0.355, 0.820, 0.938, 0.897, 0.799,~ ## $ share_hisp2000 &lt;dbl&gt; 0.00625, 0.00846, 0.01647, 0.02217, 0.015~ ## $ share_asian2000 &lt;dbl&gt; 0.003644, 0.003171, 0.003893, 0.007288, 0~ ## $ gsmn_math_g3_2013 &lt;dbl&gt; 2.76, 2.76, 2.76, 2.76, 2.76, 2.76, 2.76,~ ## $ rent_twobed2015 &lt;dbl&gt; NA, 907, 583, 713, 923, 765, 645, 532, 67~ ## $ singleparent_share2010 &lt;dbl&gt; 0.1139, 0.4885, 0.2281, 0.2275, 0.2597, 0~ ## $ singleparent_share1990 &lt;dbl&gt; 0.1812, 0.3525, 0.1259, 0.1268, 0.0744, 0~ ## $ singleparent_share2000 &lt;dbl&gt; 0.251, 0.393, 0.245, 0.191, 0.168, 0.289,~ ## $ traveltime15_2010 &lt;dbl&gt; 0.2730, 0.1520, 0.2055, 0.3507, 0.2505, 0~ ## $ emp2000 &lt;dbl&gt; 0.567, 0.493, 0.579, 0.597, 0.661, 0.643,~ ## $ mail_return_rate2010 &lt;dbl&gt; 83.5, 81.3, 79.5, 83.5, 77.3, 82.8, 83.2,~ ## $ ln_wage_growth_hs_grad &lt;dbl&gt; 0.03823, 0.08931, -0.17774, -0.07231, -0.~ ## $ jobs_total_5mi_2015 &lt;dbl&gt; 10109, 9948, 10387, 12933, 12933, 9193, 1~ ## $ jobs_highpay_5mi_2015 &lt;dbl&gt; 3396, 3328, 3230, 3635, 3635, 3052, 3389,~ ## $ nonwhite_share2010 &lt;dbl&gt; 0.1627, 0.6111, 0.2476, 0.0812, 0.2162, 0~ ## $ popdensity2010 &lt;dbl&gt; 504.8, 1682.2, 1633.4, 1780.0, 2446.3, 11~ ## $ ann_avg_job_growth_2004_2013 &lt;dbl&gt; -0.00677, -0.00425, 0.01422, -0.01984, 0.~ ## $ job_density_2013 &lt;dbl&gt; 92.133, 971.318, 340.920, 207.386, 800.27~ ## $ kfr_natam_p25 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N~ ## $ kfr_natam_p75 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N~ ## $ kfr_natam_p100 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N~ ## $ kfr_asian_p25 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N~ ## $ kfr_asian_p75 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N~ ## $ kfr_asian_p100 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N~ ## $ kfr_black_p25 &lt;dbl&gt; 26819, 18138, 20515, 12883, 26594, 19108,~ ## $ kfr_black_p75 &lt;dbl&gt; 45926, 33842, 34133, 40334, 42575, 26062,~ ## $ kfr_black_p100 &lt;dbl&gt; 84690, 60512, 56516, 105250, 72565, 35737~ ## $ kfr_hisp_p25 &lt;dbl&gt; NA, NA, NA, 26363, 17234, NA, NA, NA, 329~ ## $ kfr_hisp_p75 &lt;dbl&gt; NA, NA, NA, 67532, 44642, NA, NA, NA, 655~ ## $ kfr_hisp_p100 &lt;dbl&gt; NA, NA, NA, NA, 93976, NA, NA, NA, 147274~ ## $ kfr_pooled_p25 &lt;dbl&gt; 27621, 22303, 28215, 33331, 34633, 23583,~ ## $ kfr_pooled_p75 &lt;dbl&gt; 51531, 46650, 50754, 52337, 57007, 47735,~ ## $ kfr_pooled_p100 &lt;dbl&gt; 78922, 74225, 76055, 72586, 81792, 75188,~ ## $ kfr_white_p25 &lt;dbl&gt; 30328, 42189, 33670, 34181, 39540, 27835,~ ## $ kfr_white_p75 &lt;dbl&gt; 50820, 54239, 51579, 52848, 58699, 51198,~ ## $ kfr_white_p100 &lt;dbl&gt; 75126, 66646, 71991, 74330, 80415, 80144,~ ## $ count_pooled &lt;dbl&gt; 519, 530, 960, 1123, 1867, 994, 772, 632,~ ## $ count_white &lt;dbl&gt; 457, 173, 774, 1033, 1626, 756, 630, 523,~ ## $ count_black &lt;dbl&gt; 42, 336, 151, 40, 137, 198, 111, 89, 290,~ ## $ count_asian &lt;dbl&gt; 3, 1, 1, 6, 13, 2, 1, 1, 5, 0, 0, 0, 0, 3~ ## $ count_hisp &lt;dbl&gt; 4, 5, 21, 37, 39, 19, 14, 9, 29, 17, 7, 3~ ## $ count_natam &lt;dbl&gt; 6, 1, 2, 0, 8, 2, 9, 1, 5, 4, 8, 3, 20, 8~ We see that glimpse() will give you the first few entries of each variable in a row after the variable. In addition, the data type of the variable is given immediately after each variables name inside &lt; &gt;. Here, int and dbl refer to integer and double, which are computer coding terminology for quantitative/numerical variables. In contrast, chr refers to character, which is computer terminology for text data. Text data, such as the czname (the name of the metro area), are categorical variables. 3. $ operator Lastly, the $ operator allows us to explore a single variable within a data frame. For example, run the following in your console atlas$tract We used the $ operator to extract only the tract variable and return it as a vector of length 73,278. We will only be occasionally exploring data frames using this operator, instead favoring the View() and glimpse() functions. 2.8 Conclusion Weve given you what we feel are the most essential concepts to know before you can start exploring data in R. There is much more to explore in R but this is a great place to get started! 2.8.1 Additional resources If you want to dive more and feel you could benefit from a more detailed introduction, check this short book: Getting used to R, RStudio, and R Markdown short book. It has screencast recordings that you can follow along and pause as you learn. Furthermore, there is an introduction to R Markdown, a tool used for reproducible research in R. We will see more about that in the next class. If you truly insist on getting more information, you can check this link explaining some of the basics: https://rstudio-education.github.io/hopr/basics.html ;but again, this is not required or expected. "],["appendixA.html", "A Statistical Background A.1 Basic statistical terms", " A Statistical Background A.1 Basic statistical terms A.1.1 Mean The mean, also known as (AKA) the average, is the most commonly reported measure of center. It is commonly called the average though this term can be a little ambiguous. The mean is the sum of all of the data elements divided by how many elements there are. If we have \\(n\\) data points, the mean is given by: \\[\\overline{x} = \\frac{x_1 + x_2 + \\cdots + x_n}{n}\\] Note that you will often see shorthand notation for a sum of numbers using \\(\\sum\\) notation. For example, we could rewrite the formula for \\(\\bar{x}\\) as: \\[\\bar{x} = \\frac{x_1 + x_2 + \\cdots + x_n}{n}=\\frac{\\sum_{i = 1}^n x_i}{n}\\] Weve simply replaced the subscript on each \\(x\\) with a generic index \\(i\\), and use the \\(\\sum\\) notation to indicate we are summing \\(x\\)s with indices (i.e. subscripts) that go from 1 to \\(n\\). When summing numbers in statistics, were almost always dealing with indices that start with the value 1 and go up to a value equal to a sample size (e.g. \\(n\\)), so often you will see an even more shorthand version, where its assumed youre summing from \\(i = 1\\) to \\(i = n\\): \\[\\bar{x} = \\frac{\\sum x_i}{n}\\] A.1.2 Median The median is calculated by first sorting a variables data from smallest to largest. After sorting the data, the middle element in the list is the median. If the middle falls between two values, then the median is the mean of those two values. A.1.3 Standard deviation We will next discuss the standard deviation of a sample dataset pertaining to one variable. The formula can be a little intimidating at first but it is important to remember that it is essentially a measure of how far to expect a given data value is from its mean: \\[Standard \\, deviation = \\sqrt{\\frac{(x_1 - \\overline{x})^2 + (x_2 - \\overline{x})^2 + \\cdots + (x_n - \\overline{x})^2}{n - 1}} = \\sqrt{\\frac{\\sum_{i= 1}^n(x_i - \\overline{x})^2 }{n - 1}}\\] A.1.4 Five-number summary The five-number summary consists of five values: minimum, first quartile AKA 25th percentile, second quartile AKA median AKA 50th percentile, third quartile AKA 75th, and maximum. The quartiles are calculated as first quartile (\\(Q_1\\)): the median of the first half of the sorted data third quartile (\\(Q_3\\)): the median of the second half of the sorted data The interquartile range is defined as \\(Q_3 - Q_1\\) and is a measure of how spread out the middle 50% of values is. The five-number summary is not influenced by the presence of outliers in the ways that the mean and standard deviation are. It is, thus, recommended for skewed datasets. A.1.5 Percentiles or rank Just as you can sort the values of a variable form smallest to larges and find the middle element to get the median, you are dividing all the values into two groups with the same number of observations. If you do that but instead of dividing into 2 groups, you divide it into 4 groups, you get each of the quartiles. If you divide the sorted observations into 100 groups of equal number of observations, you get the percentiles. Depending on the application, the percentile can be read as the rank in the distribution. A.1.6 Distribution The distribution of a variable/dataset corresponds to generalizing patterns in the dataset. It often shows how frequently elements in the dataset appear. It shows how the data varies and gives some information about where a typical element in the data might fall. Distributions are most easily seen through data visualization. A.1.7 Outliers Outliers correspond to values in the dataset that fall far outside the range of ordinary values. In regards to a boxplot (by default), they correspond to values below \\(Q_1 - (1.5 * IQR)\\) or above \\(Q_3 + (1.5 * IQR)\\). Note that these terms (aside from Distribution) only apply to quantitative variables. "],["project1.html", "Project 1", " Project 1 To be filled soon. Check the due date in the schedule page. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
